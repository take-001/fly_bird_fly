{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Grayscale\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs.lidar import LIDAR\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    "    PIPE_WIDTH,\n",
    "    PIPE_VEL_X,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neural Network Model for Q-value approximation\n",
    "# Fully connected model for 1D state inputs\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)       # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, action_space)  # Output layer for Q-values\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "\n",
    "# Replay Memory to store experiences\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# DQN Agent Class\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]  # First dimension of observation\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.memory_size = 10000\n",
    "        self.episodes = 50000\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "        # Initialize policy and target networks\n",
    "        self.policy_net = DQN(self.state_dim, self.action_space)\n",
    "        self.target_net = DQN(self.state_dim, self.action_space)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "    def select_action(self, state, testing=False):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_space - 1)  # Random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        res=[]\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render,use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=True)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "\n",
    "# Policy Gradient Neural Network Model\n",
    "class PolicyGradientNet(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(PolicyGradientNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)       # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, action_space)  # Output layer for Q-values\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, input_dim)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return torch.softmax(self.fc2(x), dim=-1)  # Apply softmax for probabilities\n",
    "\n",
    "\n",
    "# Policy Gradient Agent Class\n",
    "class PolicyGradientAgent:\n",
    "    def __init__(self, env):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]  # First dimension of observation\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.discount_factor = 0.99\n",
    "        self.episodes = 50000\n",
    "\n",
    "        # Initialize policy network\n",
    "        self.policy_net = PolicyGradientNet(self.state_dim, self.action_space)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Memory for rewards and log probabilities\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action based on policy.\"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = self.policy_net(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        self.log_probs.append(action_dist.log_prob(action))\n",
    "        return action.item()\n",
    "\n",
    "    def compute_returns(self):\n",
    "        \"\"\"Compute discounted returns for each time step.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.discount_factor * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        # Normalize returns for better convergence\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        return returns\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"Perform a training step after one episode.\"\"\"\n",
    "        returns = self.compute_returns()\n",
    "        loss = 0\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            loss += -log_prob * G  # Negative sign for gradient ascent\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Clear memory\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def train(self):\n",
    "        res=[]\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.rewards.append(reward)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            # Perform a training step after the episode ends\n",
    "            self.train_step()\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render, use_lidar=True)\n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                probs = self.policy_net(state)\n",
    "                action = torch.argmax(probs).item()  # Select action with highest probability\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvdg/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/cvdg/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -5.699999999999998, Epsilon: 0.9950\n",
      "Episode: 2, Total Reward: -8.099999999999998, Epsilon: 0.9900\n",
      "Episode: 3, Total Reward: -8.099999999999998, Epsilon: 0.9851\n",
      "Episode: 4, Total Reward: -8.099999999999998, Epsilon: 0.9801\n",
      "Episode: 5, Total Reward: -8.099999999999998, Epsilon: 0.9752\n",
      "Episode: 6, Total Reward: -6.899999999999999, Epsilon: 0.9704\n",
      "Episode: 7, Total Reward: -6.899999999999999, Epsilon: 0.9655\n",
      "Episode: 8, Total Reward: -6.899999999999999, Epsilon: 0.9607\n",
      "Episode: 9, Total Reward: -6.299999999999999, Epsilon: 0.9559\n",
      "Episode: 10, Total Reward: -6.899999999999999, Epsilon: 0.9511\n",
      "Episode: 11, Total Reward: -5.099999999999998, Epsilon: 0.9464\n",
      "Episode: 12, Total Reward: -7.499999999999998, Epsilon: 0.9416\n",
      "Episode: 13, Total Reward: -8.099999999999998, Epsilon: 0.9369\n",
      "Episode: 14, Total Reward: -6.899999999999999, Epsilon: 0.9322\n",
      "Episode: 15, Total Reward: -8.099999999999998, Epsilon: 0.9276\n",
      "Episode: 16, Total Reward: -8.099999999999998, Epsilon: 0.9229\n",
      "Episode: 17, Total Reward: -8.099999999999998, Epsilon: 0.9183\n",
      "Episode: 18, Total Reward: -6.899999999999999, Epsilon: 0.9137\n",
      "Episode: 19, Total Reward: -7.499999999999998, Epsilon: 0.9092\n",
      "Episode: 20, Total Reward: -8.7, Epsilon: 0.9046\n",
      "Episode: 21, Total Reward: -5.099999999999998, Epsilon: 0.9001\n",
      "Episode: 22, Total Reward: -8.099999999999998, Epsilon: 0.8956\n",
      "Episode: 23, Total Reward: -8.099999999999998, Epsilon: 0.8911\n",
      "Episode: 24, Total Reward: -8.099999999999998, Epsilon: 0.8867\n",
      "Episode: 25, Total Reward: -5.699999999999998, Epsilon: 0.8822\n",
      "Episode: 26, Total Reward: -8.7, Epsilon: 0.8778\n",
      "Episode: 27, Total Reward: -7.499999999999998, Epsilon: 0.8734\n",
      "Episode: 28, Total Reward: -7.499999999999998, Epsilon: 0.8691\n",
      "Episode: 29, Total Reward: -8.099999999999998, Epsilon: 0.8647\n",
      "Episode: 30, Total Reward: -8.099999999999998, Epsilon: 0.8604\n",
      "Episode: 31, Total Reward: -6.299999999999999, Epsilon: 0.8561\n",
      "Episode: 32, Total Reward: -7.499999999999998, Epsilon: 0.8518\n",
      "Episode: 33, Total Reward: -8.099999999999998, Epsilon: 0.8475\n",
      "Episode: 34, Total Reward: -6.299999999999999, Epsilon: 0.8433\n",
      "Episode: 35, Total Reward: -8.099999999999998, Epsilon: 0.8391\n",
      "Episode: 36, Total Reward: -6.899999999999999, Epsilon: 0.8349\n",
      "Episode: 37, Total Reward: -7.499999999999998, Epsilon: 0.8307\n",
      "Episode: 38, Total Reward: -8.099999999999998, Epsilon: 0.8266\n",
      "Episode: 39, Total Reward: -6.899999999999999, Epsilon: 0.8224\n",
      "Episode: 40, Total Reward: -8.099999999999998, Epsilon: 0.8183\n",
      "Episode: 41, Total Reward: -6.899999999999999, Epsilon: 0.8142\n",
      "Episode: 42, Total Reward: -8.099999999999998, Epsilon: 0.8102\n",
      "Episode: 43, Total Reward: -8.099999999999998, Epsilon: 0.8061\n",
      "Episode: 44, Total Reward: -7.499999999999998, Epsilon: 0.8021\n",
      "Episode: 45, Total Reward: -7.499999999999998, Epsilon: 0.7981\n",
      "Episode: 46, Total Reward: -8.099999999999998, Epsilon: 0.7941\n",
      "Episode: 47, Total Reward: -7.499999999999998, Epsilon: 0.7901\n",
      "Episode: 48, Total Reward: -6.899999999999999, Epsilon: 0.7862\n",
      "Episode: 49, Total Reward: -8.7, Epsilon: 0.7822\n",
      "Episode: 50, Total Reward: -5.699999999999998, Epsilon: 0.7783\n",
      "Episode: 51, Total Reward: -8.099999999999998, Epsilon: 0.7744\n",
      "Episode: 52, Total Reward: -7.499999999999998, Epsilon: 0.7705\n",
      "Episode: 53, Total Reward: -7.499999999999998, Epsilon: 0.7667\n",
      "Episode: 54, Total Reward: -7.499999999999998, Epsilon: 0.7629\n",
      "Episode: 55, Total Reward: -8.099999999999998, Epsilon: 0.7590\n",
      "Episode: 56, Total Reward: -8.099999999999998, Epsilon: 0.7553\n",
      "Episode: 57, Total Reward: -7.499999999999998, Epsilon: 0.7515\n",
      "Episode: 58, Total Reward: -7.499999999999998, Epsilon: 0.7477\n",
      "Episode: 59, Total Reward: -8.099999999999998, Epsilon: 0.7440\n",
      "Episode: 60, Total Reward: -8.099999999999998, Epsilon: 0.7403\n",
      "Episode: 61, Total Reward: -8.099999999999998, Epsilon: 0.7366\n",
      "Episode: 62, Total Reward: -8.7, Epsilon: 0.7329\n",
      "Episode: 63, Total Reward: -8.099999999999998, Epsilon: 0.7292\n",
      "Episode: 64, Total Reward: -8.099999999999998, Epsilon: 0.7256\n",
      "Episode: 65, Total Reward: -8.7, Epsilon: 0.7219\n",
      "Episode: 66, Total Reward: -6.899999999999999, Epsilon: 0.7183\n",
      "Episode: 67, Total Reward: -6.299999999999999, Epsilon: 0.7147\n",
      "Episode: 68, Total Reward: -8.7, Epsilon: 0.7112\n",
      "Episode: 69, Total Reward: -8.099999999999998, Epsilon: 0.7076\n",
      "Episode: 70, Total Reward: -5.699999999999998, Epsilon: 0.7041\n",
      "Episode: 71, Total Reward: -8.7, Epsilon: 0.7005\n",
      "Episode: 72, Total Reward: -8.7, Epsilon: 0.6970\n",
      "Episode: 73, Total Reward: -6.899999999999999, Epsilon: 0.6936\n",
      "Episode: 74, Total Reward: -6.899999999999999, Epsilon: 0.6901\n",
      "Episode: 75, Total Reward: -8.099999999999998, Epsilon: 0.6866\n",
      "Episode: 76, Total Reward: -6.899999999999999, Epsilon: 0.6832\n",
      "Episode: 77, Total Reward: -6.899999999999999, Epsilon: 0.6798\n",
      "Episode: 78, Total Reward: -7.499999999999998, Epsilon: 0.6764\n",
      "Episode: 79, Total Reward: -8.099999999999998, Epsilon: 0.6730\n",
      "Episode: 80, Total Reward: -8.099999999999998, Epsilon: 0.6696\n",
      "Episode: 81, Total Reward: -8.099999999999998, Epsilon: 0.6663\n",
      "Episode: 82, Total Reward: -6.299999999999999, Epsilon: 0.6630\n",
      "Episode: 83, Total Reward: -7.499999999999998, Epsilon: 0.6597\n",
      "Episode: 84, Total Reward: -7.499999999999998, Epsilon: 0.6564\n",
      "Episode: 85, Total Reward: -6.899999999999999, Epsilon: 0.6531\n",
      "Episode: 86, Total Reward: -3.299999999999998, Epsilon: 0.6498\n",
      "Episode: 87, Total Reward: -7.499999999999998, Epsilon: 0.6466\n",
      "Episode: 88, Total Reward: -6.299999999999999, Epsilon: 0.6433\n",
      "Episode: 89, Total Reward: -6.299999999999999, Epsilon: 0.6401\n",
      "Episode: 90, Total Reward: -6.899999999999999, Epsilon: 0.6369\n",
      "Episode: 91, Total Reward: -7.499999999999998, Epsilon: 0.6337\n",
      "Episode: 92, Total Reward: -8.7, Epsilon: 0.6306\n",
      "Episode: 93, Total Reward: -6.899999999999999, Epsilon: 0.6274\n",
      "Episode: 94, Total Reward: -6.899999999999999, Epsilon: 0.6243\n",
      "Episode: 95, Total Reward: -7.499999999999998, Epsilon: 0.6211\n",
      "Episode: 96, Total Reward: -7.499999999999998, Epsilon: 0.6180\n",
      "Episode: 97, Total Reward: -8.099999999999998, Epsilon: 0.6149\n",
      "Episode: 98, Total Reward: -8.7, Epsilon: 0.6119\n",
      "Episode: 99, Total Reward: -6.299999999999999, Epsilon: 0.6088\n",
      "Episode: 100, Total Reward: -5.699999999999998, Epsilon: 0.6058\n",
      "Episode: 101, Total Reward: -6.899999999999999, Epsilon: 0.6027\n",
      "Episode: 102, Total Reward: -7.499999999999998, Epsilon: 0.5997\n",
      "Episode: 103, Total Reward: -6.899999999999999, Epsilon: 0.5967\n",
      "Episode: 104, Total Reward: -2.699999999999998, Epsilon: 0.5937\n",
      "Episode: 105, Total Reward: -7.499999999999998, Epsilon: 0.5908\n",
      "Episode: 106, Total Reward: -2.699999999999998, Epsilon: 0.5878\n",
      "Episode: 107, Total Reward: -7.499999999999998, Epsilon: 0.5849\n",
      "Episode: 108, Total Reward: -8.099999999999998, Epsilon: 0.5820\n",
      "Episode: 109, Total Reward: -3.899999999999998, Epsilon: 0.5790\n",
      "Episode: 110, Total Reward: -3.299999999999998, Epsilon: 0.5762\n",
      "Episode: 111, Total Reward: -6.299999999999999, Epsilon: 0.5733\n",
      "Episode: 112, Total Reward: -0.8999999999999986, Epsilon: 0.5704\n",
      "Episode: 113, Total Reward: -0.8999999999999986, Epsilon: 0.5676\n",
      "Episode: 114, Total Reward: -6.899999999999999, Epsilon: 0.5647\n",
      "Episode: 115, Total Reward: -6.299999999999999, Epsilon: 0.5619\n",
      "Episode: 116, Total Reward: -6.899999999999999, Epsilon: 0.5591\n",
      "Episode: 117, Total Reward: -5.699999999999998, Epsilon: 0.5563\n",
      "Episode: 118, Total Reward: -8.099999999999998, Epsilon: 0.5535\n",
      "Episode: 119, Total Reward: -5.699999999999998, Epsilon: 0.5507\n",
      "Episode: 120, Total Reward: -6.299999999999999, Epsilon: 0.5480\n",
      "Episode: 121, Total Reward: -2.699999999999998, Epsilon: 0.5452\n",
      "Episode: 122, Total Reward: -5.099999999999998, Epsilon: 0.5425\n",
      "Episode: 123, Total Reward: -0.8999999999999986, Epsilon: 0.5398\n",
      "Episode: 124, Total Reward: -3.899999999999998, Epsilon: 0.5371\n",
      "Episode: 125, Total Reward: -7.499999999999998, Epsilon: 0.5344\n",
      "Episode: 126, Total Reward: -5.099999999999998, Epsilon: 0.5318\n",
      "Episode: 127, Total Reward: -4.499999999999998, Epsilon: 0.5291\n",
      "Episode: 128, Total Reward: -8.099999999999998, Epsilon: 0.5264\n",
      "Episode: 129, Total Reward: -3.899999999999998, Epsilon: 0.5238\n",
      "Episode: 130, Total Reward: -6.299999999999999, Epsilon: 0.5212\n",
      "Episode: 131, Total Reward: -7.499999999999998, Epsilon: 0.5186\n",
      "Episode: 132, Total Reward: -6.299999999999999, Epsilon: 0.5160\n",
      "Episode: 133, Total Reward: -6.299999999999999, Epsilon: 0.5134\n",
      "Episode: 134, Total Reward: -4.499999999999998, Epsilon: 0.5108\n",
      "Episode: 135, Total Reward: -2.099999999999998, Epsilon: 0.5083\n",
      "Episode: 136, Total Reward: -5.699999999999998, Epsilon: 0.5058\n",
      "Episode: 137, Total Reward: -7.499999999999998, Epsilon: 0.5032\n",
      "Episode: 138, Total Reward: -5.699999999999998, Epsilon: 0.5007\n",
      "Episode: 139, Total Reward: -6.899999999999999, Epsilon: 0.4982\n",
      "Episode: 140, Total Reward: -8.099999999999998, Epsilon: 0.4957\n",
      "Episode: 141, Total Reward: -2.699999999999998, Epsilon: 0.4932\n",
      "Episode: 142, Total Reward: -6.299999999999999, Epsilon: 0.4908\n",
      "Episode: 143, Total Reward: -0.8999999999999986, Epsilon: 0.4883\n",
      "Episode: 144, Total Reward: -6.299999999999999, Epsilon: 0.4859\n",
      "Episode: 145, Total Reward: -7.499999999999998, Epsilon: 0.4834\n",
      "Episode: 146, Total Reward: -6.299999999999999, Epsilon: 0.4810\n",
      "Episode: 147, Total Reward: -2.699999999999998, Epsilon: 0.4786\n",
      "Episode: 148, Total Reward: -8.099999999999998, Epsilon: 0.4762\n",
      "Episode: 149, Total Reward: -3.899999999999998, Epsilon: 0.4738\n",
      "Episode: 150, Total Reward: -0.8999999999999986, Epsilon: 0.4715\n",
      "Episode: 151, Total Reward: -7.499999999999998, Epsilon: 0.4691\n",
      "Episode: 152, Total Reward: -6.299999999999999, Epsilon: 0.4668\n",
      "Episode: 153, Total Reward: -5.699999999999998, Epsilon: 0.4644\n",
      "Episode: 154, Total Reward: -2.099999999999998, Epsilon: 0.4621\n",
      "Episode: 155, Total Reward: -7.499999999999998, Epsilon: 0.4598\n",
      "Episode: 156, Total Reward: -5.699999999999998, Epsilon: 0.4575\n",
      "Episode: 157, Total Reward: -5.699999999999998, Epsilon: 0.4552\n",
      "Episode: 158, Total Reward: -1.4999999999999982, Epsilon: 0.4529\n",
      "Episode: 159, Total Reward: -0.8999999999999986, Epsilon: 0.4507\n",
      "Episode: 160, Total Reward: -6.899999999999999, Epsilon: 0.4484\n",
      "Episode: 161, Total Reward: -2.099999999999998, Epsilon: 0.4462\n",
      "Episode: 162, Total Reward: -0.8999999999999986, Epsilon: 0.4440\n",
      "Episode: 163, Total Reward: -0.8999999999999986, Epsilon: 0.4417\n",
      "Episode: 164, Total Reward: -3.299999999999998, Epsilon: 0.4395\n",
      "Episode: 165, Total Reward: -1.4999999999999982, Epsilon: 0.4373\n",
      "Episode: 166, Total Reward: -0.8999999999999986, Epsilon: 0.4351\n",
      "Episode: 167, Total Reward: -6.899999999999999, Epsilon: 0.4330\n",
      "Episode: 168, Total Reward: -5.099999999999998, Epsilon: 0.4308\n",
      "Episode: 169, Total Reward: -5.099999999999998, Epsilon: 0.4286\n",
      "Episode: 170, Total Reward: -0.8999999999999986, Epsilon: 0.4265\n",
      "Episode: 171, Total Reward: -5.099999999999998, Epsilon: 0.4244\n",
      "Episode: 172, Total Reward: -0.8999999999999986, Epsilon: 0.4223\n",
      "Episode: 173, Total Reward: -3.899999999999998, Epsilon: 0.4201\n",
      "Episode: 174, Total Reward: -0.8999999999999986, Epsilon: 0.4180\n",
      "Episode: 175, Total Reward: -3.899999999999998, Epsilon: 0.4159\n",
      "Episode: 176, Total Reward: -6.899999999999999, Epsilon: 0.4139\n",
      "Episode: 177, Total Reward: -0.8999999999999986, Epsilon: 0.4118\n",
      "Episode: 178, Total Reward: -0.8999999999999986, Epsilon: 0.4097\n",
      "Episode: 179, Total Reward: -2.099999999999998, Epsilon: 0.4077\n",
      "Episode: 180, Total Reward: -0.8999999999999986, Epsilon: 0.4057\n",
      "Episode: 181, Total Reward: -0.8999999999999986, Epsilon: 0.4036\n",
      "Episode: 182, Total Reward: -5.099999999999998, Epsilon: 0.4016\n",
      "Episode: 183, Total Reward: -3.299999999999998, Epsilon: 0.3996\n",
      "Episode: 184, Total Reward: -5.099999999999998, Epsilon: 0.3976\n",
      "Episode: 185, Total Reward: -1.4999999999999982, Epsilon: 0.3956\n",
      "Episode: 186, Total Reward: -4.499999999999998, Epsilon: 0.3936\n",
      "Episode: 187, Total Reward: -0.8999999999999986, Epsilon: 0.3917\n",
      "Episode: 188, Total Reward: -1.4999999999999982, Epsilon: 0.3897\n",
      "Episode: 189, Total Reward: -5.699999999999998, Epsilon: 0.3878\n",
      "Episode: 190, Total Reward: -8.099999999999998, Epsilon: 0.3858\n",
      "Episode: 191, Total Reward: -3.899999999999998, Epsilon: 0.3839\n",
      "Episode: 192, Total Reward: -0.8999999999999986, Epsilon: 0.3820\n",
      "Episode: 193, Total Reward: 1.9999999999999982, Epsilon: 0.3801\n",
      "Episode: 194, Total Reward: -1.4999999999999982, Epsilon: 0.3782\n",
      "Episode: 195, Total Reward: -2.099999999999998, Epsilon: 0.3763\n",
      "Episode: 196, Total Reward: -0.8999999999999986, Epsilon: 0.3744\n",
      "Episode: 197, Total Reward: -7.499999999999998, Epsilon: 0.3725\n",
      "Episode: 198, Total Reward: -0.8999999999999986, Epsilon: 0.3707\n",
      "Episode: 199, Total Reward: -0.8999999999999986, Epsilon: 0.3688\n",
      "Episode: 200, Total Reward: -0.8999999999999986, Epsilon: 0.3670\n",
      "Episode: 201, Total Reward: -0.8999999999999986, Epsilon: 0.3651\n",
      "Episode: 202, Total Reward: -4.499999999999998, Epsilon: 0.3633\n",
      "Episode: 203, Total Reward: -0.8999999999999986, Epsilon: 0.3615\n",
      "Episode: 204, Total Reward: -7.499999999999998, Epsilon: 0.3597\n",
      "Episode: 205, Total Reward: -0.8999999999999986, Epsilon: 0.3579\n",
      "Episode: 206, Total Reward: -6.899999999999999, Epsilon: 0.3561\n",
      "Episode: 207, Total Reward: -0.8999999999999986, Epsilon: 0.3543\n",
      "Episode: 208, Total Reward: -6.299999999999999, Epsilon: 0.3525\n",
      "Episode: 209, Total Reward: -8.099999999999998, Epsilon: 0.3508\n",
      "Episode: 210, Total Reward: -2.699999999999998, Epsilon: 0.3490\n",
      "Episode: 211, Total Reward: -0.8999999999999986, Epsilon: 0.3473\n",
      "Episode: 212, Total Reward: -0.8999999999999986, Epsilon: 0.3455\n",
      "Episode: 213, Total Reward: -3.299999999999998, Epsilon: 0.3438\n",
      "Episode: 214, Total Reward: -5.699999999999998, Epsilon: 0.3421\n",
      "Episode: 215, Total Reward: -5.099999999999998, Epsilon: 0.3404\n",
      "Episode: 216, Total Reward: -8.099999999999998, Epsilon: 0.3387\n",
      "Episode: 217, Total Reward: -0.8999999999999986, Epsilon: 0.3370\n",
      "Episode: 218, Total Reward: -3.299999999999998, Epsilon: 0.3353\n",
      "Episode: 219, Total Reward: -4.499999999999998, Epsilon: 0.3336\n",
      "Episode: 220, Total Reward: -2.099999999999998, Epsilon: 0.3320\n",
      "Episode: 221, Total Reward: -0.8999999999999986, Epsilon: 0.3303\n",
      "Episode: 222, Total Reward: -0.8999999999999986, Epsilon: 0.3286\n",
      "Episode: 223, Total Reward: -0.8999999999999986, Epsilon: 0.3270\n",
      "Episode: 224, Total Reward: -5.099999999999998, Epsilon: 0.3254\n",
      "Episode: 225, Total Reward: -5.099999999999998, Epsilon: 0.3237\n",
      "Episode: 226, Total Reward: -7.499999999999998, Epsilon: 0.3221\n",
      "Episode: 227, Total Reward: -0.8999999999999986, Epsilon: 0.3205\n",
      "Episode: 228, Total Reward: -1.4999999999999982, Epsilon: 0.3189\n",
      "Episode: 229, Total Reward: -2.099999999999998, Epsilon: 0.3173\n",
      "Episode: 230, Total Reward: -4.499999999999998, Epsilon: 0.3157\n",
      "Episode: 231, Total Reward: -7.499999999999998, Epsilon: 0.3141\n",
      "Episode: 232, Total Reward: -5.099999999999998, Epsilon: 0.3126\n",
      "Episode: 233, Total Reward: -0.8999999999999986, Epsilon: 0.3110\n",
      "Episode: 234, Total Reward: -3.899999999999998, Epsilon: 0.3095\n",
      "Episode: 235, Total Reward: -0.8999999999999986, Epsilon: 0.3079\n",
      "Episode: 236, Total Reward: -1.4999999999999982, Epsilon: 0.3064\n",
      "Episode: 237, Total Reward: -5.699999999999998, Epsilon: 0.3048\n",
      "Episode: 238, Total Reward: -6.299999999999999, Epsilon: 0.3033\n",
      "Episode: 239, Total Reward: -0.8999999999999986, Epsilon: 0.3018\n",
      "Episode: 240, Total Reward: -0.8999999999999986, Epsilon: 0.3003\n",
      "Episode: 241, Total Reward: -0.8999999999999986, Epsilon: 0.2988\n",
      "Episode: 242, Total Reward: -0.8999999999999986, Epsilon: 0.2973\n",
      "Episode: 243, Total Reward: -4.499999999999998, Epsilon: 0.2958\n",
      "Episode: 244, Total Reward: -0.8999999999999986, Epsilon: 0.2943\n",
      "Episode: 245, Total Reward: -2.699999999999998, Epsilon: 0.2929\n",
      "Episode: 246, Total Reward: -0.8999999999999986, Epsilon: 0.2914\n",
      "Episode: 247, Total Reward: -6.899999999999999, Epsilon: 0.2899\n",
      "Episode: 248, Total Reward: -0.8999999999999986, Epsilon: 0.2885\n",
      "Episode: 249, Total Reward: -2.699999999999998, Epsilon: 0.2870\n",
      "Episode: 250, Total Reward: -0.29999999999999893, Epsilon: 0.2856\n",
      "Episode: 251, Total Reward: -9.299999999999999, Epsilon: 0.2842\n",
      "Episode: 252, Total Reward: -0.8999999999999986, Epsilon: 0.2828\n",
      "Episode: 253, Total Reward: -0.8999999999999986, Epsilon: 0.2813\n",
      "Episode: 254, Total Reward: -0.8999999999999986, Epsilon: 0.2799\n",
      "Episode: 255, Total Reward: -4.499999999999998, Epsilon: 0.2785\n",
      "Episode: 256, Total Reward: -0.8999999999999986, Epsilon: 0.2771\n",
      "Episode: 257, Total Reward: -0.8999999999999986, Epsilon: 0.2758\n",
      "Episode: 258, Total Reward: -6.899999999999999, Epsilon: 0.2744\n",
      "Episode: 259, Total Reward: -0.8999999999999986, Epsilon: 0.2730\n",
      "Episode: 260, Total Reward: -7.499999999999998, Epsilon: 0.2716\n",
      "Episode: 261, Total Reward: -0.8999999999999986, Epsilon: 0.2703\n",
      "Episode: 262, Total Reward: -3.299999999999998, Epsilon: 0.2689\n",
      "Episode: 263, Total Reward: -0.8999999999999986, Epsilon: 0.2676\n",
      "Episode: 264, Total Reward: -2.099999999999998, Epsilon: 0.2663\n",
      "Episode: 265, Total Reward: -0.8999999999999986, Epsilon: 0.2649\n",
      "Episode: 266, Total Reward: -0.8999999999999986, Epsilon: 0.2636\n",
      "Episode: 267, Total Reward: -1.4999999999999982, Epsilon: 0.2623\n",
      "Episode: 268, Total Reward: 0.3000000000000007, Epsilon: 0.2610\n",
      "Episode: 269, Total Reward: -6.299999999999999, Epsilon: 0.2597\n",
      "Episode: 270, Total Reward: -0.8999999999999986, Epsilon: 0.2584\n",
      "Episode: 271, Total Reward: -7.499999999999998, Epsilon: 0.2571\n",
      "Episode: 272, Total Reward: -7.499999999999998, Epsilon: 0.2558\n",
      "Episode: 273, Total Reward: -0.8999999999999986, Epsilon: 0.2545\n",
      "Episode: 274, Total Reward: -5.099999999999998, Epsilon: 0.2532\n",
      "Episode: 275, Total Reward: -0.8999999999999986, Epsilon: 0.2520\n",
      "Episode: 276, Total Reward: 3.1999999999999975, Epsilon: 0.2507\n",
      "Episode: 277, Total Reward: -9.299999999999999, Epsilon: 0.2495\n",
      "Episode: 278, Total Reward: -0.8999999999999986, Epsilon: 0.2482\n",
      "Episode: 279, Total Reward: -1.4999999999999982, Epsilon: 0.2470\n",
      "Episode: 280, Total Reward: -6.899999999999999, Epsilon: 0.2457\n",
      "Episode: 281, Total Reward: -0.8999999999999986, Epsilon: 0.2445\n",
      "Episode: 282, Total Reward: -2.099999999999998, Epsilon: 0.2433\n",
      "Episode: 283, Total Reward: 1.299999999999999, Epsilon: 0.2421\n",
      "Episode: 284, Total Reward: -6.899999999999999, Epsilon: 0.2409\n",
      "Episode: 285, Total Reward: -0.8999999999999986, Epsilon: 0.2397\n",
      "Episode: 286, Total Reward: -1.4999999999999982, Epsilon: 0.2385\n",
      "Episode: 287, Total Reward: -0.8999999999999986, Epsilon: 0.2373\n",
      "Episode: 288, Total Reward: -0.8999999999999986, Epsilon: 0.2361\n",
      "Episode: 289, Total Reward: -0.8999999999999986, Epsilon: 0.2349\n",
      "Episode: 290, Total Reward: -4.499999999999998, Epsilon: 0.2337\n",
      "Episode: 291, Total Reward: -3.899999999999998, Epsilon: 0.2326\n",
      "Episode: 292, Total Reward: -4.499999999999998, Epsilon: 0.2314\n",
      "Episode: 293, Total Reward: -0.8999999999999986, Epsilon: 0.2302\n",
      "Episode: 294, Total Reward: -5.099999999999998, Epsilon: 0.2291\n",
      "Episode: 295, Total Reward: -3.899999999999998, Epsilon: 0.2279\n",
      "Episode: 296, Total Reward: -0.8999999999999986, Epsilon: 0.2268\n",
      "Episode: 297, Total Reward: -0.8999999999999986, Epsilon: 0.2257\n",
      "Episode: 298, Total Reward: -0.8999999999999986, Epsilon: 0.2245\n",
      "Episode: 299, Total Reward: -0.8999999999999986, Epsilon: 0.2234\n",
      "Episode: 300, Total Reward: -0.8999999999999986, Epsilon: 0.2223\n",
      "Episode: 301, Total Reward: -4.499999999999998, Epsilon: 0.2212\n",
      "Episode: 302, Total Reward: -0.8999999999999986, Epsilon: 0.2201\n",
      "Episode: 303, Total Reward: -0.8999999999999986, Epsilon: 0.2190\n",
      "Episode: 304, Total Reward: -0.8999999999999986, Epsilon: 0.2179\n",
      "Episode: 305, Total Reward: -0.8999999999999986, Epsilon: 0.2168\n",
      "Episode: 306, Total Reward: -0.8999999999999986, Epsilon: 0.2157\n",
      "Episode: 307, Total Reward: -0.29999999999999893, Epsilon: 0.2146\n",
      "Episode: 308, Total Reward: 1.5, Epsilon: 0.2136\n",
      "Episode: 309, Total Reward: -5.699999999999998, Epsilon: 0.2125\n",
      "Episode: 310, Total Reward: -2.099999999999998, Epsilon: 0.2114\n",
      "Episode: 311, Total Reward: -0.8999999999999986, Epsilon: 0.2104\n",
      "Episode: 312, Total Reward: -1.3999999999999981, Epsilon: 0.2093\n",
      "Episode: 313, Total Reward: -3.299999999999998, Epsilon: 0.2083\n",
      "Episode: 314, Total Reward: -0.8999999999999986, Epsilon: 0.2072\n",
      "Episode: 315, Total Reward: -0.8999999999999986, Epsilon: 0.2062\n",
      "Episode: 316, Total Reward: -0.8999999999999986, Epsilon: 0.2052\n",
      "Episode: 317, Total Reward: -0.8999999999999986, Epsilon: 0.2041\n",
      "Episode: 318, Total Reward: -0.8999999999999986, Epsilon: 0.2031\n",
      "Episode: 319, Total Reward: -0.8999999999999986, Epsilon: 0.2021\n",
      "Episode: 320, Total Reward: -0.8999999999999986, Epsilon: 0.2011\n",
      "Episode: 321, Total Reward: -0.8999999999999986, Epsilon: 0.2001\n",
      "Episode: 322, Total Reward: -0.8999999999999986, Epsilon: 0.1991\n",
      "Episode: 323, Total Reward: -3.899999999999998, Epsilon: 0.1981\n",
      "Episode: 324, Total Reward: -0.8999999999999986, Epsilon: 0.1971\n",
      "Episode: 325, Total Reward: -0.8999999999999986, Epsilon: 0.1961\n",
      "Episode: 326, Total Reward: -1.5999999999999985, Epsilon: 0.1951\n",
      "Episode: 327, Total Reward: -0.8999999999999986, Epsilon: 0.1942\n",
      "Episode: 328, Total Reward: 0.9000000000000004, Epsilon: 0.1932\n",
      "Episode: 329, Total Reward: -0.8999999999999986, Epsilon: 0.1922\n",
      "Episode: 330, Total Reward: -0.8999999999999986, Epsilon: 0.1913\n",
      "Episode: 331, Total Reward: -0.8999999999999986, Epsilon: 0.1903\n",
      "Episode: 332, Total Reward: -0.8999999999999986, Epsilon: 0.1893\n",
      "Episode: 333, Total Reward: -0.8999999999999986, Epsilon: 0.1884\n",
      "Episode: 334, Total Reward: -0.8999999999999986, Epsilon: 0.1875\n",
      "Episode: 335, Total Reward: -1.6999999999999984, Epsilon: 0.1865\n",
      "Episode: 336, Total Reward: -0.8999999999999986, Epsilon: 0.1856\n",
      "Episode: 337, Total Reward: -0.8999999999999986, Epsilon: 0.1847\n",
      "Episode: 338, Total Reward: 0.9000000000000017, Epsilon: 0.1837\n",
      "Episode: 339, Total Reward: -2.699999999999998, Epsilon: 0.1828\n",
      "Episode: 340, Total Reward: -0.8999999999999986, Epsilon: 0.1819\n",
      "Episode: 341, Total Reward: -0.8999999999999986, Epsilon: 0.1810\n",
      "Episode: 342, Total Reward: -0.8999999999999986, Epsilon: 0.1801\n",
      "Episode: 343, Total Reward: -0.8999999999999986, Epsilon: 0.1792\n",
      "Episode: 344, Total Reward: 1.799999999999999, Epsilon: 0.1783\n",
      "Episode: 345, Total Reward: -0.8999999999999986, Epsilon: 0.1774\n",
      "Episode: 346, Total Reward: -0.8999999999999986, Epsilon: 0.1765\n",
      "Episode: 347, Total Reward: -0.8999999999999986, Epsilon: 0.1756\n",
      "Episode: 348, Total Reward: -0.29999999999999893, Epsilon: 0.1748\n",
      "Episode: 349, Total Reward: -0.8999999999999986, Epsilon: 0.1739\n",
      "Episode: 350, Total Reward: -0.8999999999999986, Epsilon: 0.1730\n",
      "Episode: 351, Total Reward: -0.8999999999999986, Epsilon: 0.1721\n",
      "Episode: 352, Total Reward: -1.3999999999999986, Epsilon: 0.1713\n",
      "Episode: 353, Total Reward: -0.8999999999999986, Epsilon: 0.1704\n",
      "Episode: 354, Total Reward: -0.8999999999999986, Epsilon: 0.1696\n",
      "Episode: 355, Total Reward: -3.299999999999998, Epsilon: 0.1687\n",
      "Episode: 356, Total Reward: -0.8999999999999986, Epsilon: 0.1679\n",
      "Episode: 357, Total Reward: -2.099999999999998, Epsilon: 0.1670\n",
      "Episode: 358, Total Reward: -2.0999999999999988, Epsilon: 0.1662\n",
      "Episode: 359, Total Reward: -0.8999999999999986, Epsilon: 0.1654\n",
      "Episode: 360, Total Reward: -0.8999999999999986, Epsilon: 0.1646\n",
      "Episode: 361, Total Reward: -2.3999999999999986, Epsilon: 0.1637\n",
      "Episode: 362, Total Reward: -0.8999999999999986, Epsilon: 0.1629\n",
      "Episode: 363, Total Reward: -0.8999999999999986, Epsilon: 0.1621\n",
      "Episode: 364, Total Reward: -2.099999999999998, Epsilon: 0.1613\n",
      "Episode: 365, Total Reward: -0.8999999999999986, Epsilon: 0.1605\n",
      "Episode: 366, Total Reward: 0.9000000000000021, Epsilon: 0.1597\n",
      "Episode: 367, Total Reward: 2.6999999999999993, Epsilon: 0.1589\n",
      "Episode: 368, Total Reward: 2.1999999999999993, Epsilon: 0.1581\n",
      "Episode: 369, Total Reward: -0.8999999999999986, Epsilon: 0.1573\n",
      "Episode: 370, Total Reward: -0.8999999999999986, Epsilon: 0.1565\n",
      "Episode: 371, Total Reward: -0.8999999999999986, Epsilon: 0.1557\n",
      "Episode: 372, Total Reward: -0.8999999999999986, Epsilon: 0.1549\n",
      "Episode: 373, Total Reward: 0.800000000000002, Epsilon: 0.1542\n",
      "Episode: 374, Total Reward: -0.8999999999999986, Epsilon: 0.1534\n",
      "Episode: 375, Total Reward: 2.0999999999999996, Epsilon: 0.1526\n",
      "Episode: 376, Total Reward: -0.699999999999998, Epsilon: 0.1519\n",
      "Episode: 377, Total Reward: -0.8999999999999986, Epsilon: 0.1511\n",
      "Episode: 378, Total Reward: -2.099999999999998, Epsilon: 0.1504\n",
      "Episode: 379, Total Reward: 0.9000000000000004, Epsilon: 0.1496\n",
      "Episode: 380, Total Reward: -2.099999999999998, Epsilon: 0.1489\n",
      "Episode: 381, Total Reward: -0.8999999999999986, Epsilon: 0.1481\n",
      "Episode: 382, Total Reward: -0.8999999999999986, Epsilon: 0.1474\n",
      "Episode: 383, Total Reward: -0.399999999999999, Epsilon: 0.1466\n",
      "Episode: 384, Total Reward: -0.8999999999999986, Epsilon: 0.1459\n",
      "Episode: 385, Total Reward: -0.8999999999999986, Epsilon: 0.1452\n",
      "Episode: 386, Total Reward: -0.8999999999999986, Epsilon: 0.1444\n",
      "Episode: 387, Total Reward: -0.8999999999999986, Epsilon: 0.1437\n",
      "Episode: 388, Total Reward: -2.699999999999998, Epsilon: 0.1430\n",
      "Episode: 389, Total Reward: -0.8999999999999986, Epsilon: 0.1423\n",
      "Episode: 390, Total Reward: 0.3000000000000007, Epsilon: 0.1416\n",
      "Episode: 391, Total Reward: -0.8999999999999986, Epsilon: 0.1409\n",
      "Episode: 392, Total Reward: -0.8999999999999986, Epsilon: 0.1402\n",
      "Episode: 393, Total Reward: 2.299999999999999, Epsilon: 0.1395\n",
      "Episode: 394, Total Reward: -0.8999999999999986, Epsilon: 0.1388\n",
      "Episode: 395, Total Reward: -0.8999999999999986, Epsilon: 0.1381\n",
      "Episode: 396, Total Reward: -0.8999999999999986, Epsilon: 0.1374\n",
      "Episode: 397, Total Reward: -0.8999999999999986, Epsilon: 0.1367\n",
      "Episode: 398, Total Reward: -3.899999999999998, Epsilon: 0.1360\n",
      "Episode: 399, Total Reward: -1.4999999999999982, Epsilon: 0.1353\n",
      "Episode: 400, Total Reward: -0.8999999999999986, Epsilon: 0.1347\n",
      "Episode: 401, Total Reward: 0.40000000000000213, Epsilon: 0.1340\n",
      "Episode: 402, Total Reward: -0.8999999999999986, Epsilon: 0.1333\n",
      "Episode: 403, Total Reward: -0.8999999999999986, Epsilon: 0.1326\n",
      "Episode: 404, Total Reward: -0.8999999999999986, Epsilon: 0.1320\n",
      "Episode: 405, Total Reward: -0.8999999999999986, Epsilon: 0.1313\n",
      "Episode: 406, Total Reward: -0.8999999999999986, Epsilon: 0.1307\n",
      "Episode: 407, Total Reward: -0.8999999999999986, Epsilon: 0.1300\n",
      "Episode: 408, Total Reward: -0.8999999999999986, Epsilon: 0.1294\n",
      "Episode: 409, Total Reward: 2.299999999999999, Epsilon: 0.1287\n",
      "Episode: 410, Total Reward: -0.8999999999999986, Epsilon: 0.1281\n",
      "Episode: 411, Total Reward: 2.299999999999999, Epsilon: 0.1274\n",
      "Episode: 412, Total Reward: 2.799999999999999, Epsilon: 0.1268\n",
      "Episode: 413, Total Reward: -0.8999999999999986, Epsilon: 0.1262\n",
      "Episode: 414, Total Reward: 0.9000000000000004, Epsilon: 0.1255\n",
      "Episode: 415, Total Reward: -0.8999999999999986, Epsilon: 0.1249\n",
      "Episode: 416, Total Reward: -0.8999999999999986, Epsilon: 0.1243\n",
      "Episode: 417, Total Reward: -0.8999999999999986, Epsilon: 0.1237\n",
      "Episode: 418, Total Reward: -0.8999999999999986, Epsilon: 0.1230\n",
      "Episode: 419, Total Reward: -0.8999999999999986, Epsilon: 0.1224\n",
      "Episode: 420, Total Reward: 2.0999999999999996, Epsilon: 0.1218\n",
      "Episode: 421, Total Reward: -0.8999999999999986, Epsilon: 0.1212\n",
      "Episode: 422, Total Reward: 2.299999999999999, Epsilon: 0.1206\n",
      "Episode: 423, Total Reward: 1.9999999999999982, Epsilon: 0.1200\n",
      "Episode: 424, Total Reward: -0.8999999999999986, Epsilon: 0.1194\n",
      "Episode: 425, Total Reward: -3.299999999999998, Epsilon: 0.1188\n",
      "Episode: 426, Total Reward: -0.8999999999999986, Epsilon: 0.1182\n",
      "Episode: 427, Total Reward: -0.8999999999999986, Epsilon: 0.1176\n",
      "Episode: 428, Total Reward: -0.8999999999999986, Epsilon: 0.1170\n",
      "Episode: 429, Total Reward: -0.8999999999999986, Epsilon: 0.1164\n",
      "Episode: 430, Total Reward: -0.8999999999999986, Epsilon: 0.1159\n",
      "Episode: 431, Total Reward: -0.8999999999999986, Epsilon: 0.1153\n",
      "Episode: 432, Total Reward: 2.799999999999999, Epsilon: 0.1147\n",
      "Episode: 433, Total Reward: -1.4999999999999982, Epsilon: 0.1141\n",
      "Episode: 434, Total Reward: -0.8999999999999986, Epsilon: 0.1136\n",
      "Episode: 435, Total Reward: -0.8999999999999986, Epsilon: 0.1130\n",
      "Episode: 436, Total Reward: -0.8999999999999986, Epsilon: 0.1124\n",
      "Episode: 437, Total Reward: 3.0999999999999996, Epsilon: 0.1119\n",
      "Episode: 438, Total Reward: -0.8999999999999986, Epsilon: 0.1113\n",
      "Episode: 439, Total Reward: -4.499999999999998, Epsilon: 0.1107\n",
      "Episode: 440, Total Reward: -0.8999999999999986, Epsilon: 0.1102\n",
      "Episode: 441, Total Reward: -1.4999999999999982, Epsilon: 0.1096\n",
      "Episode: 442, Total Reward: -0.8999999999999986, Epsilon: 0.1091\n",
      "Episode: 443, Total Reward: -0.09999999999999787, Epsilon: 0.1085\n",
      "Episode: 444, Total Reward: 2.1000000000000014, Epsilon: 0.1080\n",
      "Episode: 445, Total Reward: 0.9000000000000004, Epsilon: 0.1075\n",
      "Episode: 446, Total Reward: 0.9000000000000004, Epsilon: 0.1069\n",
      "Episode: 447, Total Reward: 2.0999999999999996, Epsilon: 0.1064\n",
      "Episode: 448, Total Reward: -0.8999999999999986, Epsilon: 0.1059\n",
      "Episode: 449, Total Reward: -0.8999999999999986, Epsilon: 0.1053\n",
      "Episode: 450, Total Reward: -0.09999999999999787, Epsilon: 0.1048\n",
      "Episode: 451, Total Reward: -0.8999999999999986, Epsilon: 0.1043\n",
      "Episode: 452, Total Reward: -0.8999999999999986, Epsilon: 0.1038\n",
      "Episode: 453, Total Reward: 0.9000000000000004, Epsilon: 0.1032\n",
      "Episode: 454, Total Reward: -0.8999999999999986, Epsilon: 0.1027\n",
      "Episode: 455, Total Reward: -0.8999999999999986, Epsilon: 0.1022\n",
      "Episode: 456, Total Reward: 0.3000000000000007, Epsilon: 0.1017\n",
      "Episode: 457, Total Reward: 2.4000000000000004, Epsilon: 0.1012\n",
      "Episode: 458, Total Reward: -0.8999999999999986, Epsilon: 0.1007\n",
      "Episode: 459, Total Reward: -0.8999999999999986, Epsilon: 0.1002\n",
      "Episode: 460, Total Reward: -0.8999999999999986, Epsilon: 0.0997\n",
      "Episode: 461, Total Reward: -0.8999999999999986, Epsilon: 0.0992\n",
      "Episode: 462, Total Reward: 2.6999999999999993, Epsilon: 0.0987\n",
      "Episode: 463, Total Reward: -0.8999999999999986, Epsilon: 0.0982\n",
      "Episode: 464, Total Reward: -1.0999999999999988, Epsilon: 0.0977\n",
      "Episode: 465, Total Reward: 0.3000000000000007, Epsilon: 0.0972\n",
      "Episode: 466, Total Reward: 2.0999999999999996, Epsilon: 0.0967\n",
      "Episode: 467, Total Reward: -0.8999999999999986, Epsilon: 0.0962\n",
      "Episode: 468, Total Reward: -0.8999999999999986, Epsilon: 0.0958\n",
      "Episode: 469, Total Reward: 1.5000000000000018, Epsilon: 0.0953\n",
      "Episode: 470, Total Reward: -3.3999999999999986, Epsilon: 0.0948\n",
      "Episode: 471, Total Reward: 1.9999999999999982, Epsilon: 0.0943\n",
      "Episode: 472, Total Reward: -0.8999999999999986, Epsilon: 0.0939\n",
      "Episode: 473, Total Reward: -0.8999999999999986, Epsilon: 0.0934\n",
      "Episode: 474, Total Reward: 2.1000000000000014, Epsilon: 0.0929\n",
      "Episode: 475, Total Reward: -0.8999999999999986, Epsilon: 0.0925\n",
      "Episode: 476, Total Reward: 1.7999999999999998, Epsilon: 0.0920\n",
      "Episode: 477, Total Reward: 2.6999999999999993, Epsilon: 0.0915\n",
      "Episode: 478, Total Reward: -1.099999999999998, Epsilon: 0.0911\n",
      "Episode: 479, Total Reward: -0.8999999999999986, Epsilon: 0.0906\n",
      "Episode: 480, Total Reward: -0.8999999999999986, Epsilon: 0.0902\n",
      "Episode: 481, Total Reward: -0.8999999999999986, Epsilon: 0.0897\n",
      "Episode: 482, Total Reward: -0.8999999999999986, Epsilon: 0.0893\n",
      "Episode: 483, Total Reward: -0.8999999999999986, Epsilon: 0.0888\n",
      "Episode: 484, Total Reward: -0.8999999999999986, Epsilon: 0.0884\n",
      "Episode: 485, Total Reward: -4.499999999999998, Epsilon: 0.0879\n",
      "Episode: 486, Total Reward: -0.8999999999999986, Epsilon: 0.0875\n",
      "Episode: 487, Total Reward: 2.6999999999999993, Epsilon: 0.0871\n",
      "Episode: 488, Total Reward: -0.8999999999999986, Epsilon: 0.0866\n",
      "Episode: 489, Total Reward: -0.8999999999999986, Epsilon: 0.0862\n",
      "Episode: 490, Total Reward: -0.8999999999999986, Epsilon: 0.0858\n",
      "Episode: 491, Total Reward: -0.8999999999999986, Epsilon: 0.0853\n",
      "Episode: 492, Total Reward: 0.800000000000002, Epsilon: 0.0849\n",
      "Episode: 493, Total Reward: -0.8999999999999986, Epsilon: 0.0845\n",
      "Episode: 494, Total Reward: -2.699999999999998, Epsilon: 0.0841\n",
      "Episode: 495, Total Reward: -0.29999999999999893, Epsilon: 0.0836\n",
      "Episode: 496, Total Reward: 1.4000000000000026, Epsilon: 0.0832\n",
      "Episode: 497, Total Reward: 1.1999999999999913, Epsilon: 0.0828\n",
      "Episode: 498, Total Reward: -0.5999999999999979, Epsilon: 0.0824\n",
      "Episode: 499, Total Reward: -0.8999999999999986, Epsilon: 0.0820\n",
      "Episode: 500, Total Reward: 0.9000000000000004, Epsilon: 0.0816\n",
      "Episode: 501, Total Reward: -0.8999999999999986, Epsilon: 0.0812\n",
      "Episode: 502, Total Reward: -0.7999999999999989, Epsilon: 0.0808\n",
      "Episode: 503, Total Reward: -1.4999999999999982, Epsilon: 0.0804\n",
      "Episode: 504, Total Reward: -0.8999999999999986, Epsilon: 0.0800\n",
      "Episode: 505, Total Reward: -2.3999999999999977, Epsilon: 0.0796\n",
      "Episode: 506, Total Reward: -0.8999999999999986, Epsilon: 0.0792\n",
      "Episode: 507, Total Reward: 1.9999999999999982, Epsilon: 0.0788\n",
      "Episode: 508, Total Reward: 2.299999999999999, Epsilon: 0.0784\n",
      "Episode: 509, Total Reward: -0.8999999999999986, Epsilon: 0.0780\n",
      "Episode: 510, Total Reward: 2.6999999999999993, Epsilon: 0.0776\n",
      "Episode: 511, Total Reward: -3.5999999999999988, Epsilon: 0.0772\n",
      "Episode: 512, Total Reward: 0.8999999999999986, Epsilon: 0.0768\n",
      "Episode: 513, Total Reward: 1.9999999999999982, Epsilon: 0.0764\n",
      "Episode: 514, Total Reward: -0.8999999999999986, Epsilon: 0.0760\n",
      "Episode: 515, Total Reward: -0.8999999999999986, Epsilon: 0.0757\n",
      "Episode: 516, Total Reward: -0.8999999999999986, Epsilon: 0.0753\n",
      "Episode: 517, Total Reward: 2.0999999999999996, Epsilon: 0.0749\n",
      "Episode: 518, Total Reward: -0.8999999999999986, Epsilon: 0.0745\n",
      "Episode: 519, Total Reward: -0.8999999999999986, Epsilon: 0.0742\n",
      "Episode: 520, Total Reward: -0.8999999999999986, Epsilon: 0.0738\n",
      "Episode: 521, Total Reward: -0.8999999999999986, Epsilon: 0.0734\n",
      "Episode: 522, Total Reward: -3.299999999999998, Epsilon: 0.0731\n",
      "Episode: 523, Total Reward: -0.49999999999999867, Epsilon: 0.0727\n",
      "Episode: 524, Total Reward: -0.8999999999999986, Epsilon: 0.0723\n",
      "Episode: 525, Total Reward: 1.8999999999999986, Epsilon: 0.0720\n",
      "Episode: 526, Total Reward: -0.6000000000000023, Epsilon: 0.0716\n",
      "Episode: 527, Total Reward: 0.3000000000000007, Epsilon: 0.0712\n",
      "Episode: 528, Total Reward: 0.7000000000000024, Epsilon: 0.0709\n",
      "Episode: 529, Total Reward: -0.8999999999999986, Epsilon: 0.0705\n",
      "Episode: 530, Total Reward: 0.5999999999999917, Epsilon: 0.0702\n",
      "Episode: 531, Total Reward: -1.0999999999999988, Epsilon: 0.0698\n",
      "Episode: 532, Total Reward: -0.8999999999999986, Epsilon: 0.0695\n",
      "Episode: 533, Total Reward: -2.7999999999999985, Epsilon: 0.0691\n",
      "Episode: 534, Total Reward: -0.8999999999999986, Epsilon: 0.0688\n",
      "Episode: 535, Total Reward: -0.8999999999999986, Epsilon: 0.0684\n",
      "Episode: 536, Total Reward: -0.8999999999999986, Epsilon: 0.0681\n",
      "Episode: 537, Total Reward: -0.8999999999999986, Epsilon: 0.0678\n",
      "Episode: 538, Total Reward: -1.1999999999999975, Epsilon: 0.0674\n",
      "Episode: 539, Total Reward: 0.3000000000000007, Epsilon: 0.0671\n",
      "Episode: 540, Total Reward: -0.8999999999999986, Epsilon: 0.0668\n",
      "Episode: 541, Total Reward: -0.8999999999999986, Epsilon: 0.0664\n",
      "Episode: 542, Total Reward: -0.8999999999999986, Epsilon: 0.0661\n",
      "Episode: 543, Total Reward: 3.599999999999996, Epsilon: 0.0658\n",
      "Episode: 544, Total Reward: -0.8999999999999986, Epsilon: 0.0654\n",
      "Episode: 545, Total Reward: -0.8999999999999986, Epsilon: 0.0651\n",
      "Episode: 546, Total Reward: -0.8999999999999986, Epsilon: 0.0648\n",
      "Episode: 547, Total Reward: -0.8999999999999986, Epsilon: 0.0645\n",
      "Episode: 548, Total Reward: -1.0999999999999985, Epsilon: 0.0641\n",
      "Episode: 549, Total Reward: -0.29999999999999893, Epsilon: 0.0638\n",
      "Episode: 550, Total Reward: -0.8999999999999986, Epsilon: 0.0635\n",
      "Episode: 551, Total Reward: 2.6999999999999975, Epsilon: 0.0632\n",
      "Episode: 552, Total Reward: -0.8999999999999986, Epsilon: 0.0629\n",
      "Episode: 553, Total Reward: -2.3999999999999964, Epsilon: 0.0625\n",
      "Episode: 554, Total Reward: -0.8999999999999986, Epsilon: 0.0622\n",
      "Episode: 555, Total Reward: -2.1999999999999984, Epsilon: 0.0619\n",
      "Episode: 556, Total Reward: -0.8999999999999986, Epsilon: 0.0616\n",
      "Episode: 557, Total Reward: -0.8999999999999986, Epsilon: 0.0613\n",
      "Episode: 558, Total Reward: -0.8999999999999986, Epsilon: 0.0610\n",
      "Episode: 559, Total Reward: 1.8999999999999995, Epsilon: 0.0607\n",
      "Episode: 560, Total Reward: -0.8999999999999986, Epsilon: 0.0604\n",
      "Episode: 561, Total Reward: -0.8999999999999986, Epsilon: 0.0601\n",
      "Episode: 562, Total Reward: -0.8999999999999986, Epsilon: 0.0598\n",
      "Episode: 563, Total Reward: -0.8999999999999986, Epsilon: 0.0595\n",
      "Episode: 564, Total Reward: -0.8999999999999986, Epsilon: 0.0592\n",
      "Episode: 565, Total Reward: -0.8999999999999986, Epsilon: 0.0589\n",
      "Episode: 566, Total Reward: -0.8999999999999986, Epsilon: 0.0586\n",
      "Episode: 567, Total Reward: -0.49999999999999734, Epsilon: 0.0583\n",
      "Episode: 568, Total Reward: -0.8999999999999986, Epsilon: 0.0580\n",
      "Episode: 569, Total Reward: -0.8999999999999986, Epsilon: 0.0577\n",
      "Episode: 570, Total Reward: -0.8999999999999986, Epsilon: 0.0574\n",
      "Episode: 571, Total Reward: -0.8999999999999986, Epsilon: 0.0571\n",
      "Episode: 572, Total Reward: 1.0000000000000036, Epsilon: 0.0569\n",
      "Episode: 573, Total Reward: -0.8999999999999986, Epsilon: 0.0566\n",
      "Episode: 574, Total Reward: -2.399999999999999, Epsilon: 0.0563\n",
      "Episode: 575, Total Reward: -0.8999999999999986, Epsilon: 0.0560\n",
      "Episode: 576, Total Reward: -0.8999999999999986, Epsilon: 0.0557\n",
      "Episode: 577, Total Reward: -0.8999999999999986, Epsilon: 0.0555\n",
      "Episode: 578, Total Reward: -0.8999999999999986, Epsilon: 0.0552\n",
      "Episode: 579, Total Reward: 2.0000000000000018, Epsilon: 0.0549\n",
      "Episode: 580, Total Reward: -1.0999999999999988, Epsilon: 0.0546\n",
      "Episode: 581, Total Reward: -0.8999999999999986, Epsilon: 0.0544\n",
      "Episode: 582, Total Reward: -0.29999999999999893, Epsilon: 0.0541\n",
      "Episode: 583, Total Reward: 0.9000000000000004, Epsilon: 0.0538\n",
      "Episode: 584, Total Reward: -0.8999999999999986, Epsilon: 0.0535\n",
      "Episode: 585, Total Reward: -0.8999999999999986, Epsilon: 0.0533\n",
      "Episode: 586, Total Reward: -0.8999999999999986, Epsilon: 0.0530\n",
      "Episode: 587, Total Reward: -0.8999999999999986, Epsilon: 0.0527\n",
      "Episode: 588, Total Reward: 0.9000000000000004, Epsilon: 0.0525\n",
      "Episode: 589, Total Reward: 0.9000000000000004, Epsilon: 0.0522\n",
      "Episode: 590, Total Reward: 0.9000000000000004, Epsilon: 0.0520\n",
      "Episode: 591, Total Reward: -0.8999999999999986, Epsilon: 0.0517\n",
      "Episode: 592, Total Reward: -0.8999999999999986, Epsilon: 0.0514\n",
      "Episode: 593, Total Reward: -0.5999999999999983, Epsilon: 0.0512\n",
      "Episode: 594, Total Reward: -0.8999999999999986, Epsilon: 0.0509\n",
      "Episode: 595, Total Reward: -0.8999999999999986, Epsilon: 0.0507\n",
      "Episode: 596, Total Reward: -0.8999999999999986, Epsilon: 0.0504\n",
      "Episode: 597, Total Reward: -0.8999999999999986, Epsilon: 0.0502\n",
      "Episode: 598, Total Reward: -1.1999999999999988, Epsilon: 0.0499\n",
      "Episode: 599, Total Reward: 0.10000000000000098, Epsilon: 0.0497\n",
      "Episode: 600, Total Reward: -0.8999999999999986, Epsilon: 0.0494\n",
      "Episode: 601, Total Reward: -0.8999999999999986, Epsilon: 0.0492\n",
      "Episode: 602, Total Reward: 0.3000000000000007, Epsilon: 0.0489\n",
      "Episode: 603, Total Reward: -0.3999999999999986, Epsilon: 0.0487\n",
      "Episode: 604, Total Reward: 2.6999999999999993, Epsilon: 0.0484\n",
      "Episode: 605, Total Reward: -0.09999999999999964, Epsilon: 0.0482\n",
      "Episode: 606, Total Reward: -2.299999999999999, Epsilon: 0.0479\n",
      "Episode: 607, Total Reward: 0.9000000000000004, Epsilon: 0.0477\n",
      "Episode: 608, Total Reward: -3.899999999999998, Epsilon: 0.0475\n",
      "Episode: 609, Total Reward: -0.29999999999999893, Epsilon: 0.0472\n",
      "Episode: 610, Total Reward: -0.29999999999999893, Epsilon: 0.0470\n",
      "Episode: 611, Total Reward: -1.4999999999999982, Epsilon: 0.0468\n",
      "Episode: 612, Total Reward: 0.3000000000000007, Epsilon: 0.0465\n",
      "Episode: 613, Total Reward: 0.20000000000000195, Epsilon: 0.0463\n",
      "Episode: 614, Total Reward: 0.10000000000000142, Epsilon: 0.0461\n",
      "Episode: 615, Total Reward: -0.8999999999999986, Epsilon: 0.0458\n",
      "Episode: 616, Total Reward: -0.8999999999999986, Epsilon: 0.0456\n",
      "Episode: 617, Total Reward: 0.6000000000000032, Epsilon: 0.0454\n",
      "Episode: 618, Total Reward: -0.699999999999998, Epsilon: 0.0452\n",
      "Episode: 619, Total Reward: 1.9000000000000021, Epsilon: 0.0449\n",
      "Episode: 620, Total Reward: 0.9000000000000004, Epsilon: 0.0447\n",
      "Episode: 621, Total Reward: -1.0999999999999988, Epsilon: 0.0445\n",
      "Episode: 622, Total Reward: 0.3000000000000007, Epsilon: 0.0443\n",
      "Episode: 623, Total Reward: -0.8999999999999986, Epsilon: 0.0440\n",
      "Episode: 624, Total Reward: 0.30000000000000115, Epsilon: 0.0438\n",
      "Episode: 625, Total Reward: -0.8999999999999986, Epsilon: 0.0436\n",
      "Episode: 626, Total Reward: -1.4999999999999987, Epsilon: 0.0434\n",
      "Episode: 627, Total Reward: 0.3000000000000007, Epsilon: 0.0432\n",
      "Episode: 628, Total Reward: 0.9000000000000017, Epsilon: 0.0429\n",
      "Episode: 629, Total Reward: -0.8999999999999986, Epsilon: 0.0427\n",
      "Episode: 630, Total Reward: 0.6000000000000014, Epsilon: 0.0425\n",
      "Episode: 631, Total Reward: -0.8999999999999986, Epsilon: 0.0423\n",
      "Episode: 632, Total Reward: -0.3999999999999986, Epsilon: 0.0421\n",
      "Episode: 633, Total Reward: -0.09999999999999787, Epsilon: 0.0419\n",
      "Episode: 634, Total Reward: -0.8999999999999986, Epsilon: 0.0417\n",
      "Episode: 635, Total Reward: 0.8000000000000012, Epsilon: 0.0415\n",
      "Episode: 636, Total Reward: 0.5000000000000022, Epsilon: 0.0413\n",
      "Episode: 637, Total Reward: -0.8999999999999986, Epsilon: 0.0410\n",
      "Episode: 638, Total Reward: -6.899999999999999, Epsilon: 0.0408\n",
      "Episode: 639, Total Reward: -0.8999999999999986, Epsilon: 0.0406\n",
      "Episode: 640, Total Reward: -0.5999999999999979, Epsilon: 0.0404\n",
      "Episode: 641, Total Reward: -2.699999999999998, Epsilon: 0.0402\n",
      "Episode: 642, Total Reward: 2.599999999999996, Epsilon: 0.0400\n",
      "Episode: 643, Total Reward: -2.399999999999998, Epsilon: 0.0398\n",
      "Episode: 644, Total Reward: -0.8999999999999986, Epsilon: 0.0396\n",
      "Episode: 645, Total Reward: 3.099999999999998, Epsilon: 0.0394\n",
      "Episode: 646, Total Reward: -0.8999999999999986, Epsilon: 0.0392\n",
      "Episode: 647, Total Reward: -3.899999999999997, Epsilon: 0.0390\n",
      "Episode: 648, Total Reward: -0.8999999999999986, Epsilon: 0.0388\n",
      "Episode: 649, Total Reward: -0.8999999999999986, Epsilon: 0.0387\n",
      "Episode: 650, Total Reward: -1.3999999999999986, Epsilon: 0.0385\n",
      "Episode: 651, Total Reward: 1.800000000000002, Epsilon: 0.0383\n",
      "Episode: 652, Total Reward: -1.999999999999999, Epsilon: 0.0381\n",
      "Episode: 653, Total Reward: -3.999999999999998, Epsilon: 0.0379\n",
      "Episode: 654, Total Reward: -2.3999999999999986, Epsilon: 0.0377\n",
      "Episode: 655, Total Reward: -1.7999999999999972, Epsilon: 0.0375\n",
      "Episode: 656, Total Reward: 2.3999999999999897, Epsilon: 0.0373\n",
      "Episode: 657, Total Reward: -0.8999999999999986, Epsilon: 0.0371\n",
      "Episode: 658, Total Reward: 1.4000000000000021, Epsilon: 0.0369\n",
      "Episode: 659, Total Reward: 3.099999999999998, Epsilon: 0.0368\n",
      "Episode: 660, Total Reward: 1.5000000000000027, Epsilon: 0.0366\n",
      "Episode: 661, Total Reward: 2.0000000000000018, Epsilon: 0.0364\n",
      "Episode: 662, Total Reward: 3.099999999999998, Epsilon: 0.0362\n",
      "Episode: 663, Total Reward: -4.499999999999998, Epsilon: 0.0360\n",
      "Episode: 664, Total Reward: -0.9999999999999987, Epsilon: 0.0359\n",
      "Episode: 665, Total Reward: 3.1999999999999975, Epsilon: 0.0357\n",
      "Episode: 666, Total Reward: 3.099999999999998, Epsilon: 0.0355\n",
      "Episode: 667, Total Reward: -2.0999999999999988, Epsilon: 0.0353\n",
      "Episode: 668, Total Reward: 1.800000000000002, Epsilon: 0.0351\n",
      "Episode: 669, Total Reward: -1.499999999999996, Epsilon: 0.0350\n",
      "Episode: 670, Total Reward: -0.29999999999999893, Epsilon: 0.0348\n",
      "Episode: 671, Total Reward: 1.9999999999999982, Epsilon: 0.0346\n",
      "Episode: 672, Total Reward: 1.0000000000000018, Epsilon: 0.0344\n",
      "Episode: 673, Total Reward: -1.6999999999999988, Epsilon: 0.0343\n",
      "Episode: 674, Total Reward: 2.6000000000000014, Epsilon: 0.0341\n",
      "Episode: 675, Total Reward: -2.299999999999999, Epsilon: 0.0339\n",
      "Episode: 676, Total Reward: -2.099999999999998, Epsilon: 0.0338\n",
      "Episode: 677, Total Reward: -0.8999999999999986, Epsilon: 0.0336\n",
      "Episode: 678, Total Reward: 2.4999999999999982, Epsilon: 0.0334\n",
      "Episode: 679, Total Reward: 2.4999999999999982, Epsilon: 0.0333\n",
      "Episode: 680, Total Reward: -0.8999999999999986, Epsilon: 0.0331\n",
      "Episode: 681, Total Reward: -0.8999999999999986, Epsilon: 0.0329\n",
      "Episode: 682, Total Reward: 4.899999999999995, Epsilon: 0.0328\n",
      "Episode: 683, Total Reward: 1.6000000000000023, Epsilon: 0.0326\n",
      "Episode: 684, Total Reward: -0.8999999999999986, Epsilon: 0.0324\n",
      "Episode: 685, Total Reward: 1.7000000000000024, Epsilon: 0.0323\n",
      "Episode: 686, Total Reward: 1.6999999999999993, Epsilon: 0.0321\n",
      "Episode: 687, Total Reward: 2.8999999999999986, Epsilon: 0.0319\n",
      "Episode: 688, Total Reward: -0.8999999999999986, Epsilon: 0.0318\n",
      "Episode: 689, Total Reward: 2.0000000000000036, Epsilon: 0.0316\n",
      "Episode: 690, Total Reward: -0.8999999999999986, Epsilon: 0.0315\n",
      "Episode: 691, Total Reward: 2.9999999999999982, Epsilon: 0.0313\n",
      "Episode: 692, Total Reward: -0.8999999999999986, Epsilon: 0.0312\n",
      "Episode: 693, Total Reward: -2.4999999999999987, Epsilon: 0.0310\n",
      "Episode: 694, Total Reward: -0.8999999999999986, Epsilon: 0.0308\n",
      "Episode: 695, Total Reward: 0.4000000000000017, Epsilon: 0.0307\n",
      "Episode: 696, Total Reward: -0.8999999999999986, Epsilon: 0.0305\n",
      "Episode: 697, Total Reward: -0.8999999999999986, Epsilon: 0.0304\n",
      "Episode: 698, Total Reward: -0.9999999999999987, Epsilon: 0.0302\n",
      "Episode: 699, Total Reward: 0.30000000000000204, Epsilon: 0.0301\n",
      "Episode: 700, Total Reward: -0.8999999999999986, Epsilon: 0.0299\n",
      "Episode: 701, Total Reward: -1.6999999999999988, Epsilon: 0.0298\n",
      "Episode: 702, Total Reward: -0.8999999999999986, Epsilon: 0.0296\n",
      "Episode: 703, Total Reward: -0.29999999999999893, Epsilon: 0.0295\n",
      "Episode: 704, Total Reward: 2.3999999999999986, Epsilon: 0.0293\n",
      "Episode: 705, Total Reward: 6.7999999999999865, Epsilon: 0.0292\n",
      "Episode: 706, Total Reward: 3.099999999999998, Epsilon: 0.0290\n",
      "Episode: 707, Total Reward: 1.7999999999999994, Epsilon: 0.0289\n",
      "Episode: 708, Total Reward: -1.5999999999999985, Epsilon: 0.0288\n",
      "Episode: 709, Total Reward: -1.0999999999999983, Epsilon: 0.0286\n",
      "Episode: 710, Total Reward: 3.1999999999999975, Epsilon: 0.0285\n",
      "Episode: 711, Total Reward: 1.5, Epsilon: 0.0283\n",
      "Episode: 712, Total Reward: -0.5999999999999986, Epsilon: 0.0282\n",
      "Episode: 713, Total Reward: -0.5999999999999983, Epsilon: 0.0280\n",
      "Episode: 714, Total Reward: -0.8999999999999986, Epsilon: 0.0279\n",
      "Episode: 715, Total Reward: 3.899999999999988, Epsilon: 0.0278\n",
      "Episode: 716, Total Reward: -0.8999999999999986, Epsilon: 0.0276\n",
      "Episode: 717, Total Reward: 0.9000000000000021, Epsilon: 0.0275\n",
      "Episode: 718, Total Reward: 4.999999999999995, Epsilon: 0.0274\n",
      "Episode: 719, Total Reward: 1.4000000000000021, Epsilon: 0.0272\n",
      "Episode: 720, Total Reward: -1.4999999999999987, Epsilon: 0.0271\n",
      "Episode: 721, Total Reward: -0.8999999999999986, Epsilon: 0.0269\n",
      "Episode: 722, Total Reward: 2.1999999999999993, Epsilon: 0.0268\n",
      "Episode: 723, Total Reward: 3.9999999999999956, Epsilon: 0.0267\n",
      "Episode: 724, Total Reward: -0.49999999999999867, Epsilon: 0.0265\n",
      "Episode: 725, Total Reward: -0.8999999999999986, Epsilon: 0.0264\n",
      "Episode: 726, Total Reward: 2.1999999999999993, Epsilon: 0.0263\n",
      "Episode: 727, Total Reward: 2.3999999999999986, Epsilon: 0.0261\n",
      "Episode: 728, Total Reward: 1.1999999999999993, Epsilon: 0.0260\n",
      "Episode: 729, Total Reward: 1.299999999999999, Epsilon: 0.0259\n",
      "Episode: 730, Total Reward: -0.8999999999999986, Epsilon: 0.0258\n",
      "Episode: 731, Total Reward: 0.8999999999999995, Epsilon: 0.0256\n",
      "Episode: 732, Total Reward: -1.3999999999999986, Epsilon: 0.0255\n",
      "Episode: 733, Total Reward: 2.499999999999999, Epsilon: 0.0254\n",
      "Episode: 734, Total Reward: 3.9999999999999947, Epsilon: 0.0252\n",
      "Episode: 735, Total Reward: 4.399999999999995, Epsilon: 0.0251\n",
      "Episode: 736, Total Reward: 2.0999999999999996, Epsilon: 0.0250\n",
      "Episode: 737, Total Reward: 2.0000000000000018, Epsilon: 0.0249\n",
      "Episode: 738, Total Reward: -0.29999999999999893, Epsilon: 0.0247\n",
      "Episode: 739, Total Reward: 3.099999999999998, Epsilon: 0.0246\n",
      "Episode: 740, Total Reward: 2.9999999999999964, Epsilon: 0.0245\n",
      "Episode: 741, Total Reward: -0.8999999999999986, Epsilon: 0.0244\n",
      "Episode: 742, Total Reward: 2.0999999999999996, Epsilon: 0.0243\n",
      "Episode: 743, Total Reward: 4.199999999999996, Epsilon: 0.0241\n",
      "Episode: 744, Total Reward: 3.1999999999999966, Epsilon: 0.0240\n",
      "Episode: 745, Total Reward: 2.9999999999999982, Epsilon: 0.0239\n",
      "Episode: 746, Total Reward: -0.9999999999999978, Epsilon: 0.0238\n",
      "Episode: 747, Total Reward: 0.10000000000000142, Epsilon: 0.0237\n",
      "Episode: 748, Total Reward: -0.8999999999999986, Epsilon: 0.0235\n",
      "Episode: 749, Total Reward: 4.399999999999988, Epsilon: 0.0234\n",
      "Episode: 750, Total Reward: 4.899999999999995, Epsilon: 0.0233\n",
      "Episode: 751, Total Reward: 2.799999999999999, Epsilon: 0.0232\n",
      "Episode: 752, Total Reward: 2.0999999999999996, Epsilon: 0.0231\n",
      "Episode: 753, Total Reward: 2.6999999999999993, Epsilon: 0.0229\n",
      "Episode: 754, Total Reward: 1.3999999999999986, Epsilon: 0.0228\n",
      "Episode: 755, Total Reward: 2.6999999999999993, Epsilon: 0.0227\n",
      "Episode: 756, Total Reward: 3.7999999999999954, Epsilon: 0.0226\n",
      "Episode: 757, Total Reward: 2.3999999999999995, Epsilon: 0.0225\n",
      "Episode: 758, Total Reward: 2.799999999999999, Epsilon: 0.0224\n",
      "Episode: 759, Total Reward: 2.9999999999999982, Epsilon: 0.0223\n",
      "Episode: 760, Total Reward: 4.299999999999996, Epsilon: 0.0222\n",
      "Episode: 761, Total Reward: 1.6999999999999993, Epsilon: 0.0220\n",
      "Episode: 762, Total Reward: 2.8999999999999986, Epsilon: 0.0219\n",
      "Episode: 763, Total Reward: -0.8999999999999986, Epsilon: 0.0218\n",
      "Episode: 764, Total Reward: 3.1000000000000005, Epsilon: 0.0217\n",
      "Episode: 765, Total Reward: 2.799999999999999, Epsilon: 0.0216\n",
      "Episode: 766, Total Reward: 3.4999999999999964, Epsilon: 0.0215\n",
      "Episode: 767, Total Reward: 5.199999999999994, Epsilon: 0.0214\n",
      "Episode: 768, Total Reward: 5.399999999999995, Epsilon: 0.0213\n",
      "Episode: 769, Total Reward: 4.899999999999995, Epsilon: 0.0212\n",
      "Episode: 770, Total Reward: -0.9999999999999982, Epsilon: 0.0211\n",
      "Episode: 771, Total Reward: 2.8999999999999986, Epsilon: 0.0210\n",
      "Episode: 772, Total Reward: 3.099999999999998, Epsilon: 0.0209\n",
      "Episode: 773, Total Reward: 3.2999999999999883, Epsilon: 0.0208\n",
      "Episode: 774, Total Reward: 1.3000000000000007, Epsilon: 0.0207\n",
      "Episode: 775, Total Reward: 2.0000000000000018, Epsilon: 0.0206\n",
      "Episode: 776, Total Reward: -0.8999999999999986, Epsilon: 0.0205\n",
      "Episode: 777, Total Reward: 0.5000000000000022, Epsilon: 0.0203\n",
      "Episode: 778, Total Reward: -0.8999999999999986, Epsilon: 0.0202\n",
      "Episode: 779, Total Reward: 3.2999999999999954, Epsilon: 0.0201\n",
      "Episode: 780, Total Reward: 1.8000000000000016, Epsilon: 0.0200\n",
      "Episode: 781, Total Reward: 3.4999999999999964, Epsilon: 0.0199\n",
      "Episode: 782, Total Reward: 2.499999999999999, Epsilon: 0.0198\n",
      "Episode: 783, Total Reward: 3.399999999999995, Epsilon: 0.0197\n",
      "Episode: 784, Total Reward: 0.8999999999999986, Epsilon: 0.0196\n",
      "Episode: 785, Total Reward: -0.8999999999999986, Epsilon: 0.0195\n",
      "Episode: 786, Total Reward: 5.499999999999993, Epsilon: 0.0195\n",
      "Episode: 787, Total Reward: 4.499999999999995, Epsilon: 0.0194\n",
      "Episode: 788, Total Reward: 4.399999999999995, Epsilon: 0.0193\n",
      "Episode: 789, Total Reward: 7.399999999999983, Epsilon: 0.0192\n",
      "Episode: 790, Total Reward: 3.9999999999999947, Epsilon: 0.0191\n",
      "Episode: 791, Total Reward: 5.799999999999985, Epsilon: 0.0190\n",
      "Episode: 792, Total Reward: 3.7999999999999963, Epsilon: 0.0189\n",
      "Episode: 793, Total Reward: -0.8999999999999986, Epsilon: 0.0188\n",
      "Episode: 794, Total Reward: -0.8999999999999986, Epsilon: 0.0187\n",
      "Episode: 795, Total Reward: 2.599999999999998, Epsilon: 0.0186\n",
      "Episode: 796, Total Reward: 4.899999999999995, Epsilon: 0.0185\n",
      "Episode: 797, Total Reward: 2.4999999999999982, Epsilon: 0.0184\n",
      "Episode: 798, Total Reward: 3.1999999999999975, Epsilon: 0.0183\n",
      "Episode: 799, Total Reward: 2.799999999999997, Epsilon: 0.0182\n",
      "Episode: 800, Total Reward: 2.0000000000000018, Epsilon: 0.0181\n",
      "Episode: 801, Total Reward: 3.1999999999999975, Epsilon: 0.0180\n",
      "Episode: 802, Total Reward: 0.699999999999998, Epsilon: 0.0180\n",
      "Episode: 803, Total Reward: 2.3999999999999977, Epsilon: 0.0179\n",
      "Episode: 804, Total Reward: 7.099999999999984, Epsilon: 0.0178\n",
      "Episode: 805, Total Reward: 2.299999999999999, Epsilon: 0.0177\n",
      "Episode: 806, Total Reward: 2.799999999999999, Epsilon: 0.0176\n",
      "Episode: 807, Total Reward: -3.299999999999998, Epsilon: 0.0175\n",
      "Episode: 808, Total Reward: 2.799999999999999, Epsilon: 0.0174\n",
      "Episode: 809, Total Reward: 0.20000000000000195, Epsilon: 0.0173\n",
      "Episode: 810, Total Reward: 5.999999999999988, Epsilon: 0.0172\n",
      "Episode: 811, Total Reward: 2.799999999999999, Epsilon: 0.0172\n",
      "Episode: 812, Total Reward: 1.9999999999999982, Epsilon: 0.0171\n",
      "Episode: 813, Total Reward: 0.10000000000000142, Epsilon: 0.0170\n",
      "Episode: 814, Total Reward: 2.599999999999998, Epsilon: 0.0169\n",
      "Episode: 815, Total Reward: 0.40000000000000124, Epsilon: 0.0168\n",
      "Episode: 816, Total Reward: -0.8999999999999986, Epsilon: 0.0167\n",
      "Episode: 817, Total Reward: 1.799999999999999, Epsilon: 0.0167\n",
      "Episode: 818, Total Reward: 1.799999999999999, Epsilon: 0.0166\n",
      "Episode: 819, Total Reward: 2.200000000000001, Epsilon: 0.0165\n",
      "Episode: 820, Total Reward: -0.4999999999999978, Epsilon: 0.0164\n",
      "Episode: 821, Total Reward: 1.1000000000000023, Epsilon: 0.0163\n",
      "Episode: 822, Total Reward: 1.9000000000000004, Epsilon: 0.0162\n",
      "Episode: 823, Total Reward: -0.6999999999999984, Epsilon: 0.0162\n",
      "Episode: 824, Total Reward: 3.6999999999999957, Epsilon: 0.0161\n",
      "Episode: 825, Total Reward: -0.8999999999999986, Epsilon: 0.0160\n",
      "Episode: 826, Total Reward: 2.8999999999999986, Epsilon: 0.0159\n",
      "Episode: 827, Total Reward: 1.8999999999999986, Epsilon: 0.0158\n",
      "Episode: 828, Total Reward: 3.1999999999999975, Epsilon: 0.0158\n",
      "Episode: 829, Total Reward: -0.09999999999999876, Epsilon: 0.0157\n",
      "Episode: 830, Total Reward: 1.6000000000000023, Epsilon: 0.0156\n",
      "Episode: 831, Total Reward: -0.09999999999999876, Epsilon: 0.0155\n",
      "Episode: 832, Total Reward: 2.9999999999999982, Epsilon: 0.0154\n",
      "Episode: 833, Total Reward: 2.6999999999999975, Epsilon: 0.0154\n",
      "Episode: 834, Total Reward: 3.099999999999998, Epsilon: 0.0153\n",
      "Episode: 835, Total Reward: 2.6999999999999993, Epsilon: 0.0152\n",
      "Episode: 836, Total Reward: 2.599999999999998, Epsilon: 0.0151\n",
      "Episode: 837, Total Reward: 0.4000000000000017, Epsilon: 0.0151\n",
      "Episode: 838, Total Reward: 2.1999999999999993, Epsilon: 0.0150\n",
      "Episode: 839, Total Reward: -0.8999999999999986, Epsilon: 0.0149\n",
      "Episode: 840, Total Reward: 2.299999999999999, Epsilon: 0.0148\n",
      "Episode: 841, Total Reward: 2.1999999999999993, Epsilon: 0.0148\n",
      "Episode: 842, Total Reward: 3.6000000000000023, Epsilon: 0.0147\n",
      "Episode: 843, Total Reward: 1.300000000000002, Epsilon: 0.0146\n",
      "Episode: 844, Total Reward: 2.3000000000000034, Epsilon: 0.0145\n",
      "Episode: 845, Total Reward: 2.6999999999999993, Epsilon: 0.0145\n",
      "Episode: 846, Total Reward: 1.8999999999999986, Epsilon: 0.0144\n",
      "Episode: 847, Total Reward: 0.3000000000000007, Epsilon: 0.0143\n",
      "Episode: 848, Total Reward: 2.220446049250313e-15, Epsilon: 0.0143\n",
      "Episode: 849, Total Reward: 3.1999999999999975, Epsilon: 0.0142\n",
      "Episode: 850, Total Reward: 1.1000000000000014, Epsilon: 0.0141\n",
      "Episode: 851, Total Reward: -0.8999999999999986, Epsilon: 0.0140\n",
      "Episode: 852, Total Reward: 0.9000000000000039, Epsilon: 0.0140\n",
      "Episode: 853, Total Reward: 2.9999999999999982, Epsilon: 0.0139\n",
      "Episode: 854, Total Reward: -1.5999999999999988, Epsilon: 0.0138\n",
      "Episode: 855, Total Reward: 1.2000000000000042, Epsilon: 0.0138\n",
      "Episode: 856, Total Reward: -0.8999999999999986, Epsilon: 0.0137\n",
      "Episode: 857, Total Reward: 2.499999999999999, Epsilon: 0.0136\n",
      "Episode: 858, Total Reward: 1.2000000000000024, Epsilon: 0.0136\n",
      "Episode: 859, Total Reward: 2.799999999999999, Epsilon: 0.0135\n",
      "Episode: 860, Total Reward: 2.6999999999999975, Epsilon: 0.0134\n",
      "Episode: 861, Total Reward: -0.8999999999999986, Epsilon: 0.0134\n",
      "Episode: 862, Total Reward: -0.8999999999999986, Epsilon: 0.0133\n",
      "Episode: 863, Total Reward: 0.6000000000000023, Epsilon: 0.0132\n",
      "Episode: 864, Total Reward: 2.599999999999998, Epsilon: 0.0132\n",
      "Episode: 865, Total Reward: 1.9000000000000035, Epsilon: 0.0131\n",
      "Episode: 866, Total Reward: -0.8999999999999986, Epsilon: 0.0130\n",
      "Episode: 867, Total Reward: 0.7000000000000015, Epsilon: 0.0130\n",
      "Episode: 868, Total Reward: -1.5999999999999985, Epsilon: 0.0129\n",
      "Episode: 869, Total Reward: 2.3000000000000043, Epsilon: 0.0128\n",
      "Episode: 870, Total Reward: 2.9999999999999982, Epsilon: 0.0128\n",
      "Episode: 871, Total Reward: -1.0999999999999988, Epsilon: 0.0127\n",
      "Episode: 872, Total Reward: -0.5999999999999981, Epsilon: 0.0126\n",
      "Episode: 873, Total Reward: -0.8999999999999986, Epsilon: 0.0126\n",
      "Episode: 874, Total Reward: 0.6000000000000023, Epsilon: 0.0125\n",
      "Episode: 875, Total Reward: -0.8999999999999986, Epsilon: 0.0125\n",
      "Episode: 876, Total Reward: -0.8999999999999986, Epsilon: 0.0124\n",
      "Episode: 877, Total Reward: 0.2000000000000015, Epsilon: 0.0123\n",
      "Episode: 878, Total Reward: 2.220446049250313e-15, Epsilon: 0.0123\n",
      "Episode: 879, Total Reward: -0.8999999999999986, Epsilon: 0.0122\n",
      "Episode: 880, Total Reward: 0.6000000000000014, Epsilon: 0.0121\n",
      "Episode: 881, Total Reward: -0.8999999999999986, Epsilon: 0.0121\n",
      "Episode: 882, Total Reward: 3.099999999999998, Epsilon: 0.0120\n",
      "Episode: 883, Total Reward: 0.10000000000000231, Epsilon: 0.0120\n",
      "Episode: 884, Total Reward: 1.9000000000000026, Epsilon: 0.0119\n",
      "Episode: 885, Total Reward: 0.2000000000000024, Epsilon: 0.0118\n",
      "Episode: 886, Total Reward: -0.8999999999999986, Epsilon: 0.0118\n",
      "Episode: 887, Total Reward: -0.8999999999999986, Epsilon: 0.0117\n",
      "Episode: 888, Total Reward: 3.799999999999997, Epsilon: 0.0117\n",
      "Episode: 889, Total Reward: 2.9000000000000057, Epsilon: 0.0116\n",
      "Episode: 890, Total Reward: 1.7000000000000024, Epsilon: 0.0115\n",
      "Episode: 891, Total Reward: -0.8999999999999986, Epsilon: 0.0115\n",
      "Episode: 892, Total Reward: -0.8999999999999986, Epsilon: 0.0114\n",
      "Episode: 893, Total Reward: -0.8999999999999986, Epsilon: 0.0114\n",
      "Episode: 894, Total Reward: -0.8999999999999986, Epsilon: 0.0113\n",
      "Episode: 895, Total Reward: -1.1999999999999988, Epsilon: 0.0113\n",
      "Episode: 896, Total Reward: -0.5999999999999985, Epsilon: 0.0112\n",
      "Episode: 897, Total Reward: 2.9999999999999982, Epsilon: 0.0112\n",
      "Episode: 898, Total Reward: 0.5000000000000027, Epsilon: 0.0111\n",
      "Episode: 899, Total Reward: 3.1999999999999975, Epsilon: 0.0110\n",
      "Episode: 900, Total Reward: -0.8999999999999986, Epsilon: 0.0110\n",
      "Episode: 901, Total Reward: -0.5999999999999981, Epsilon: 0.0109\n",
      "Episode: 902, Total Reward: 2.099999999999998, Epsilon: 0.0109\n",
      "Episode: 903, Total Reward: 1.6000000000000023, Epsilon: 0.0108\n",
      "Episode: 904, Total Reward: 2.1000000000000014, Epsilon: 0.0108\n",
      "Episode: 905, Total Reward: 3.1086244689504383e-15, Epsilon: 0.0107\n",
      "Episode: 906, Total Reward: 2.1000000000000014, Epsilon: 0.0107\n",
      "Episode: 907, Total Reward: 2.9999999999999982, Epsilon: 0.0106\n",
      "Episode: 908, Total Reward: -0.8999999999999986, Epsilon: 0.0106\n",
      "Episode: 909, Total Reward: -0.8999999999999986, Epsilon: 0.0105\n",
      "Episode: 910, Total Reward: 0.9000000000000021, Epsilon: 0.0104\n",
      "Episode: 911, Total Reward: 1.4000000000000021, Epsilon: 0.0104\n",
      "Episode: 912, Total Reward: 1.1000000000000019, Epsilon: 0.0103\n",
      "Episode: 913, Total Reward: 2.9999999999999982, Epsilon: 0.0103\n",
      "Episode: 914, Total Reward: -4.499999999999998, Epsilon: 0.0102\n",
      "Episode: 915, Total Reward: 2.599999999999998, Epsilon: 0.0102\n",
      "Episode: 916, Total Reward: -0.49999999999999867, Epsilon: 0.0101\n",
      "Episode: 917, Total Reward: -0.8999999999999986, Epsilon: 0.0101\n",
      "Episode: 918, Total Reward: 3.1999999999999975, Epsilon: 0.0100\n",
      "Episode: 919, Total Reward: 2.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 920, Total Reward: 2.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 921, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 922, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 923, Total Reward: -0.49999999999999867, Epsilon: 0.0100\n",
      "Episode: 924, Total Reward: 2.599999999999998, Epsilon: 0.0100\n",
      "Episode: 925, Total Reward: 2.499999999999999, Epsilon: 0.0100\n",
      "Episode: 926, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 927, Total Reward: 2.0, Epsilon: 0.0100\n",
      "Episode: 928, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 929, Total Reward: 1.5000000000000004, Epsilon: 0.0100\n",
      "Episode: 930, Total Reward: -3.899999999999998, Epsilon: 0.0100\n",
      "Episode: 931, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 932, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 933, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 934, Total Reward: 1.700000000000005, Epsilon: 0.0100\n",
      "Episode: 935, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 936, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 937, Total Reward: 2.599999999999998, Epsilon: 0.0100\n",
      "Episode: 938, Total Reward: 2.3999999999999995, Epsilon: 0.0100\n",
      "Episode: 939, Total Reward: 0.3000000000000007, Epsilon: 0.0100\n",
      "Episode: 940, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 941, Total Reward: 2.4000000000000004, Epsilon: 0.0100\n",
      "Episode: 942, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 943, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 944, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 945, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 946, Total Reward: -1.0999999999999988, Epsilon: 0.0100\n",
      "Episode: 947, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 948, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 949, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 950, Total Reward: 1.5999999999999979, Epsilon: 0.0100\n",
      "Episode: 951, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 952, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 953, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 954, Total Reward: 3.1999999999999975, Epsilon: 0.0100\n",
      "Episode: 955, Total Reward: 4.199999999999996, Epsilon: 0.0100\n",
      "Episode: 956, Total Reward: 2.1999999999999993, Epsilon: 0.0100\n",
      "Episode: 957, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 958, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 959, Total Reward: 1.5, Epsilon: 0.0100\n",
      "Episode: 960, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 961, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 962, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 963, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 964, Total Reward: 2.499999999999999, Epsilon: 0.0100\n",
      "Episode: 965, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 966, Total Reward: -0.0999999999999972, Epsilon: 0.0100\n",
      "Episode: 967, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 968, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 969, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 970, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 971, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 972, Total Reward: 3.6999999999999957, Epsilon: 0.0100\n",
      "Episode: 973, Total Reward: 0.9000000000000004, Epsilon: 0.0100\n",
      "Episode: 974, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 975, Total Reward: 4.799999999999996, Epsilon: 0.0100\n",
      "Episode: 976, Total Reward: 4.999999999999995, Epsilon: 0.0100\n",
      "Episode: 977, Total Reward: 2.3999999999999995, Epsilon: 0.0100\n",
      "Episode: 978, Total Reward: 5.499999999999988, Epsilon: 0.0100\n",
      "Episode: 979, Total Reward: 2.0999999999999908, Epsilon: 0.0100\n",
      "Episode: 980, Total Reward: 1.399999999999999, Epsilon: 0.0100\n",
      "Episode: 981, Total Reward: -7.499999999999998, Epsilon: 0.0100\n",
      "Episode: 982, Total Reward: 2.799999999999997, Epsilon: 0.0100\n",
      "Episode: 983, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 984, Total Reward: 1.7999999999999998, Epsilon: 0.0100\n",
      "Episode: 985, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 986, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 987, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 988, Total Reward: 3.8, Epsilon: 0.0100\n",
      "Episode: 989, Total Reward: 1.9999999999999978, Epsilon: 0.0100\n",
      "Episode: 990, Total Reward: 3.5999999999999908, Epsilon: 0.0100\n",
      "Episode: 991, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 992, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 993, Total Reward: -1.5999999999999988, Epsilon: 0.0100\n",
      "Episode: 994, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 995, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 996, Total Reward: 5.599999999999987, Epsilon: 0.0100\n",
      "Episode: 997, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 998, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 999, Total Reward: 3.6999999999999957, Epsilon: 0.0100\n",
      "Episode: 1000, Total Reward: 6.499999999999993, Epsilon: 0.0100\n",
      "Episode: 1001, Total Reward: 6.399999999999958, Epsilon: 0.0100\n",
      "Episode: 1002, Total Reward: 3.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1003, Total Reward: 6.299999999999978, Epsilon: 0.0100\n",
      "Episode: 1004, Total Reward: 1.5, Epsilon: 0.0100\n",
      "Episode: 1005, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 1006, Total Reward: 3.6999999999999975, Epsilon: 0.0100\n",
      "Episode: 1007, Total Reward: 3.5999999999999943, Epsilon: 0.0100\n",
      "Episode: 1008, Total Reward: 4.9999999999999964, Epsilon: 0.0100\n",
      "Episode: 1009, Total Reward: 4.299999999999988, Epsilon: 0.0100\n",
      "Episode: 1010, Total Reward: 7.5999999999999766, Epsilon: 0.0100\n",
      "Episode: 1011, Total Reward: 1.1, Epsilon: 0.0100\n",
      "Episode: 1012, Total Reward: -1.0999999999999988, Epsilon: 0.0100\n",
      "Episode: 1013, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1014, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 1015, Total Reward: 11.199999999999958, Epsilon: 0.0100\n",
      "Episode: 1016, Total Reward: 0.10000000000000187, Epsilon: 0.0100\n",
      "Episode: 1017, Total Reward: 0.10000000000000187, Epsilon: 0.0100\n",
      "Episode: 1018, Total Reward: 5.199999999999987, Epsilon: 0.0100\n",
      "Episode: 1019, Total Reward: 4.7999999999999865, Epsilon: 0.0100\n",
      "Episode: 1020, Total Reward: 4.299999999999995, Epsilon: 0.0100\n",
      "Episode: 1021, Total Reward: 1.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 1022, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1023, Total Reward: 8.899999999999975, Epsilon: 0.0100\n",
      "Episode: 1024, Total Reward: 6.099999999999987, Epsilon: 0.0100\n",
      "Episode: 1025, Total Reward: 7.999999999999975, Epsilon: 0.0100\n",
      "Episode: 1026, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1027, Total Reward: 2.000000000000001, Epsilon: 0.0100\n",
      "Episode: 1028, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1029, Total Reward: 0.9000000000000012, Epsilon: 0.0100\n",
      "Episode: 1030, Total Reward: 1.5, Epsilon: 0.0100\n",
      "Episode: 1031, Total Reward: 5.499999999999988, Epsilon: 0.0100\n",
      "Episode: 1032, Total Reward: 4.899999999999988, Epsilon: 0.0100\n",
      "Episode: 1033, Total Reward: 3.5999999999999908, Epsilon: 0.0100\n",
      "Episode: 1034, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1035, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1036, Total Reward: 6.699999999999987, Epsilon: 0.0100\n",
      "Episode: 1037, Total Reward: 2.6999999999999984, Epsilon: 0.0100\n",
      "Episode: 1038, Total Reward: 2.6999999999999975, Epsilon: 0.0100\n",
      "Episode: 1039, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 1040, Total Reward: 4.999999999999995, Epsilon: 0.0100\n",
      "Episode: 1041, Total Reward: 7.199999999999983, Epsilon: 0.0100\n",
      "Episode: 1042, Total Reward: 4.199999999999989, Epsilon: 0.0100\n",
      "Episode: 1043, Total Reward: 0.6000000000000023, Epsilon: 0.0100\n",
      "Episode: 1044, Total Reward: 4.399999999999988, Epsilon: 0.0100\n",
      "Episode: 1045, Total Reward: -1.0999999999999988, Epsilon: 0.0100\n",
      "Episode: 1046, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1047, Total Reward: 0.9000000000000021, Epsilon: 0.0100\n",
      "Episode: 1048, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1049, Total Reward: 4.599999999999994, Epsilon: 0.0100\n",
      "Episode: 1050, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 1051, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1052, Total Reward: 0.800000000000002, Epsilon: 0.0100\n",
      "Episode: 1053, Total Reward: 4.9999999999999964, Epsilon: 0.0100\n",
      "Episode: 1054, Total Reward: 4.399999999999993, Epsilon: 0.0100\n",
      "Episode: 1055, Total Reward: 7.5999999999999766, Epsilon: 0.0100\n",
      "Episode: 1056, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1057, Total Reward: 7.599999999999984, Epsilon: 0.0100\n",
      "Episode: 1058, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 1059, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1060, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1061, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1062, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1063, Total Reward: 0.40000000000000213, Epsilon: 0.0100\n",
      "Episode: 1064, Total Reward: 0.2000000000000024, Epsilon: 0.0100\n",
      "Episode: 1065, Total Reward: 3.1000000000000014, Epsilon: 0.0100\n",
      "Episode: 1066, Total Reward: 1.9000000000000008, Epsilon: 0.0100\n",
      "Episode: 1067, Total Reward: 2.220446049250313e-15, Epsilon: 0.0100\n",
      "Episode: 1068, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1069, Total Reward: 0.5000000000000022, Epsilon: 0.0100\n",
      "Episode: 1070, Total Reward: 0.5000000000000027, Epsilon: 0.0100\n",
      "Episode: 1071, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 1072, Total Reward: 5.399999999999993, Epsilon: 0.0100\n",
      "Episode: 1073, Total Reward: 2.599999999999996, Epsilon: 0.0100\n",
      "Episode: 1074, Total Reward: 2.599999999999998, Epsilon: 0.0100\n",
      "Episode: 1075, Total Reward: 2.1999999999999993, Epsilon: 0.0100\n",
      "Episode: 1076, Total Reward: 9.69999999999997, Epsilon: 0.0100\n",
      "Episode: 1077, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 1078, Total Reward: 6.199999999999987, Epsilon: 0.0100\n",
      "Episode: 1079, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1080, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1081, Total Reward: -0.19999999999999662, Epsilon: 0.0100\n",
      "Episode: 1082, Total Reward: -0.09999999999999787, Epsilon: 0.0100\n",
      "Episode: 1083, Total Reward: -1.5999999999999979, Epsilon: 0.0100\n",
      "Episode: 1084, Total Reward: 2.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 1085, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 1086, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1087, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1088, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1089, Total Reward: 2.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 1090, Total Reward: -1.5999999999999974, Epsilon: 0.0100\n",
      "Episode: 1091, Total Reward: -1.5999999999999965, Epsilon: 0.0100\n",
      "Episode: 1092, Total Reward: 0.30000000000000204, Epsilon: 0.0100\n",
      "Episode: 1093, Total Reward: -1.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 1094, Total Reward: 2.6999999999999975, Epsilon: 0.0100\n",
      "Episode: 1095, Total Reward: 1.1000000000000028, Epsilon: 0.0100\n",
      "Episode: 1096, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1097, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1098, Total Reward: 4.599999999999994, Epsilon: 0.0100\n",
      "Episode: 1099, Total Reward: 7.999999999999984, Epsilon: 0.0100\n",
      "Episode: 1100, Total Reward: -0.7999999999999985, Epsilon: 0.0100\n",
      "Episode: 1101, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1102, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 1103, Total Reward: -3.5999999999999974, Epsilon: 0.0100\n",
      "Episode: 1104, Total Reward: 6.899999999999986, Epsilon: 0.0100\n",
      "Episode: 1105, Total Reward: -0.4999999999999978, Epsilon: 0.0100\n",
      "Episode: 1106, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1107, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1108, Total Reward: 0.1000000000000032, Epsilon: 0.0100\n",
      "Episode: 1109, Total Reward: -2.6999999999999984, Epsilon: 0.0100\n",
      "Episode: 1110, Total Reward: 4.299999999999988, Epsilon: 0.0100\n",
      "Episode: 1111, Total Reward: 2.6999999999999975, Epsilon: 0.0100\n",
      "Episode: 1112, Total Reward: 4.299999999999996, Epsilon: 0.0100\n",
      "Episode: 1113, Total Reward: 0.6000000000000023, Epsilon: 0.0100\n",
      "Episode: 1114, Total Reward: 3.1999999999999966, Epsilon: 0.0100\n",
      "Episode: 1115, Total Reward: -3.299999999999998, Epsilon: 0.0100\n",
      "Episode: 1116, Total Reward: 0.40000000000000213, Epsilon: 0.0100\n",
      "Episode: 1117, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1118, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1119, Total Reward: 3.6999999999999886, Epsilon: 0.0100\n",
      "Episode: 1120, Total Reward: 1.0000000000000022, Epsilon: 0.0100\n",
      "Episode: 1121, Total Reward: 0.2000000000000033, Epsilon: 0.0100\n",
      "Episode: 1122, Total Reward: 4.499999999999995, Epsilon: 0.0100\n",
      "Episode: 1123, Total Reward: 4.499999999999995, Epsilon: 0.0100\n",
      "Episode: 1124, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1125, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 1126, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 1127, Total Reward: 2.0999999999999996, Epsilon: 0.0100\n",
      "Episode: 1128, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1129, Total Reward: 4.399999999999988, Epsilon: 0.0100\n",
      "Episode: 1130, Total Reward: -0.6999999999999984, Epsilon: 0.0100\n",
      "Episode: 1131, Total Reward: 3.7999999999999954, Epsilon: 0.0100\n",
      "Episode: 1132, Total Reward: -1.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 1133, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1134, Total Reward: 1.5, Epsilon: 0.0100\n",
      "Episode: 1135, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1136, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 1137, Total Reward: 4.799999999999996, Epsilon: 0.0100\n",
      "Episode: 1138, Total Reward: 4.299999999999988, Epsilon: 0.0100\n",
      "Episode: 1139, Total Reward: 7.499999999999977, Epsilon: 0.0100\n",
      "Episode: 1140, Total Reward: 4.299999999999988, Epsilon: 0.0100\n",
      "Episode: 1141, Total Reward: 1.3000000000000056, Epsilon: 0.0100\n",
      "Episode: 1142, Total Reward: 2.299999999999997, Epsilon: 0.0100\n",
      "Episode: 1143, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1144, Total Reward: 0.9000000000000004, Epsilon: 0.0100\n",
      "Episode: 1145, Total Reward: 0.40000000000000213, Epsilon: 0.0100\n",
      "Episode: 1146, Total Reward: -1.399999999999998, Epsilon: 0.0100\n",
      "Episode: 1147, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 1148, Total Reward: -0.09999999999999787, Epsilon: 0.0100\n",
      "Episode: 1149, Total Reward: 3.099999999999996, Epsilon: 0.0100\n",
      "Episode: 1150, Total Reward: 3.099999999999996, Epsilon: 0.0100\n",
      "Episode: 1151, Total Reward: 6.39999999999999, Epsilon: 0.0100\n",
      "Episode: 1152, Total Reward: 4.899999999999988, Epsilon: 0.0100\n",
      "Episode: 1153, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1154, Total Reward: 3.599999999999989, Epsilon: 0.0100\n",
      "Episode: 1155, Total Reward: 5.399999999999994, Epsilon: 0.0100\n",
      "Episode: 1156, Total Reward: -2.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 1157, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 1158, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 1159, Total Reward: 3.8999999999999915, Epsilon: 0.0100\n",
      "Episode: 1160, Total Reward: 4.899999999999988, Epsilon: 0.0100\n",
      "Episode: 1161, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1162, Total Reward: 3.1999999999999975, Epsilon: 0.0100\n",
      "Episode: 1163, Total Reward: -0.5999999999999983, Epsilon: 0.0100\n",
      "Episode: 1164, Total Reward: -1.2999999999999985, Epsilon: 0.0100\n",
      "Episode: 1165, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1166, Total Reward: 0.9000000000000021, Epsilon: 0.0100\n",
      "Episode: 1167, Total Reward: 5.099999999999978, Epsilon: 0.0100\n",
      "Episode: 1168, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1169, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1170, Total Reward: 14.799999999999962, Epsilon: 0.0100\n",
      "Episode: 1171, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 1172, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1173, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1174, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1175, Total Reward: 2.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 1176, Total Reward: 2.1999999999999993, Epsilon: 0.0100\n",
      "Episode: 1177, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1178, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1179, Total Reward: 2.0999999999999996, Epsilon: 0.0100\n",
      "Episode: 1180, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1181, Total Reward: 2.9999999999999982, Epsilon: 0.0100\n",
      "Episode: 1182, Total Reward: 2.0999999999999996, Epsilon: 0.0100\n",
      "Episode: 1183, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 1184, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 1185, Total Reward: 2.1999999999999993, Epsilon: 0.0100\n",
      "Episode: 1186, Total Reward: 1.4000000000000008, Epsilon: 0.0100\n",
      "Episode: 1187, Total Reward: 2.4999999999999982, Epsilon: 0.0100\n",
      "Episode: 1188, Total Reward: 7.499999999999986, Epsilon: 0.0100\n",
      "Episode: 1189, Total Reward: 1.5, Epsilon: 0.0100\n",
      "Episode: 1190, Total Reward: 0.3000000000000007, Epsilon: 0.0100\n",
      "Episode: 1191, Total Reward: 2.599999999999998, Epsilon: 0.0100\n",
      "Episode: 1192, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1193, Total Reward: 2.3999999999999995, Epsilon: 0.0100\n",
      "Episode: 1194, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 1195, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1196, Total Reward: 0.700000000000002, Epsilon: 0.0100\n",
      "Episode: 1197, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1198, Total Reward: 2.599999999999998, Epsilon: 0.0100\n",
      "Episode: 1199, Total Reward: 3.099999999999998, Epsilon: 0.0100\n",
      "Episode: 1200, Total Reward: -0.8999999999999986, Epsilon: 0.0100\n",
      "Episode: 1201, Total Reward: 2.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 1202, Total Reward: 2.3999999999999915, Epsilon: 0.0100\n",
      "Episode: 1203, Total Reward: 5.499999999999988, Epsilon: 0.0100\n",
      "Episode: 1204, Total Reward: 2.1999999999999993, Epsilon: 0.0100\n",
      "Episode: 1205, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1206, Total Reward: -1.3999999999999986, Epsilon: 0.0100\n",
      "Episode: 1207, Total Reward: -0.29999999999999805, Epsilon: 0.0100\n",
      "Episode: 1208, Total Reward: 0.3000000000000016, Epsilon: 0.0100\n",
      "Episode: 1209, Total Reward: 2.799999999999999, Epsilon: 0.0100\n",
      "Episode: 1210, Total Reward: 5.199999999999987, Epsilon: 0.0100\n",
      "Episode: 1211, Total Reward: 2.299999999999999, Epsilon: 0.0100\n",
      "Episode: 1212, Total Reward: 3.1999999999999957, Epsilon: 0.0100\n",
      "Episode: 1213, Total Reward: 5.499999999999988, Epsilon: 0.0100\n",
      "Episode: 1214, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1215, Total Reward: 9.899999999999972, Epsilon: 0.0100\n",
      "Episode: 1216, Total Reward: 4.999999999999995, Epsilon: 0.0100\n",
      "Episode: 1217, Total Reward: 6.999999999999984, Epsilon: 0.0100\n",
      "Episode: 1218, Total Reward: 3.1999999999999957, Epsilon: 0.0100\n",
      "Episode: 1219, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1220, Total Reward: 3.299999999999997, Epsilon: 0.0100\n",
      "Episode: 1221, Total Reward: 5.599999999999987, Epsilon: 0.0100\n",
      "Episode: 1222, Total Reward: 2.599999999999998, Epsilon: 0.0100\n",
      "Episode: 1223, Total Reward: 3.7999999999999883, Epsilon: 0.0100\n",
      "Episode: 1224, Total Reward: 2.6999999999999993, Epsilon: 0.0100\n",
      "Episode: 1225, Total Reward: 2.8999999999999986, Epsilon: 0.0100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gymnasium\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlappyBird-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m,use_lidar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m ) \u001b[38;5;66;03m##use_lidar=True option avaliable\u001b[39;00m\n\u001b[1;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env)\n\u001b[0;32m----> 5\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      9\u001b[0m pg_agent \u001b[38;5;241m=\u001b[39m PolicyGradientAgent(env)\n",
      "Cell \u001b[0;32mIn[111], line 113\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# Optimize model\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Update target network periodically\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[111], line 86\u001b[0m, in \u001b[0;36mDQNAgent.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dones, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Compute Q-values and targets\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[1;32m     87\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net(next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m next_q_values \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones))\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[111], line 11\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\",use_lidar=True ) ##use_lidar=True option avaliable\n",
    "\n",
    "\n",
    "agent = DQNAgent(env)\n",
    "agent.train()\n",
    "agent.test(num_episodes=10)\n",
    "\n",
    "\n",
    "pg_agent = PolicyGradientAgent(env)\n",
    "pg_agent.train()\n",
    "pg_agent.test(num_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCNN(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DQNCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self._calculate_conv_output(input_dim), 512)\n",
    "        self.fc2 = nn.Linear(512, action_space)\n",
    "\n",
    "    def _calculate_conv_output(self, input_dim):\n",
    "        dummy_input = torch.zeros(1, *input_dim)\n",
    "        x = self.conv1(dummy_input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(torch.prod(torch.tensor(x.shape[1:])))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1, x.size(-2), x.size(-1))\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, input_dim, action_space):\n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.0001\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.1\n",
    "        self.batch_size = 64\n",
    "        self.memory_size = 10000\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "        # Networks\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQNCNN(input_dim, action_space).to(self.device)\n",
    "        self.target_net = DQNCNN(input_dim, action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "        # Preprocessing pipeline\n",
    "        self.preprocess = Compose([ToTensor(), Resize((84, 84)), Grayscale()])\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        return self.preprocess(frame).numpy()\n",
    "\n",
    "    def select_action(self, state, testing=False):\n",
    "        if not testing and np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.action_space - 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Q-value computation\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.discount_factor * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        results = []\n",
    "        for episode in range(episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            frame = self.env.render()  # Query the initial rendered frame\n",
    "            state = self.preprocess_frame(frame)\n",
    "            state_stack = np.stack([state] * 4, axis=0)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state_stack)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                next_frame = self.env.render()  # Query the next rendered frame\n",
    "                next_state = self.preprocess_frame(next_frame)\n",
    "                next_state_stack = np.append(state_stack[1:], [next_state], axis=0)\n",
    "\n",
    "                self.memory.push(state_stack, action, reward, next_state_stack, done)\n",
    "                self.optimize_model()\n",
    "                state_stack = next_state_stack\n",
    "                total_reward += reward\n",
    "\n",
    "                if truncated:\n",
    "                    done = True\n",
    "\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            results.append(total_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "        return results\n",
    "\n",
    "\n",
    "    def test(self, num_episodes=10):\n",
    "        self.epsilon = 0.0  # Test with deterministic policy\n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            frame = self.env.render()  # Query the rendered frame\n",
    "            state = self.preprocess_frame(frame)\n",
    "            state_stack = np.stack([state] * 4, axis=0)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state_stack, testing=True)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                next_frame = self.env.render()  # Query the rendered frame\n",
    "                state_stack = np.append(state_stack[1:], [self.preprocess_frame(next_frame)], axis=0)\n",
    "                total_reward += reward\n",
    "\n",
    "                if truncated:\n",
    "                    done = True\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -8.099999999999998, Epsilon: 0.9950\n",
      "Episode 2, Total Reward: -6.899999999999999, Epsilon: 0.9900\n",
      "Episode 3, Total Reward: -7.499999999999998, Epsilon: 0.9851\n",
      "Episode 4, Total Reward: -6.299999999999999, Epsilon: 0.9801\n",
      "Episode 5, Total Reward: -7.499999999999998, Epsilon: 0.9752\n",
      "Episode 6, Total Reward: -8.099999999999998, Epsilon: 0.9704\n",
      "Episode 7, Total Reward: -8.099999999999998, Epsilon: 0.9655\n",
      "Episode 8, Total Reward: -8.099999999999998, Epsilon: 0.9607\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m action_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, input_dim, action_space)\n\u001b[0;32m----> 6\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[109], line 123\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m    120\u001b[0m next_state_stack \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(state_stack[\u001b[38;5;241m1\u001b[39m:], [next_state], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush(state_stack, action, reward, next_state_stack, done)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m state_stack \u001b[38;5;241m=\u001b[39m next_state_stack\n\u001b[1;32m    125\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[109], line 91\u001b[0m, in \u001b[0;36mDQNAgent.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(actions)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(rewards)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 91\u001b[0m next_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     92\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(dones)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Q-value computation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")\n",
    "input_dim = (4, 84, 84)  # 4 stacked grayscale frames\n",
    "action_space = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(env, input_dim, action_space)\n",
    "agent.train(episodes=1000)\n",
    "agent.test(num_episodes=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
