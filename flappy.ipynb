{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    "    PIPE_WIDTH,\n",
    "    PIPE_VEL_X,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neural Network Model for Q-value approximation\n",
    "class DQNCNN(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DQNCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1)  # 1 input channel\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, input_dim)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# Replay Memory to store experiences\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# DQN Agent Class\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]  # First dimension of observation\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0  # Initial exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.memory_size = 10000\n",
    "        self.episodes = 500\n",
    "        self.target_update_freq = 100\n",
    "\n",
    "        # Initialize policy and target networks\n",
    "        self.policy_net = DQNCNN(self.state_dim, self.action_space)\n",
    "        self.target_net = DQNCNN(self.state_dim, self.action_space)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "    def select_action(self, state, testing=False):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_space - 1)  # Random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def test(self, num_episodes=10):\n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\")  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=True)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -8.099999999999998, Epsilon: 0.9950\n",
      "Episode: 2, Total Reward: -7.499999999999998, Epsilon: 0.9900\n",
      "Episode: 3, Total Reward: -8.7, Epsilon: 0.9851\n",
      "Episode: 4, Total Reward: -6.299999999999999, Epsilon: 0.9801\n",
      "Episode: 5, Total Reward: -8.7, Epsilon: 0.9752\n",
      "Episode: 6, Total Reward: -6.899999999999999, Epsilon: 0.9704\n",
      "Episode: 7, Total Reward: -8.099999999999998, Epsilon: 0.9655\n",
      "Episode: 8, Total Reward: -8.099999999999998, Epsilon: 0.9607\n",
      "Episode: 9, Total Reward: -7.499999999999998, Epsilon: 0.9559\n",
      "Episode: 10, Total Reward: -7.499999999999998, Epsilon: 0.9511\n",
      "Episode: 11, Total Reward: -6.899999999999999, Epsilon: 0.9464\n",
      "Episode: 12, Total Reward: -5.099999999999998, Epsilon: 0.9416\n",
      "Episode: 13, Total Reward: -6.299999999999999, Epsilon: 0.9369\n",
      "Episode: 14, Total Reward: -8.099999999999998, Epsilon: 0.9322\n",
      "Episode: 15, Total Reward: -7.499999999999998, Epsilon: 0.9276\n",
      "Episode: 16, Total Reward: -7.499999999999998, Epsilon: 0.9229\n",
      "Episode: 17, Total Reward: -6.899999999999999, Epsilon: 0.9183\n",
      "Episode: 18, Total Reward: -7.499999999999998, Epsilon: 0.9137\n",
      "Episode: 19, Total Reward: -7.499999999999998, Epsilon: 0.9092\n",
      "Episode: 20, Total Reward: -8.099999999999998, Epsilon: 0.9046\n",
      "Episode: 21, Total Reward: -6.899999999999999, Epsilon: 0.9001\n",
      "Episode: 22, Total Reward: -8.099999999999998, Epsilon: 0.8956\n",
      "Episode: 23, Total Reward: -8.7, Epsilon: 0.8911\n",
      "Episode: 24, Total Reward: -7.499999999999998, Epsilon: 0.8867\n",
      "Episode: 25, Total Reward: -8.099999999999998, Epsilon: 0.8822\n",
      "Episode: 26, Total Reward: -8.099999999999998, Epsilon: 0.8778\n",
      "Episode: 27, Total Reward: -8.7, Epsilon: 0.8734\n",
      "Episode: 28, Total Reward: -8.099999999999998, Epsilon: 0.8691\n",
      "Episode: 29, Total Reward: -8.099999999999998, Epsilon: 0.8647\n",
      "Episode: 30, Total Reward: -3.299999999999998, Epsilon: 0.8604\n",
      "Episode: 31, Total Reward: -8.099999999999998, Epsilon: 0.8561\n",
      "Episode: 32, Total Reward: -7.499999999999998, Epsilon: 0.8518\n",
      "Episode: 33, Total Reward: -8.7, Epsilon: 0.8475\n",
      "Episode: 34, Total Reward: -7.499999999999998, Epsilon: 0.8433\n",
      "Episode: 35, Total Reward: -8.099999999999998, Epsilon: 0.8391\n",
      "Episode: 36, Total Reward: -6.899999999999999, Epsilon: 0.8349\n",
      "Episode: 37, Total Reward: -8.099999999999998, Epsilon: 0.8307\n",
      "Episode: 38, Total Reward: -8.099999999999998, Epsilon: 0.8266\n",
      "Episode: 39, Total Reward: -7.499999999999998, Epsilon: 0.8224\n",
      "Episode: 40, Total Reward: -8.7, Epsilon: 0.8183\n",
      "Episode: 41, Total Reward: -6.899999999999999, Epsilon: 0.8142\n",
      "Episode: 42, Total Reward: -6.899999999999999, Epsilon: 0.8102\n",
      "Episode: 43, Total Reward: -7.499999999999998, Epsilon: 0.8061\n",
      "Episode: 44, Total Reward: -8.099999999999998, Epsilon: 0.8021\n",
      "Episode: 45, Total Reward: -8.7, Epsilon: 0.7981\n",
      "Episode: 46, Total Reward: -8.7, Epsilon: 0.7941\n",
      "Episode: 47, Total Reward: -6.899999999999999, Epsilon: 0.7901\n",
      "Episode: 48, Total Reward: -8.7, Epsilon: 0.7862\n",
      "Episode: 49, Total Reward: -7.499999999999998, Epsilon: 0.7822\n",
      "Episode: 50, Total Reward: -8.7, Epsilon: 0.7783\n",
      "Episode: 51, Total Reward: -6.899999999999999, Epsilon: 0.7744\n",
      "Episode: 52, Total Reward: -7.499999999999998, Epsilon: 0.7705\n",
      "Episode: 53, Total Reward: -7.499999999999998, Epsilon: 0.7667\n",
      "Episode: 54, Total Reward: -7.499999999999998, Epsilon: 0.7629\n",
      "Episode: 55, Total Reward: -6.899999999999999, Epsilon: 0.7590\n",
      "Episode: 56, Total Reward: -8.099999999999998, Epsilon: 0.7553\n",
      "Episode: 57, Total Reward: -8.7, Epsilon: 0.7515\n",
      "Episode: 58, Total Reward: -6.299999999999999, Epsilon: 0.7477\n",
      "Episode: 59, Total Reward: -8.099999999999998, Epsilon: 0.7440\n",
      "Episode: 60, Total Reward: -6.899999999999999, Epsilon: 0.7403\n",
      "Episode: 61, Total Reward: -8.099999999999998, Epsilon: 0.7366\n",
      "Episode: 62, Total Reward: -9.299999999999999, Epsilon: 0.7329\n",
      "Episode: 63, Total Reward: -8.7, Epsilon: 0.7292\n",
      "Episode: 64, Total Reward: -8.099999999999998, Epsilon: 0.7256\n",
      "Episode: 65, Total Reward: -5.699999999999998, Epsilon: 0.7219\n",
      "Episode: 66, Total Reward: -4.499999999999998, Epsilon: 0.7183\n",
      "Episode: 67, Total Reward: -8.7, Epsilon: 0.7147\n",
      "Episode: 68, Total Reward: -7.499999999999998, Epsilon: 0.7112\n",
      "Episode: 69, Total Reward: -6.899999999999999, Epsilon: 0.7076\n",
      "Episode: 70, Total Reward: -5.099999999999998, Epsilon: 0.7041\n",
      "Episode: 71, Total Reward: -8.099999999999998, Epsilon: 0.7005\n",
      "Episode: 72, Total Reward: -5.699999999999998, Epsilon: 0.6970\n",
      "Episode: 73, Total Reward: -7.499999999999998, Epsilon: 0.6936\n",
      "Episode: 74, Total Reward: -8.099999999999998, Epsilon: 0.6901\n",
      "Episode: 75, Total Reward: -6.299999999999999, Epsilon: 0.6866\n",
      "Episode: 76, Total Reward: -8.099999999999998, Epsilon: 0.6832\n",
      "Episode: 77, Total Reward: -7.499999999999998, Epsilon: 0.6798\n",
      "Episode: 78, Total Reward: -7.499999999999998, Epsilon: 0.6764\n",
      "Episode: 79, Total Reward: -3.299999999999998, Epsilon: 0.6730\n",
      "Episode: 80, Total Reward: -6.299999999999999, Epsilon: 0.6696\n",
      "Episode: 81, Total Reward: -7.499999999999998, Epsilon: 0.6663\n",
      "Episode: 82, Total Reward: -6.899999999999999, Epsilon: 0.6630\n",
      "Episode: 83, Total Reward: -7.499999999999998, Epsilon: 0.6597\n",
      "Episode: 84, Total Reward: -6.299999999999999, Epsilon: 0.6564\n",
      "Episode: 85, Total Reward: -6.899999999999999, Epsilon: 0.6531\n",
      "Episode: 86, Total Reward: -8.7, Epsilon: 0.6498\n",
      "Episode: 87, Total Reward: -6.899999999999999, Epsilon: 0.6466\n",
      "Episode: 88, Total Reward: -7.499999999999998, Epsilon: 0.6433\n",
      "Episode: 89, Total Reward: -5.099999999999998, Epsilon: 0.6401\n",
      "Episode: 90, Total Reward: -8.7, Epsilon: 0.6369\n",
      "Episode: 91, Total Reward: -3.899999999999998, Epsilon: 0.6337\n",
      "Episode: 92, Total Reward: -4.499999999999998, Epsilon: 0.6306\n",
      "Episode: 93, Total Reward: -8.099999999999998, Epsilon: 0.6274\n",
      "Episode: 94, Total Reward: -6.899999999999999, Epsilon: 0.6243\n",
      "Episode: 95, Total Reward: -6.899999999999999, Epsilon: 0.6211\n",
      "Episode: 96, Total Reward: -4.499999999999998, Epsilon: 0.6180\n",
      "Episode: 97, Total Reward: -5.099999999999998, Epsilon: 0.6149\n",
      "Episode: 98, Total Reward: -7.499999999999998, Epsilon: 0.6119\n",
      "Episode: 99, Total Reward: -7.499999999999998, Epsilon: 0.6088\n",
      "Episode: 100, Total Reward: -6.899999999999999, Epsilon: 0.6058\n",
      "Episode: 101, Total Reward: -8.7, Epsilon: 0.6027\n",
      "Episode: 102, Total Reward: -8.7, Epsilon: 0.5997\n",
      "Episode: 103, Total Reward: -6.899999999999999, Epsilon: 0.5967\n",
      "Episode: 104, Total Reward: -8.7, Epsilon: 0.5937\n",
      "Episode: 105, Total Reward: -6.899999999999999, Epsilon: 0.5908\n",
      "Episode: 106, Total Reward: -5.699999999999998, Epsilon: 0.5878\n",
      "Episode: 107, Total Reward: -6.299999999999999, Epsilon: 0.5849\n",
      "Episode: 108, Total Reward: -5.099999999999998, Epsilon: 0.5820\n",
      "Episode: 109, Total Reward: -8.7, Epsilon: 0.5790\n",
      "Episode: 110, Total Reward: -9.299999999999999, Epsilon: 0.5762\n",
      "Episode: 111, Total Reward: -5.699999999999998, Epsilon: 0.5733\n",
      "Episode: 112, Total Reward: -6.299999999999999, Epsilon: 0.5704\n",
      "Episode: 113, Total Reward: -5.099999999999998, Epsilon: 0.5676\n",
      "Episode: 114, Total Reward: -4.499999999999998, Epsilon: 0.5647\n",
      "Episode: 115, Total Reward: -7.499999999999998, Epsilon: 0.5619\n",
      "Episode: 116, Total Reward: -7.499999999999998, Epsilon: 0.5591\n",
      "Episode: 117, Total Reward: -8.099999999999998, Epsilon: 0.5563\n",
      "Episode: 118, Total Reward: -7.499999999999998, Epsilon: 0.5535\n",
      "Episode: 119, Total Reward: -7.499999999999998, Epsilon: 0.5507\n",
      "Episode: 120, Total Reward: -7.499999999999998, Epsilon: 0.5480\n",
      "Episode: 121, Total Reward: -7.499999999999998, Epsilon: 0.5452\n",
      "Episode: 122, Total Reward: -8.099999999999998, Epsilon: 0.5425\n",
      "Episode: 123, Total Reward: -7.499999999999998, Epsilon: 0.5398\n",
      "Episode: 124, Total Reward: -6.299999999999999, Epsilon: 0.5371\n",
      "Episode: 125, Total Reward: -9.299999999999999, Epsilon: 0.5344\n",
      "Episode: 126, Total Reward: -8.7, Epsilon: 0.5318\n",
      "Episode: 127, Total Reward: -8.099999999999998, Epsilon: 0.5291\n",
      "Episode: 128, Total Reward: -7.499999999999998, Epsilon: 0.5264\n",
      "Episode: 129, Total Reward: -9.299999999999999, Epsilon: 0.5238\n",
      "Episode: 130, Total Reward: -3.899999999999998, Epsilon: 0.5212\n",
      "Episode: 131, Total Reward: -8.7, Epsilon: 0.5186\n",
      "Episode: 132, Total Reward: -8.7, Epsilon: 0.5160\n",
      "Episode: 133, Total Reward: -8.099999999999998, Epsilon: 0.5134\n",
      "Episode: 134, Total Reward: -3.299999999999998, Epsilon: 0.5108\n",
      "Episode: 135, Total Reward: -3.899999999999998, Epsilon: 0.5083\n",
      "Episode: 136, Total Reward: -8.099999999999998, Epsilon: 0.5058\n",
      "Episode: 137, Total Reward: -8.7, Epsilon: 0.5032\n",
      "Episode: 138, Total Reward: -7.499999999999998, Epsilon: 0.5007\n",
      "Episode: 139, Total Reward: -8.7, Epsilon: 0.4982\n",
      "Episode: 140, Total Reward: -8.099999999999998, Epsilon: 0.4957\n",
      "Episode: 141, Total Reward: -7.499999999999998, Epsilon: 0.4932\n",
      "Episode: 142, Total Reward: -7.499999999999998, Epsilon: 0.4908\n",
      "Episode: 143, Total Reward: -7.499999999999998, Epsilon: 0.4883\n",
      "Episode: 144, Total Reward: -0.8999999999999986, Epsilon: 0.4859\n",
      "Episode: 145, Total Reward: -7.499999999999998, Epsilon: 0.4834\n",
      "Episode: 146, Total Reward: -2.099999999999998, Epsilon: 0.4810\n",
      "Episode: 147, Total Reward: -5.099999999999998, Epsilon: 0.4786\n",
      "Episode: 148, Total Reward: -8.099999999999998, Epsilon: 0.4762\n",
      "Episode: 149, Total Reward: -6.899999999999999, Epsilon: 0.4738\n",
      "Episode: 150, Total Reward: -5.699999999999998, Epsilon: 0.4715\n",
      "Episode: 151, Total Reward: -4.499999999999998, Epsilon: 0.4691\n",
      "Episode: 152, Total Reward: -4.499999999999998, Epsilon: 0.4668\n",
      "Episode: 153, Total Reward: -6.299999999999999, Epsilon: 0.4644\n",
      "Episode: 154, Total Reward: -2.699999999999998, Epsilon: 0.4621\n",
      "Episode: 155, Total Reward: -5.699999999999998, Epsilon: 0.4598\n",
      "Episode: 156, Total Reward: -6.899999999999999, Epsilon: 0.4575\n",
      "Episode: 157, Total Reward: -8.7, Epsilon: 0.4552\n",
      "Episode: 158, Total Reward: -8.099999999999998, Epsilon: 0.4529\n",
      "Episode: 159, Total Reward: -3.299999999999998, Epsilon: 0.4507\n",
      "Episode: 160, Total Reward: -3.899999999999998, Epsilon: 0.4484\n",
      "Episode: 161, Total Reward: -7.499999999999998, Epsilon: 0.4462\n",
      "Episode: 162, Total Reward: -8.099999999999998, Epsilon: 0.4440\n",
      "Episode: 163, Total Reward: -6.899999999999999, Epsilon: 0.4417\n",
      "Episode: 164, Total Reward: -5.699999999999998, Epsilon: 0.4395\n",
      "Episode: 165, Total Reward: -2.9999999999999982, Epsilon: 0.4373\n",
      "Episode: 166, Total Reward: -4.499999999999998, Epsilon: 0.4351\n",
      "Episode: 167, Total Reward: -4.499999999999998, Epsilon: 0.4330\n",
      "Episode: 168, Total Reward: -6.899999999999999, Epsilon: 0.4308\n",
      "Episode: 169, Total Reward: -8.099999999999998, Epsilon: 0.4286\n",
      "Episode: 170, Total Reward: -8.7, Epsilon: 0.4265\n",
      "Episode: 171, Total Reward: -6.299999999999999, Epsilon: 0.4244\n",
      "Episode: 172, Total Reward: -8.099999999999998, Epsilon: 0.4223\n",
      "Episode: 173, Total Reward: -1.4999999999999982, Epsilon: 0.4201\n",
      "Episode: 174, Total Reward: -8.7, Epsilon: 0.4180\n",
      "Episode: 175, Total Reward: -8.7, Epsilon: 0.4159\n",
      "Episode: 176, Total Reward: -6.899999999999999, Epsilon: 0.4139\n",
      "Episode: 177, Total Reward: -8.7, Epsilon: 0.4118\n",
      "Episode: 178, Total Reward: -8.7, Epsilon: 0.4097\n",
      "Episode: 179, Total Reward: -2.099999999999998, Epsilon: 0.4077\n",
      "Episode: 180, Total Reward: -0.8999999999999986, Epsilon: 0.4057\n",
      "Episode: 181, Total Reward: -6.899999999999999, Epsilon: 0.4036\n",
      "Episode: 182, Total Reward: -0.8999999999999986, Epsilon: 0.4016\n",
      "Episode: 183, Total Reward: -6.899999999999999, Epsilon: 0.3996\n",
      "Episode: 184, Total Reward: -7.499999999999998, Epsilon: 0.3976\n",
      "Episode: 185, Total Reward: -8.099999999999998, Epsilon: 0.3956\n",
      "Episode: 186, Total Reward: -8.7, Epsilon: 0.3936\n",
      "Episode: 187, Total Reward: -0.8999999999999986, Epsilon: 0.3917\n",
      "Episode: 188, Total Reward: -9.299999999999999, Epsilon: 0.3897\n",
      "Episode: 189, Total Reward: -8.099999999999998, Epsilon: 0.3878\n",
      "Episode: 190, Total Reward: -5.699999999999998, Epsilon: 0.3858\n",
      "Episode: 191, Total Reward: -9.299999999999999, Epsilon: 0.3839\n",
      "Episode: 192, Total Reward: -7.499999999999998, Epsilon: 0.3820\n",
      "Episode: 193, Total Reward: -6.899999999999999, Epsilon: 0.3801\n",
      "Episode: 194, Total Reward: -9.299999999999999, Epsilon: 0.3782\n",
      "Episode: 195, Total Reward: -2.099999999999998, Epsilon: 0.3763\n",
      "Episode: 196, Total Reward: -2.099999999999998, Epsilon: 0.3744\n",
      "Episode: 197, Total Reward: -7.499999999999998, Epsilon: 0.3725\n",
      "Episode: 198, Total Reward: -7.499999999999998, Epsilon: 0.3707\n",
      "Episode: 199, Total Reward: -4.499999999999998, Epsilon: 0.3688\n",
      "Episode: 200, Total Reward: -3.899999999999998, Epsilon: 0.3670\n",
      "Episode: 201, Total Reward: -5.699999999999998, Epsilon: 0.3651\n",
      "Episode: 202, Total Reward: -0.8999999999999986, Epsilon: 0.3633\n",
      "Episode: 203, Total Reward: -6.899999999999999, Epsilon: 0.3615\n",
      "Episode: 204, Total Reward: -7.499999999999998, Epsilon: 0.3597\n",
      "Episode: 205, Total Reward: -3.299999999999998, Epsilon: 0.3579\n",
      "Episode: 206, Total Reward: -2.099999999999998, Epsilon: 0.3561\n",
      "Episode: 207, Total Reward: -6.899999999999999, Epsilon: 0.3543\n",
      "Episode: 208, Total Reward: -0.8999999999999986, Epsilon: 0.3525\n",
      "Episode: 209, Total Reward: -8.7, Epsilon: 0.3508\n",
      "Episode: 210, Total Reward: -5.099999999999998, Epsilon: 0.3490\n",
      "Episode: 211, Total Reward: -0.8999999999999986, Epsilon: 0.3473\n",
      "Episode: 212, Total Reward: -2.699999999999998, Epsilon: 0.3455\n",
      "Episode: 213, Total Reward: -2.699999999999998, Epsilon: 0.3438\n",
      "Episode: 214, Total Reward: -5.699999999999998, Epsilon: 0.3421\n",
      "Episode: 215, Total Reward: -6.299999999999999, Epsilon: 0.3404\n",
      "Episode: 216, Total Reward: -0.8999999999999986, Epsilon: 0.3387\n",
      "Episode: 217, Total Reward: -8.099999999999998, Epsilon: 0.3370\n",
      "Episode: 218, Total Reward: -5.099999999999998, Epsilon: 0.3353\n",
      "Episode: 219, Total Reward: -6.299999999999999, Epsilon: 0.3336\n",
      "Episode: 220, Total Reward: -6.899999999999999, Epsilon: 0.3320\n",
      "Episode: 221, Total Reward: -5.699999999999998, Epsilon: 0.3303\n",
      "Episode: 222, Total Reward: -0.8999999999999986, Epsilon: 0.3286\n",
      "Episode: 223, Total Reward: -0.8999999999999986, Epsilon: 0.3270\n",
      "Episode: 224, Total Reward: -0.8999999999999986, Epsilon: 0.3254\n",
      "Episode: 225, Total Reward: -3.899999999999998, Epsilon: 0.3237\n",
      "Episode: 226, Total Reward: -6.899999999999999, Epsilon: 0.3221\n",
      "Episode: 227, Total Reward: -6.899999999999999, Epsilon: 0.3205\n",
      "Episode: 228, Total Reward: -5.099999999999998, Epsilon: 0.3189\n",
      "Episode: 229, Total Reward: -3.299999999999998, Epsilon: 0.3173\n",
      "Episode: 230, Total Reward: -1.4999999999999982, Epsilon: 0.3157\n",
      "Episode: 231, Total Reward: -7.499999999999998, Epsilon: 0.3141\n",
      "Episode: 232, Total Reward: -6.899999999999999, Epsilon: 0.3126\n",
      "Episode: 233, Total Reward: -3.299999999999998, Epsilon: 0.3110\n",
      "Episode: 234, Total Reward: -8.7, Epsilon: 0.3095\n",
      "Episode: 235, Total Reward: -8.7, Epsilon: 0.3079\n",
      "Episode: 236, Total Reward: -6.899999999999999, Epsilon: 0.3064\n",
      "Episode: 237, Total Reward: -1.4999999999999982, Epsilon: 0.3048\n",
      "Episode: 238, Total Reward: -4.499999999999998, Epsilon: 0.3033\n",
      "Episode: 239, Total Reward: -8.099999999999998, Epsilon: 0.3018\n",
      "Episode: 240, Total Reward: -3.899999999999998, Epsilon: 0.3003\n",
      "Episode: 241, Total Reward: -0.8999999999999986, Epsilon: 0.2988\n",
      "Episode: 242, Total Reward: -6.299999999999999, Epsilon: 0.2973\n",
      "Episode: 243, Total Reward: -3.299999999999998, Epsilon: 0.2958\n",
      "Episode: 244, Total Reward: -6.899999999999999, Epsilon: 0.2943\n",
      "Episode: 245, Total Reward: -4.499999999999998, Epsilon: 0.2929\n",
      "Episode: 246, Total Reward: -8.099999999999998, Epsilon: 0.2914\n",
      "Episode: 247, Total Reward: -0.8999999999999986, Epsilon: 0.2899\n",
      "Episode: 248, Total Reward: -7.499999999999998, Epsilon: 0.2885\n",
      "Episode: 249, Total Reward: -4.499999999999998, Epsilon: 0.2870\n",
      "Episode: 250, Total Reward: -3.899999999999998, Epsilon: 0.2856\n",
      "Episode: 251, Total Reward: -6.899999999999999, Epsilon: 0.2842\n",
      "Episode: 252, Total Reward: -5.099999999999998, Epsilon: 0.2828\n",
      "Episode: 253, Total Reward: -5.699999999999998, Epsilon: 0.2813\n",
      "Episode: 254, Total Reward: -3.899999999999998, Epsilon: 0.2799\n",
      "Episode: 255, Total Reward: -7.499999999999998, Epsilon: 0.2785\n",
      "Episode: 256, Total Reward: -3.299999999999998, Epsilon: 0.2771\n",
      "Episode: 257, Total Reward: -8.7, Epsilon: 0.2758\n",
      "Episode: 258, Total Reward: -5.099999999999998, Epsilon: 0.2744\n",
      "Episode: 259, Total Reward: -6.299999999999999, Epsilon: 0.2730\n",
      "Episode: 260, Total Reward: -5.099999999999998, Epsilon: 0.2716\n",
      "Episode: 261, Total Reward: -8.099999999999998, Epsilon: 0.2703\n",
      "Episode: 262, Total Reward: -5.699999999999998, Epsilon: 0.2689\n",
      "Episode: 263, Total Reward: -8.099999999999998, Epsilon: 0.2676\n",
      "Episode: 264, Total Reward: -7.499999999999998, Epsilon: 0.2663\n",
      "Episode: 265, Total Reward: -7.499999999999998, Epsilon: 0.2649\n",
      "Episode: 266, Total Reward: -3.299999999999998, Epsilon: 0.2636\n",
      "Episode: 267, Total Reward: -6.899999999999999, Epsilon: 0.2623\n",
      "Episode: 268, Total Reward: -3.899999999999998, Epsilon: 0.2610\n",
      "Episode: 269, Total Reward: -8.099999999999998, Epsilon: 0.2597\n",
      "Episode: 270, Total Reward: -0.8999999999999986, Epsilon: 0.2584\n",
      "Episode: 271, Total Reward: -5.699999999999998, Epsilon: 0.2571\n",
      "Episode: 272, Total Reward: -5.699999999999998, Epsilon: 0.2558\n",
      "Episode: 273, Total Reward: -1.4999999999999982, Epsilon: 0.2545\n",
      "Episode: 274, Total Reward: -3.899999999999998, Epsilon: 0.2532\n",
      "Episode: 275, Total Reward: -8.7, Epsilon: 0.2520\n",
      "Episode: 276, Total Reward: -9.299999999999999, Epsilon: 0.2507\n",
      "Episode: 277, Total Reward: -6.899999999999999, Epsilon: 0.2495\n",
      "Episode: 278, Total Reward: -5.099999999999998, Epsilon: 0.2482\n",
      "Episode: 279, Total Reward: -5.699999999999998, Epsilon: 0.2470\n",
      "Episode: 280, Total Reward: -8.7, Epsilon: 0.2457\n",
      "Episode: 281, Total Reward: -6.299999999999999, Epsilon: 0.2445\n",
      "Episode: 282, Total Reward: -6.899999999999999, Epsilon: 0.2433\n",
      "Episode: 283, Total Reward: -5.099999999999998, Epsilon: 0.2421\n",
      "Episode: 284, Total Reward: -8.099999999999998, Epsilon: 0.2409\n",
      "Episode: 285, Total Reward: -6.299999999999999, Epsilon: 0.2397\n",
      "Episode: 286, Total Reward: -3.299999999999998, Epsilon: 0.2385\n",
      "Episode: 287, Total Reward: -4.499999999999998, Epsilon: 0.2373\n",
      "Episode: 288, Total Reward: -9.299999999999999, Epsilon: 0.2361\n",
      "Episode: 289, Total Reward: -0.8999999999999986, Epsilon: 0.2349\n",
      "Episode: 290, Total Reward: -4.499999999999998, Epsilon: 0.2337\n",
      "Episode: 291, Total Reward: -5.699999999999998, Epsilon: 0.2326\n",
      "Episode: 292, Total Reward: -4.499999999999998, Epsilon: 0.2314\n",
      "Episode: 293, Total Reward: -3.899999999999998, Epsilon: 0.2302\n",
      "Episode: 294, Total Reward: -3.299999999999998, Epsilon: 0.2291\n",
      "Episode: 295, Total Reward: 0.3000000000000007, Epsilon: 0.2279\n",
      "Episode: 296, Total Reward: -3.899999999999998, Epsilon: 0.2268\n",
      "Episode: 297, Total Reward: -8.7, Epsilon: 0.2257\n",
      "Episode: 298, Total Reward: -0.8999999999999986, Epsilon: 0.2245\n",
      "Episode: 299, Total Reward: -0.8999999999999986, Epsilon: 0.2234\n",
      "Episode: 300, Total Reward: -8.7, Epsilon: 0.2223\n",
      "Episode: 301, Total Reward: -0.8999999999999986, Epsilon: 0.2212\n",
      "Episode: 302, Total Reward: -5.099999999999998, Epsilon: 0.2201\n",
      "Episode: 303, Total Reward: -6.899999999999999, Epsilon: 0.2190\n",
      "Episode: 304, Total Reward: -5.099999999999998, Epsilon: 0.2179\n",
      "Episode: 305, Total Reward: -0.8999999999999986, Epsilon: 0.2168\n",
      "Episode: 306, Total Reward: -9.299999999999999, Epsilon: 0.2157\n",
      "Episode: 307, Total Reward: -3.899999999999998, Epsilon: 0.2146\n",
      "Episode: 308, Total Reward: -2.099999999999998, Epsilon: 0.2136\n",
      "Episode: 309, Total Reward: -5.099999999999998, Epsilon: 0.2125\n",
      "Episode: 310, Total Reward: -3.899999999999998, Epsilon: 0.2114\n",
      "Episode: 311, Total Reward: -0.8999999999999986, Epsilon: 0.2104\n",
      "Episode: 312, Total Reward: -0.8999999999999986, Epsilon: 0.2093\n",
      "Episode: 313, Total Reward: -3.899999999999998, Epsilon: 0.2083\n",
      "Episode: 314, Total Reward: -2.699999999999998, Epsilon: 0.2072\n",
      "Episode: 315, Total Reward: -7.499999999999998, Epsilon: 0.2062\n",
      "Episode: 316, Total Reward: -4.499999999999998, Epsilon: 0.2052\n",
      "Episode: 317, Total Reward: -5.699999999999998, Epsilon: 0.2041\n",
      "Episode: 318, Total Reward: -8.099999999999998, Epsilon: 0.2031\n",
      "Episode: 319, Total Reward: -5.699999999999998, Epsilon: 0.2021\n",
      "Episode: 320, Total Reward: -7.499999999999998, Epsilon: 0.2011\n",
      "Episode: 321, Total Reward: -5.099999999999998, Epsilon: 0.2001\n",
      "Episode: 322, Total Reward: -0.8999999999999986, Epsilon: 0.1991\n",
      "Episode: 323, Total Reward: -3.899999999999998, Epsilon: 0.1981\n",
      "Episode: 324, Total Reward: -5.699999999999998, Epsilon: 0.1971\n",
      "Episode: 325, Total Reward: -8.099999999999998, Epsilon: 0.1961\n",
      "Episode: 326, Total Reward: -6.899999999999999, Epsilon: 0.1951\n",
      "Episode: 327, Total Reward: -0.8999999999999986, Epsilon: 0.1942\n",
      "Episode: 328, Total Reward: 1.5999999999999996, Epsilon: 0.1932\n",
      "Episode: 329, Total Reward: -5.699999999999998, Epsilon: 0.1922\n",
      "Episode: 330, Total Reward: -0.8999999999999986, Epsilon: 0.1913\n",
      "Episode: 331, Total Reward: -0.8999999999999986, Epsilon: 0.1903\n",
      "Episode: 332, Total Reward: -7.499999999999998, Epsilon: 0.1893\n",
      "Episode: 333, Total Reward: -2.699999999999998, Epsilon: 0.1884\n",
      "Episode: 334, Total Reward: -7.499999999999998, Epsilon: 0.1875\n",
      "Episode: 335, Total Reward: -0.8999999999999986, Epsilon: 0.1865\n",
      "Episode: 336, Total Reward: -3.299999999999998, Epsilon: 0.1856\n",
      "Episode: 337, Total Reward: -8.7, Epsilon: 0.1847\n",
      "Episode: 338, Total Reward: -5.099999999999998, Epsilon: 0.1837\n",
      "Episode: 339, Total Reward: -8.099999999999998, Epsilon: 0.1828\n",
      "Episode: 340, Total Reward: -3.299999999999998, Epsilon: 0.1819\n",
      "Episode: 341, Total Reward: -3.299999999999998, Epsilon: 0.1810\n",
      "Episode: 342, Total Reward: -0.8999999999999986, Epsilon: 0.1801\n",
      "Episode: 343, Total Reward: -4.499999999999998, Epsilon: 0.1792\n",
      "Episode: 344, Total Reward: -0.8999999999999986, Epsilon: 0.1783\n",
      "Episode: 345, Total Reward: -2.099999999999998, Epsilon: 0.1774\n",
      "Episode: 346, Total Reward: -0.8999999999999986, Epsilon: 0.1765\n",
      "Episode: 347, Total Reward: -6.299999999999999, Epsilon: 0.1756\n",
      "Episode: 348, Total Reward: -3.299999999999998, Epsilon: 0.1748\n",
      "Episode: 349, Total Reward: -0.8999999999999986, Epsilon: 0.1739\n",
      "Episode: 350, Total Reward: -1.4999999999999982, Epsilon: 0.1730\n",
      "Episode: 351, Total Reward: -8.7, Epsilon: 0.1721\n",
      "Episode: 352, Total Reward: -6.899999999999999, Epsilon: 0.1713\n",
      "Episode: 353, Total Reward: -8.7, Epsilon: 0.1704\n",
      "Episode: 354, Total Reward: -0.8999999999999986, Epsilon: 0.1696\n",
      "Episode: 355, Total Reward: -3.899999999999998, Epsilon: 0.1687\n",
      "Episode: 356, Total Reward: -2.099999999999998, Epsilon: 0.1679\n",
      "Episode: 357, Total Reward: -4.499999999999998, Epsilon: 0.1670\n",
      "Episode: 358, Total Reward: -5.099999999999998, Epsilon: 0.1662\n",
      "Episode: 359, Total Reward: -7.499999999999998, Epsilon: 0.1654\n",
      "Episode: 360, Total Reward: 2.0999999999999996, Epsilon: 0.1646\n",
      "Episode: 361, Total Reward: -0.8999999999999986, Epsilon: 0.1637\n",
      "Episode: 362, Total Reward: -5.699999999999998, Epsilon: 0.1629\n",
      "Episode: 363, Total Reward: -6.899999999999999, Epsilon: 0.1621\n",
      "Episode: 364, Total Reward: -6.899999999999999, Epsilon: 0.1613\n",
      "Episode: 365, Total Reward: -5.099999999999998, Epsilon: 0.1605\n",
      "Episode: 366, Total Reward: -0.8999999999999986, Epsilon: 0.1597\n",
      "Episode: 367, Total Reward: -5.699999999999998, Epsilon: 0.1589\n",
      "Episode: 368, Total Reward: -0.8999999999999986, Epsilon: 0.1581\n",
      "Episode: 369, Total Reward: -3.299999999999998, Epsilon: 0.1573\n",
      "Episode: 370, Total Reward: -0.8999999999999986, Epsilon: 0.1565\n",
      "Episode: 371, Total Reward: 0.3000000000000007, Epsilon: 0.1557\n",
      "Episode: 372, Total Reward: -0.8999999999999986, Epsilon: 0.1549\n",
      "Episode: 373, Total Reward: -6.299999999999999, Epsilon: 0.1542\n",
      "Episode: 374, Total Reward: -0.8999999999999986, Epsilon: 0.1534\n",
      "Episode: 375, Total Reward: -8.7, Epsilon: 0.1526\n",
      "Episode: 376, Total Reward: -7.499999999999998, Epsilon: 0.1519\n",
      "Episode: 377, Total Reward: -5.099999999999998, Epsilon: 0.1511\n",
      "Episode: 378, Total Reward: -5.099999999999998, Epsilon: 0.1504\n",
      "Episode: 379, Total Reward: -3.299999999999998, Epsilon: 0.1496\n",
      "Episode: 380, Total Reward: -0.8999999999999986, Epsilon: 0.1489\n",
      "Episode: 381, Total Reward: -0.8999999999999986, Epsilon: 0.1481\n",
      "Episode: 382, Total Reward: -0.29999999999999893, Epsilon: 0.1474\n",
      "Episode: 383, Total Reward: -5.699999999999998, Epsilon: 0.1466\n",
      "Episode: 384, Total Reward: -0.8999999999999986, Epsilon: 0.1459\n",
      "Episode: 385, Total Reward: -0.8999999999999986, Epsilon: 0.1452\n",
      "Episode: 386, Total Reward: -0.8999999999999986, Epsilon: 0.1444\n",
      "Episode: 387, Total Reward: 1.5000000000000018, Epsilon: 0.1437\n",
      "Episode: 388, Total Reward: -1.4999999999999982, Epsilon: 0.1430\n",
      "Episode: 389, Total Reward: -4.499999999999998, Epsilon: 0.1423\n",
      "Episode: 390, Total Reward: -6.899999999999999, Epsilon: 0.1416\n",
      "Episode: 391, Total Reward: -4.499999999999998, Epsilon: 0.1409\n",
      "Episode: 392, Total Reward: -5.699999999999998, Epsilon: 0.1402\n",
      "Episode: 393, Total Reward: -7.499999999999998, Epsilon: 0.1395\n",
      "Episode: 394, Total Reward: -0.8999999999999986, Epsilon: 0.1388\n",
      "Episode: 395, Total Reward: -8.099999999999998, Epsilon: 0.1381\n",
      "Episode: 396, Total Reward: -6.899999999999999, Epsilon: 0.1374\n",
      "Episode: 397, Total Reward: -0.29999999999999893, Epsilon: 0.1367\n",
      "Episode: 398, Total Reward: -0.8999999999999986, Epsilon: 0.1360\n",
      "Episode: 399, Total Reward: -8.7, Epsilon: 0.1353\n",
      "Episode: 400, Total Reward: -2.699999999999998, Epsilon: 0.1347\n",
      "Episode: 401, Total Reward: -0.8999999999999986, Epsilon: 0.1340\n",
      "Episode: 402, Total Reward: -7.499999999999998, Epsilon: 0.1333\n",
      "Episode: 403, Total Reward: -0.8999999999999986, Epsilon: 0.1326\n",
      "Episode: 404, Total Reward: -5.699999999999998, Epsilon: 0.1320\n",
      "Episode: 405, Total Reward: 0.6000000000000014, Epsilon: 0.1313\n",
      "Episode: 406, Total Reward: -8.099999999999998, Epsilon: 0.1307\n",
      "Episode: 407, Total Reward: -0.29999999999999893, Epsilon: 0.1300\n",
      "Episode: 408, Total Reward: -0.8999999999999986, Epsilon: 0.1294\n",
      "Episode: 409, Total Reward: 1.4000000000000012, Epsilon: 0.1287\n",
      "Episode: 410, Total Reward: -6.899999999999999, Epsilon: 0.1281\n",
      "Episode: 411, Total Reward: -8.7, Epsilon: 0.1274\n",
      "Episode: 412, Total Reward: -2.099999999999998, Epsilon: 0.1268\n",
      "Episode: 413, Total Reward: -0.8999999999999986, Epsilon: 0.1262\n",
      "Episode: 414, Total Reward: -0.8999999999999986, Epsilon: 0.1255\n",
      "Episode: 415, Total Reward: -0.8999999999999986, Epsilon: 0.1249\n",
      "Episode: 416, Total Reward: -2.099999999999998, Epsilon: 0.1243\n",
      "Episode: 417, Total Reward: -0.8999999999999986, Epsilon: 0.1237\n",
      "Episode: 418, Total Reward: -3.299999999999998, Epsilon: 0.1230\n",
      "Episode: 419, Total Reward: -5.699999999999998, Epsilon: 0.1224\n",
      "Episode: 420, Total Reward: -9.299999999999999, Epsilon: 0.1218\n",
      "Episode: 421, Total Reward: -5.099999999999998, Epsilon: 0.1212\n",
      "Episode: 422, Total Reward: 1.7763568394002505e-15, Epsilon: 0.1206\n",
      "Episode: 423, Total Reward: -2.099999999999998, Epsilon: 0.1200\n",
      "Episode: 424, Total Reward: -8.099999999999998, Epsilon: 0.1194\n",
      "Episode: 425, Total Reward: -8.7, Epsilon: 0.1188\n",
      "Episode: 426, Total Reward: -6.899999999999999, Epsilon: 0.1182\n",
      "Episode: 427, Total Reward: -0.8999999999999986, Epsilon: 0.1176\n",
      "Episode: 428, Total Reward: -6.899999999999999, Epsilon: 0.1170\n",
      "Episode: 429, Total Reward: -5.099999999999998, Epsilon: 0.1164\n",
      "Episode: 430, Total Reward: -0.8999999999999986, Epsilon: 0.1159\n",
      "Episode: 431, Total Reward: -4.499999999999998, Epsilon: 0.1153\n",
      "Episode: 432, Total Reward: -7.499999999999998, Epsilon: 0.1147\n",
      "Episode: 433, Total Reward: -6.899999999999999, Epsilon: 0.1141\n",
      "Episode: 434, Total Reward: -0.5999999999999983, Epsilon: 0.1136\n",
      "Episode: 435, Total Reward: -2.099999999999998, Epsilon: 0.1130\n",
      "Episode: 436, Total Reward: -0.2999999999999985, Epsilon: 0.1124\n",
      "Episode: 437, Total Reward: -4.499999999999998, Epsilon: 0.1119\n",
      "Episode: 438, Total Reward: -0.8999999999999986, Epsilon: 0.1113\n",
      "Episode: 439, Total Reward: -3.299999999999998, Epsilon: 0.1107\n",
      "Episode: 440, Total Reward: -0.8999999999999986, Epsilon: 0.1102\n",
      "Episode: 441, Total Reward: -0.8999999999999986, Epsilon: 0.1096\n",
      "Episode: 442, Total Reward: -1.4999999999999982, Epsilon: 0.1091\n",
      "Episode: 443, Total Reward: -0.8999999999999986, Epsilon: 0.1085\n",
      "Episode: 444, Total Reward: -0.8999999999999986, Epsilon: 0.1080\n",
      "Episode: 445, Total Reward: -0.8999999999999986, Epsilon: 0.1075\n",
      "Episode: 446, Total Reward: -6.899999999999999, Epsilon: 0.1069\n",
      "Episode: 447, Total Reward: 0.20000000000000018, Epsilon: 0.1064\n",
      "Episode: 448, Total Reward: -0.8999999999999986, Epsilon: 0.1059\n",
      "Episode: 449, Total Reward: -0.8999999999999986, Epsilon: 0.1053\n",
      "Episode: 450, Total Reward: -0.8999999999999986, Epsilon: 0.1048\n",
      "Episode: 451, Total Reward: -8.099999999999998, Epsilon: 0.1043\n",
      "Episode: 452, Total Reward: -5.699999999999998, Epsilon: 0.1038\n",
      "Episode: 453, Total Reward: -5.699999999999998, Epsilon: 0.1032\n",
      "Episode: 454, Total Reward: -6.899999999999999, Epsilon: 0.1027\n",
      "Episode: 455, Total Reward: -0.8999999999999986, Epsilon: 0.1022\n",
      "Episode: 456, Total Reward: -0.8999999999999986, Epsilon: 0.1017\n",
      "Episode: 457, Total Reward: -4.499999999999998, Epsilon: 0.1012\n",
      "Episode: 458, Total Reward: -6.299999999999999, Epsilon: 0.1007\n",
      "Episode: 459, Total Reward: -0.8999999999999986, Epsilon: 0.1002\n",
      "Episode: 460, Total Reward: -3.899999999999998, Epsilon: 0.0997\n",
      "Episode: 461, Total Reward: -3.899999999999998, Epsilon: 0.0992\n",
      "Episode: 462, Total Reward: -3.899999999999998, Epsilon: 0.0987\n",
      "Episode: 463, Total Reward: -1.1999999999999984, Epsilon: 0.0982\n",
      "Episode: 464, Total Reward: -6.899999999999999, Epsilon: 0.0977\n",
      "Episode: 465, Total Reward: -1.4999999999999982, Epsilon: 0.0972\n",
      "Episode: 466, Total Reward: -0.8999999999999986, Epsilon: 0.0967\n",
      "Episode: 467, Total Reward: -0.8999999999999986, Epsilon: 0.0962\n",
      "Episode: 468, Total Reward: 0.9000000000000004, Epsilon: 0.0958\n",
      "Episode: 469, Total Reward: -8.7, Epsilon: 0.0953\n",
      "Episode: 470, Total Reward: -0.9999999999999987, Epsilon: 0.0948\n",
      "Episode: 471, Total Reward: -0.8999999999999986, Epsilon: 0.0943\n",
      "Episode: 472, Total Reward: -0.8999999999999986, Epsilon: 0.0939\n",
      "Episode: 473, Total Reward: -0.8999999999999986, Epsilon: 0.0934\n",
      "Episode: 474, Total Reward: -1.4999999999999987, Epsilon: 0.0929\n",
      "Episode: 475, Total Reward: -0.8999999999999986, Epsilon: 0.0925\n",
      "Episode: 476, Total Reward: -2.699999999999998, Epsilon: 0.0920\n",
      "Episode: 477, Total Reward: -4.499999999999998, Epsilon: 0.0915\n",
      "Episode: 478, Total Reward: -6.899999999999999, Epsilon: 0.0911\n",
      "Episode: 479, Total Reward: -7.499999999999998, Epsilon: 0.0906\n",
      "Episode: 480, Total Reward: -5.699999999999998, Epsilon: 0.0902\n",
      "Episode: 481, Total Reward: -6.299999999999999, Epsilon: 0.0897\n",
      "Episode: 482, Total Reward: -0.8999999999999986, Epsilon: 0.0893\n",
      "Episode: 483, Total Reward: -0.8999999999999986, Epsilon: 0.0888\n",
      "Episode: 484, Total Reward: -2.099999999999998, Epsilon: 0.0884\n",
      "Episode: 485, Total Reward: -5.699999999999998, Epsilon: 0.0879\n",
      "Episode: 486, Total Reward: -2.699999999999998, Epsilon: 0.0875\n",
      "Episode: 487, Total Reward: -0.8999999999999986, Epsilon: 0.0871\n",
      "Episode: 488, Total Reward: -0.8999999999999986, Epsilon: 0.0866\n",
      "Episode: 489, Total Reward: -3.299999999999998, Epsilon: 0.0862\n",
      "Episode: 490, Total Reward: -0.8999999999999986, Epsilon: 0.0858\n",
      "Episode: 491, Total Reward: -6.899999999999999, Epsilon: 0.0853\n",
      "Episode: 492, Total Reward: 2.0999999999999996, Epsilon: 0.0849\n",
      "Episode: 493, Total Reward: -6.899999999999999, Epsilon: 0.0845\n",
      "Episode: 494, Total Reward: -5.099999999999998, Epsilon: 0.0841\n",
      "Episode: 495, Total Reward: -8.099999999999998, Epsilon: 0.0836\n",
      "Episode: 496, Total Reward: -2.999999999999997, Epsilon: 0.0832\n",
      "Episode: 497, Total Reward: -4.499999999999998, Epsilon: 0.0828\n",
      "Episode: 498, Total Reward: -0.8999999999999986, Epsilon: 0.0824\n",
      "Episode: 499, Total Reward: 0.3000000000000007, Epsilon: 0.0820\n",
      "Episode: 500, Total Reward: -2.699999999999998, Epsilon: 0.0816\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -8.7\n",
      "Test Episode: 2, Total Reward: -8.7\n",
      "Test Episode: 3, Total Reward: -8.7\n",
      "Test Episode: 4, Total Reward: -8.7\n",
      "Test Episode: 5, Total Reward: -8.7\n",
      "Test Episode: 6, Total Reward: -8.099999999999998\n",
      "Test Episode: 7, Total Reward: -8.7\n",
      "Test Episode: 8, Total Reward: -8.7\n",
      "Test Episode: 9, Total Reward: -8.7\n",
      "Test Episode: 10, Total Reward: -8.099999999999998\n",
      "\n",
      "Average Reward over 10 Test Episodes: -8.58\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\") ##use_lidar=True option avaliable\n",
    "\n",
    "# Create and train DQN agent\n",
    "agent = DQNAgent(env)\n",
    "agent.train()\n",
    "\n",
    "# Test the trained agent\n",
    "agent.test(num_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -8.7\n",
      "Test Episode: 2, Total Reward: -8.7\n",
      "Test Episode: 3, Total Reward: -8.7\n",
      "Test Episode: 4, Total Reward: -8.7\n",
      "Test Episode: 5, Total Reward: -8.099999999999998\n",
      "Test Episode: 6, Total Reward: -8.099999999999998\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 142\u001b[0m, in \u001b[0;36mDQNAgent.test\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    141\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state, testing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 142\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    144\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flappy-gym/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flappy-gym/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flappy-gym/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flappy-gym/lib/python3.10/site-packages/flappy_bird_gymnasium/envs/flappy_bird_env.py:260\u001b[0m, in \u001b[0;36mFlappyBirdEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    257\u001b[0m         low_pipe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_low_pipe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m obs, reward_private_zone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation()\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flappy-gym/lib/python3.10/site-packages/flappy_bird_gymnasium/envs/flappy_bird_env.py:407\u001b[0m, in \u001b[0;36mFlappyBirdEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_display()\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_display()\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fps_clock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.test(num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_customization(update_step=0):\n",
    "    \"\"\"Override the step method dynamically based on update_option.\"\"\"\n",
    "    # Save the original step method\n",
    "    original_step = FlappyBirdEnv.step\n",
    "\n",
    "    def custom_step(self, action):\n",
    "        \"\"\"Custom step method to modify the reward for staying alive.\"\"\"\n",
    "        if update_step == 0:\n",
    "            # Use the original step method\n",
    "            return original_step(self, action)\n",
    "        if update_step ==1:\n",
    "            # Custom step logic (update_option != 0)\n",
    "            terminal = False\n",
    "            reward = None\n",
    "\n",
    "            self._sound_cache = None\n",
    "            if action == self.Actions.FLAP:\n",
    "                if self._player_y > -2 * self.PLAYER_HEIGHT:\n",
    "                    self._player_vel_y = self.PLAYER_FLAP_ACC\n",
    "                    self._player_flapped = True\n",
    "                    self._sound_cache = \"wing\"\n",
    "\n",
    "            # Check for score\n",
    "            player_mid_pos = self._player_x + self.PLAYER_WIDTH / 2\n",
    "            for pipe in self._upper_pipes:\n",
    "                pipe_mid_pos = pipe[\"x\"] + self.PIPE_WIDTH / 2\n",
    "                if pipe_mid_pos <= player_mid_pos < pipe_mid_pos + 4:\n",
    "                    self._score += 1\n",
    "                    reward = 1  # reward for passed pipe\n",
    "                    self._sound_cache = \"point\"\n",
    "\n",
    "            # Player movement and rotation\n",
    "            if self._player_rot > -90:\n",
    "                self._player_rot -= self.PLAYER_VEL_ROT\n",
    "            if self._player_vel_y < self.PLAYER_MAX_VEL_Y and not self._player_flapped:\n",
    "                self._player_vel_y += self.PLAYER_ACC_Y\n",
    "            if self._player_flapped:\n",
    "                self._player_flapped = False\n",
    "                self._player_rot = 45\n",
    "            self._player_y += min(\n",
    "                self._player_vel_y, self._ground[\"y\"] - self._player_y - self.PLAYER_HEIGHT\n",
    "            )\n",
    "\n",
    "            # Move pipes\n",
    "            for up_pipe, low_pipe in zip(self._upper_pipes, self._lower_pipes):\n",
    "                up_pipe[\"x\"] += self.PIPE_VEL_X\n",
    "                low_pipe[\"x\"] += self.PIPE_VEL_X\n",
    "                if up_pipe[\"x\"] < -self.PIPE_WIDTH:\n",
    "                    new_up_pipe, new_low_pipe = self._get_random_pipe()\n",
    "                    up_pipe.update(new_up_pipe)\n",
    "                    low_pipe.update(new_low_pipe)\n",
    "\n",
    "            if self.render_mode == \"human\":\n",
    "                self.render()\n",
    "\n",
    "            obs, reward_private_zone = self._get_observation()\n",
    "            if reward is None:\n",
    "                if reward_private_zone is not None:\n",
    "                    reward = reward_private_zone\n",
    "                else:\n",
    "                    reward = 0.1  # Change reward for staying alive\n",
    "\n",
    "            # Agent touches the top of the screen\n",
    "            if self._player_y < 0 or self._player_y + self.PLAYER_HEIGHT > self._ground[\"y\"]:\n",
    "                reward = -1.0\n",
    "                terminal = True\n",
    "\n",
    "            # Check for crash\n",
    "            if self._check_crash():\n",
    "                self._sound_cache = \"hit\"\n",
    "                reward = -1  # reward for dying\n",
    "                terminal = True\n",
    "                self._player_vel_y = 0\n",
    "\n",
    "            info = {\"score\": self._score}\n",
    "\n",
    "            return (\n",
    "                obs,\n",
    "                reward,\n",
    "                terminal,\n",
    "                (self._score_limit is not None) and (self._score >= self._score_limit),\n",
    "                info,\n",
    "            )\n",
    "\n",
    "        # Override the step method in FlappyBirdEnv\n",
    "        FlappyBirdEnv.step = custom_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
