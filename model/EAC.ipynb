{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import pygame\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Grayscale\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs import utils\n",
    "from flappy_bird_gymnasium.envs.lidar import LIDAR\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    BACKGROUND_WIDTH,\n",
    "    BASE_WIDTH,\n",
    "    FILL_BACKGROUND_COLOR,\n",
    "    LIDAR_MAX_DISTANCE,\n",
    "    PIPE_HEIGHT,\n",
    "    PIPE_VEL_X,\n",
    "    PIPE_WIDTH,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_PRIVATE_ZONE,\n",
    "    PLAYER_ROT_THR,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    ")\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "\n",
    "def new_render(self):\n",
    "    \"\"\"Renders the next frame.\"\"\"\n",
    "    if self.render_mode == \"rgb_array\":\n",
    "        self._draw_surface(show_score=False, show_rays=False)\n",
    "        # Flip the image to retrieve a correct aspect\n",
    "        return np.transpose(pygame.surfarray.array3d(self._surface), axes=(1, 0, 2))\n",
    "    else:\n",
    "        self._draw_surface(show_score=True, show_rays=False)\n",
    "        if self._display is None:\n",
    "            self._make_display()\n",
    "\n",
    "        self._update_display()\n",
    "        self._fps_clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "def new_step(\n",
    "    self,\n",
    "    action: Union[Actions, int],\n",
    ") -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "    \"\"\"Given an action, updates the game state.\n",
    "\n",
    "    Args:\n",
    "        action (Union[FlappyBirdLogic.Actions, int]): The action taken by\n",
    "            the agent. Zero (0) means \"do nothing\" and one (1) means \"flap\".\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing, respectively:\n",
    "\n",
    "            * an observation (horizontal distance to the next pipe\n",
    "                difference between the player's y position and the next hole's\n",
    "                y position)\n",
    "            * a reward (alive = +0.1, pipe = +1.0, dead = -1.0)\n",
    "            * a status report (`True` if the game is over and `False`\n",
    "                otherwise)\n",
    "            * an info dictionary\n",
    "    \"\"\"\n",
    "    \"\"\"Given an action taken by the player, updates the game's state.\n",
    "\n",
    "    Args:\n",
    "        action (Union[FlappyBirdLogic.Actions, int]): The action taken by\n",
    "            the player.\n",
    "\n",
    "    Returns:\n",
    "        `True` if the player is alive and `False` otherwise.\n",
    "    \"\"\"\n",
    "    terminal = False\n",
    "    reward = None\n",
    "\n",
    "    self._sound_cache = None\n",
    "    if action == Actions.FLAP:\n",
    "        if self._player_y > -2 * PLAYER_HEIGHT:\n",
    "            self._player_vel_y = PLAYER_FLAP_ACC\n",
    "            self._player_flapped = True\n",
    "            self._sound_cache = \"wing\"\n",
    "\n",
    "    # check for score\n",
    "    player_mid_pos = self._player_x + PLAYER_WIDTH / 2\n",
    "    for pipe in self._upper_pipes:\n",
    "        pipe_mid_pos = pipe[\"x\"] + PIPE_WIDTH / 2\n",
    "        if pipe_mid_pos <= player_mid_pos < pipe_mid_pos + 4:\n",
    "            self._score += 1\n",
    "            reward = 1.0  # reward for passed pipe\n",
    "            self._sound_cache = \"point\"\n",
    "\n",
    "    # player_index base_x change\n",
    "    if (self._loop_iter + 1) % 3 == 0:\n",
    "        self._player_idx = next(self._player_idx_gen)\n",
    "\n",
    "    self._loop_iter = (self._loop_iter + 1) % 30\n",
    "    self._ground[\"x\"] = -((-self._ground[\"x\"] + 100) % self._base_shift)\n",
    "\n",
    "    # rotate the player\n",
    "    if self._player_rot > -90:\n",
    "        self._player_rot -= PLAYER_VEL_ROT\n",
    "\n",
    "    # player's movement\n",
    "    if self._player_vel_y < PLAYER_MAX_VEL_Y and not self._player_flapped:\n",
    "        self._player_vel_y += PLAYER_ACC_Y\n",
    "\n",
    "    if self._player_flapped:\n",
    "        self._player_flapped = False\n",
    "\n",
    "        # more rotation to cover the threshold\n",
    "        # (calculated in visible rotation)\n",
    "        self._player_rot = 45\n",
    "\n",
    "    self._player_y += min(\n",
    "        self._player_vel_y, self._ground[\"y\"] - self._player_y - PLAYER_HEIGHT\n",
    "    )\n",
    "\n",
    "    # move pipes to left\n",
    "    for up_pipe, low_pipe in zip(self._upper_pipes, self._lower_pipes):\n",
    "        up_pipe[\"x\"] += PIPE_VEL_X\n",
    "        low_pipe[\"x\"] += PIPE_VEL_X\n",
    "\n",
    "        # it is out of the screen\n",
    "        if up_pipe[\"x\"] < -PIPE_WIDTH:\n",
    "            new_up_pipe, new_low_pipe = self._get_random_pipe()\n",
    "            up_pipe[\"x\"] = new_up_pipe[\"x\"]\n",
    "            up_pipe[\"y\"] = new_up_pipe[\"y\"]\n",
    "            low_pipe[\"x\"] = new_low_pipe[\"x\"]\n",
    "            low_pipe[\"y\"] = new_low_pipe[\"y\"]\n",
    "\n",
    "    if self.render_mode == \"human\":\n",
    "        self.render()\n",
    "\n",
    "    obs, reward_private_zone = self._get_observation()\n",
    "    if reward is None:\n",
    "        if reward_private_zone is not None:\n",
    "            reward = float(reward_private_zone)\n",
    "        else:\n",
    "            reward = 0.1  # reward for staying alive\n",
    "\n",
    "    # check\n",
    "    if self._debug and self._use_lidar:\n",
    "        # sort pipes by the distance between pipe and agent\n",
    "        up_pipe = sorted(\n",
    "            self._upper_pipes,\n",
    "            key=lambda x: np.sqrt(\n",
    "                (self._player_x - x[\"x\"]) ** 2\n",
    "                + (self._player_y - (x[\"y\"] + PIPE_HEIGHT)) ** 2\n",
    "            ),\n",
    "        )[0]\n",
    "        # find ray closest to the obstacle\n",
    "        min_index = np.argmin(obs)\n",
    "        min_value = obs[min_index] * LIDAR_MAX_DISTANCE\n",
    "        # mean approach to the obstacle\n",
    "        if \"pipe_mean_value\" in self._statistics:\n",
    "            self._statistics[\"pipe_mean_value\"] = self._statistics[\n",
    "                \"pipe_mean_value\"\n",
    "            ] * 0.99 + min_value * (1 - 0.99)\n",
    "        else:\n",
    "            self._statistics[\"pipe_mean_value\"] = min_value\n",
    "\n",
    "        # Nearest to the pipe\n",
    "        if \"pipe_min_value\" in self._statistics:\n",
    "            if min_value < self._statistics[\"pipe_min_value\"]:\n",
    "                self._statistics[\"pipe_min_value\"] = min_value\n",
    "                self._statistics[\"pipe_min_index\"] = min_index\n",
    "        else:\n",
    "            self._statistics[\"pipe_min_value\"] = min_value\n",
    "            self._statistics[\"pipe_min_index\"] = min_index\n",
    "\n",
    "        # Nearest to the ground\n",
    "        diff = np.abs(self._player_y - self._ground[\"y\"])\n",
    "        if \"ground_min_value\" in self._statistics:\n",
    "            if diff < self._statistics[\"ground_min_value\"]:\n",
    "                self._statistics[\"ground_min_value\"] = diff\n",
    "        else:\n",
    "            self._statistics[\"ground_min_value\"] = diff\n",
    "\n",
    "    # agent touch the top of the screen as punishment\n",
    "    if self._player_y < 0:\n",
    "        reward = -0.5\n",
    "\n",
    "    # check for crash\n",
    "    if self._check_crash():\n",
    "        self._sound_cache = \"hit\"\n",
    "        reward = -1.0  # reward for dying\n",
    "        terminal = True\n",
    "        self._player_vel_y = 0\n",
    "        if self._debug and self._use_lidar:\n",
    "            if ((self._player_x + PLAYER_WIDTH) - up_pipe[\"x\"]) > (0 + 5) and (\n",
    "                self._player_x - up_pipe[\"x\"]\n",
    "            ) < PIPE_WIDTH:\n",
    "                print(\"BETWEEN PIPES\")\n",
    "            elif ((self._player_x + PLAYER_WIDTH) - up_pipe[\"x\"]) < (0 + 5):\n",
    "                print(\"IN FRONT OF\")\n",
    "            print(\n",
    "                f\"obs: [{self._statistics['pipe_min_index']},\"\n",
    "                f\"{self._statistics['pipe_min_value']},\"\n",
    "                f\"{self._statistics['pipe_mean_value']}],\"\n",
    "                f\"Ground: {self._statistics['ground_min_value']}\"\n",
    "            )\n",
    "\n",
    "    info = {\"score\": self._score}\n",
    "\n",
    "    return (\n",
    "        obs,\n",
    "        np.float32(reward),\n",
    "        terminal,\n",
    "        (self._score_limit is not None) and (self._score >= self._score_limit),\n",
    "        info,\n",
    "    )\n",
    "\n",
    "\n",
    "FlappyBirdEnv.render = new_render\n",
    "FlappyBirdEnv.step = new_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "######EAC+######\n",
    "################\n",
    "\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, action_space)\n",
    "        )\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        return F.softmax(self.actor(shared), dim=-1), self.critic(shared)\n",
    "\n",
    "class EAC_agent:\n",
    "    def __init__(self, env, config):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        # Parameters\n",
    "        self.actor_lr = config.get('actor_lr', 0.001)\n",
    "        self.critic_lr = config.get('critic_lr', 0.005)\n",
    "        self.gamma = config.get('gamma', 0.99)\n",
    "        self.gae_lambda = config.get('gae_lambda', 0.95)\n",
    "        self.entropy_coef = config.get('entropy_coef', 0.01)\n",
    "        self.value_loss_coef = config.get('value_loss_coef', 0.5)\n",
    "        self.max_grad_norm = config.get('max_grad_norm', 0.5)\n",
    "        self.episodes = config.get('episodes', 5000)\n",
    "        \n",
    "        self.network = ActorCriticNet(self.state_dim, self.action_space).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.network.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.network.parameters(), lr=self.critic_lr)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.masks = []\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs, value = self.network(state)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "    def compute_gae(self):\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            if step == len(self.rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = self.values[step + 1]\n",
    "                \n",
    "            delta = self.rewards[step] + self.gamma * next_value * self.masks[step] - self.values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * self.masks[step] * gae\n",
    "            returns.insert(0, gae + self.values[step])\n",
    "            \n",
    "        return torch.FloatTensor(returns).to(self.device)\n",
    "\n",
    "    def train_step(self):\n",
    "        returns = self.compute_gae()\n",
    "        \n",
    "        values = torch.FloatTensor(self.values).to(self.device)\n",
    "        log_probs = torch.FloatTensor(self.log_probs).to(self.device)\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        states = torch.FloatTensor(self.states).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        new_probs, new_values = self.network(states)\n",
    "        dist = Categorical(new_probs)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Actor loss\n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        actor_loss = -(surr1.mean() + self.entropy_coef * entropy)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = self.value_loss_coef * F.mse_loss(new_values.squeeze(-1), returns)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Clear memory\n",
    "        self.log_probs.clear()\n",
    "        self.values.clear()\n",
    "        self.rewards.clear()\n",
    "        self.masks.clear()\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "\n",
    "    def train(self):\n",
    "        rewards_history = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            self.states = []\n",
    "            self.actions = []\n",
    "            \n",
    "            while not done:\n",
    "                action, log_prob, value = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                \n",
    "                self.states.append(state)\n",
    "                self.actions.append(action)\n",
    "                self.rewards.append(reward)\n",
    "                self.log_probs.append(log_prob)\n",
    "                self.values.append(value)\n",
    "                self.masks.append(1 - done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            self.train_step()\n",
    "            rewards_history.append(total_reward)\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(rewards_history[-100:])\n",
    "                print(f\"Episode {episode}, Reward: {total_reward:.2f}, Avg: {avg_reward:.2f}\")\n",
    "        \n",
    "        return rewards_history\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render,use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    probs, _ = self.network(state)\n",
    "                action = probs.argmax().item()\n",
    "                state, reward, done, truncated, _ = test_env.step(action)\n",
    "                total_reward += reward\n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode {episode}, Total Reward: {total_reward}\")\n",
    "        \n",
    "        print(f\"Average Test Reward: {np.mean(total_rewards):.2f}\")\n",
    "        return total_rewards\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -3.40, Avg: -3.40\n",
      "Episode 100, Reward: -5.20, Avg: -5.06\n",
      "Episode 200, Reward: -2.20, Avg: -2.02\n",
      "Episode 300, Reward: -1.60, Avg: -1.96\n",
      "Episode 400, Reward: -2.20, Avg: -1.95\n",
      "Episode 500, Reward: -1.60, Avg: -1.95\n",
      "Episode 600, Reward: -1.60, Avg: -1.91\n",
      "Episode 700, Reward: -1.60, Avg: -1.97\n",
      "Episode 800, Reward: -2.20, Avg: -2.00\n",
      "Episode 900, Reward: -1.60, Avg: -2.02\n",
      "Episode 1000, Reward: -2.20, Avg: -1.90\n",
      "Episode 1100, Reward: -2.20, Avg: -1.98\n",
      "Episode 1200, Reward: -2.20, Avg: -1.96\n",
      "Episode 1300, Reward: -1.60, Avg: -1.98\n",
      "Episode 1400, Reward: -1.60, Avg: -1.97\n",
      "Episode 1500, Reward: -1.60, Avg: -1.92\n",
      "Episode 1600, Reward: -1.60, Avg: -1.96\n",
      "Episode 1700, Reward: -2.20, Avg: -2.00\n",
      "Episode 1800, Reward: -1.60, Avg: -1.97\n",
      "Episode 1900, Reward: -2.20, Avg: -1.97\n",
      "Episode 2000, Reward: -1.60, Avg: -1.98\n",
      "Episode 2100, Reward: -1.60, Avg: -2.00\n",
      "Episode 2200, Reward: -2.20, Avg: -1.95\n",
      "Episode 2300, Reward: -2.20, Avg: -1.95\n",
      "Episode 2400, Reward: -1.60, Avg: -2.03\n",
      "Episode 2500, Reward: -1.60, Avg: -1.94\n",
      "Episode 2600, Reward: -1.60, Avg: -1.98\n",
      "Episode 2700, Reward: -2.20, Avg: -1.99\n",
      "Episode 2800, Reward: -2.20, Avg: -2.01\n",
      "Episode 2900, Reward: -2.20, Avg: -2.00\n",
      "Episode 3000, Reward: -2.20, Avg: -2.03\n",
      "Episode 3100, Reward: -2.20, Avg: -1.98\n",
      "Episode 3200, Reward: -1.60, Avg: -1.94\n",
      "Episode 3300, Reward: -2.20, Avg: -1.96\n",
      "Episode 3400, Reward: -1.60, Avg: -1.92\n",
      "Episode 3500, Reward: -1.60, Avg: -1.95\n",
      "Episode 3600, Reward: -2.20, Avg: -2.00\n",
      "Episode 3700, Reward: -2.20, Avg: -2.02\n",
      "Episode 3800, Reward: -2.20, Avg: -1.95\n",
      "Episode 3900, Reward: -1.60, Avg: -1.98\n",
      "Episode 4000, Reward: -2.20, Avg: -2.02\n",
      "Episode 4100, Reward: -2.20, Avg: -2.02\n",
      "Episode 4200, Reward: -2.20, Avg: -1.97\n",
      "Episode 4300, Reward: -1.60, Avg: -1.95\n",
      "Episode 4400, Reward: -1.60, Avg: -2.01\n",
      "Episode 4500, Reward: -2.20, Avg: -1.96\n",
      "Episode 4600, Reward: -1.60, Avg: -2.00\n",
      "Episode 4700, Reward: -1.60, Avg: -1.95\n",
      "Episode 4800, Reward: -2.20, Avg: -2.02\n",
      "Episode 4900, Reward: -2.20, Avg: -2.00\n",
      "Episode 5000, Reward: -2.20, Avg: -1.89\n",
      "Episode 5100, Reward: -2.20, Avg: -1.99\n",
      "Episode 5200, Reward: -2.20, Avg: -2.01\n",
      "Episode 5300, Reward: -1.60, Avg: -2.00\n",
      "Episode 5400, Reward: -1.60, Avg: -1.94\n",
      "Episode 5500, Reward: -2.20, Avg: -1.94\n",
      "Episode 5600, Reward: -1.60, Avg: -1.94\n",
      "Episode 5700, Reward: -2.20, Avg: -2.02\n",
      "Episode 5800, Reward: -1.60, Avg: -2.00\n",
      "Episode 5900, Reward: -2.20, Avg: -1.97\n",
      "Episode 6000, Reward: -1.60, Avg: -1.98\n",
      "Episode 6100, Reward: -2.20, Avg: -1.98\n",
      "Episode 6200, Reward: -2.20, Avg: -1.98\n",
      "Episode 6300, Reward: -2.20, Avg: -2.00\n",
      "Episode 6400, Reward: -1.60, Avg: -1.97\n",
      "Episode 6500, Reward: -2.20, Avg: -2.02\n",
      "Episode 6600, Reward: -2.20, Avg: -1.95\n",
      "Episode 6700, Reward: -1.60, Avg: -1.97\n",
      "Episode 6800, Reward: -2.20, Avg: -2.00\n",
      "Episode 6900, Reward: -2.20, Avg: -1.96\n",
      "Episode 7000, Reward: -2.20, Avg: -1.93\n",
      "Episode 7100, Reward: -2.20, Avg: -2.01\n",
      "Episode 7200, Reward: -2.20, Avg: -2.04\n",
      "Episode 7300, Reward: -2.20, Avg: -1.96\n",
      "Episode 7400, Reward: -1.60, Avg: -1.97\n",
      "Episode 7500, Reward: -1.60, Avg: -1.98\n",
      "Episode 7600, Reward: -1.60, Avg: -2.02\n",
      "Episode 7700, Reward: -2.20, Avg: -2.00\n",
      "Episode 7800, Reward: -2.20, Avg: -2.00\n",
      "Episode 7900, Reward: -2.20, Avg: -1.97\n",
      "Episode 8000, Reward: -2.20, Avg: -1.96\n",
      "Episode 8100, Reward: -1.60, Avg: -1.97\n",
      "Episode 8200, Reward: -2.20, Avg: -1.94\n",
      "Episode 8300, Reward: -2.20, Avg: -2.01\n",
      "Episode 8400, Reward: -1.60, Avg: -2.00\n",
      "Episode 8500, Reward: -1.60, Avg: -2.04\n",
      "Episode 8600, Reward: -1.60, Avg: -1.95\n",
      "Episode 8700, Reward: -1.60, Avg: -2.01\n",
      "Episode 8800, Reward: -1.60, Avg: -1.98\n",
      "Episode 8900, Reward: -1.60, Avg: -2.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m     agent \u001b[38;5;241m=\u001b[39m EAC_plus_agent(env, current_hyperparams)\n\u001b[1;32m     52\u001b[0m     exp_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c_lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcritic_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gae=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgae_lambda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ent=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentropy_coef\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_val=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_loss_coef\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m     exp_res[exp_key] \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     test_res[exp_key] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtest(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m     56\u001b[0m  \u001b[38;5;66;03m#    with open(\"ac_exp_res.pkl\", \"wb\") as f:\u001b[39;00m\n\u001b[1;32m     57\u001b[0m  \u001b[38;5;66;03m#        pickle.dump(exp_res, f)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m  \u001b[38;5;66;03m#    with open(\"ac_test_res.pkl\", \"wb\") as f:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[16], line 138\u001b[0m, in \u001b[0;36mEAC_plus_agent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 138\u001b[0m     action, log_prob, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "Cell \u001b[0;32mIn[16], line 62\u001b[0m, in \u001b[0;36mEAC_plus_agent.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     60\u001b[0m     probs, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(state)\n\u001b[1;32m     61\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m---> 62\u001b[0m     action \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle(\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), log_prob\u001b[38;5;241m.\u001b[39mitem(), value\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/torch/distributions/categorical.py:134\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    132\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    133\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 134\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "\n",
    "# Real\n",
    "hyper = {\n",
    "   \"actor_lr\": [0.0001, 0.0005],\n",
    "   \"critic_lr\": [0.001, 0.005], \n",
    "   \"gamma\": 0.99,\n",
    "   \"gae_lambda\": [0.9, 0.95],\n",
    "   \"entropy_coef\": [0.01, 0.05],\n",
    "   \"value_loss_coef\": [0.5, 1.0],\n",
    "   \"max_grad_norm\": 0.5,\n",
    "   \"episodes\": 20000\n",
    "}\n",
    "\n",
    "# For test\n",
    "# hyper = {\n",
    "#    \"actor_lr\": [0.0005],\n",
    "#    \"critic_lr\": [0.005], \n",
    "#    \"gamma\": 0.99,\n",
    "#    \"gae_lambda\": [0.9, 0.95],\n",
    "#    \"entropy_coef\": [0.01, 0.05],\n",
    "#    \"value_loss_coef\": [0.5, 1.0],\n",
    "#    \"max_grad_norm\": 0.5,\n",
    "#    \"episodes\": 200\n",
    "# }\n",
    "\n",
    "param_combinations = itertools.product(\n",
    "   hyper[\"actor_lr\"],\n",
    "   hyper[\"critic_lr\"], \n",
    "   hyper[\"gae_lambda\"],\n",
    "   hyper[\"entropy_coef\"],\n",
    "   hyper[\"value_loss_coef\"]\n",
    ")\n",
    "\n",
    "exp_res = {}\n",
    "test_res = {}\n",
    "\n",
    "for actor_lr, critic_lr, gae_lambda, entropy_coef, value_loss_coef in param_combinations:\n",
    "   current_hyperparams = {\n",
    "       \"actor_lr\": actor_lr,\n",
    "       \"critic_lr\": critic_lr,\n",
    "       \"gamma\": hyper[\"gamma\"],\n",
    "       \"gae_lambda\": gae_lambda,\n",
    "       \"entropy_coef\": entropy_coef,\n",
    "       \"value_loss_coef\": value_loss_coef,\n",
    "       \"max_grad_norm\": hyper[\"max_grad_norm\"],\n",
    "       \"episodes\": hyper[\"episodes\"]\n",
    "   }\n",
    "   \n",
    "   try:\n",
    "       agent = EAC_agent(env, current_hyperparams)\n",
    "       exp_key = f\"a_lr={actor_lr}_c_lr={critic_lr}_gae={gae_lambda}_ent={entropy_coef}_val={value_loss_coef}\"\n",
    "       exp_res[exp_key] = agent.train()\n",
    "       test_res[exp_key] = agent.test(num_episodes=5000,render=None)\n",
    "       \n",
    "    #    with open(\"ac_exp_res.pkl\", \"wb\") as f:\n",
    "    #        pickle.dump(exp_res, f)\n",
    "    #    with open(\"ac_test_res.pkl\", \"wb\") as f:\n",
    "    #        pickle.dump(test_res, f)\n",
    "           \n",
    "    #    print(f\"Completed: {exp_key}\")\n",
    "       \n",
    "   finally:\n",
    "       del agent\n",
    "       gc.collect()\n",
    "       env.close()\n",
    "\n",
    "with open(\"ac_exp_res.pkl\", \"rb\") as f:\n",
    "   exp_res = pickle.load(f)\n",
    "with open(\"ac_test_res.pkl\", \"rb\") as f:\n",
    "   test_res = pickle.load(f)\n",
    "\n",
    "print(\"Experiment complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
