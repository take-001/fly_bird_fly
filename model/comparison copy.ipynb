{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import gc\n",
    "import numpy as np\n",
    "import pygame\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Grayscale\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs import utils\n",
    "from flappy_bird_gymnasium.envs.lidar import LIDAR\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    BACKGROUND_WIDTH,\n",
    "    BASE_WIDTH,\n",
    "    FILL_BACKGROUND_COLOR,\n",
    "    LIDAR_MAX_DISTANCE,\n",
    "    PIPE_HEIGHT,\n",
    "    PIPE_VEL_X,\n",
    "    PIPE_WIDTH,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_PRIVATE_ZONE,\n",
    "    PLAYER_ROT_THR,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    ")\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "\n",
    "def new_render(self):\n",
    "    \"\"\"Renders the next frame.\"\"\"\n",
    "    if self.render_mode == \"rgb_array\":\n",
    "        self._draw_surface(show_score=False, show_rays=False)\n",
    "        # Flip the image to retrieve a correct aspect\n",
    "        return np.transpose(pygame.surfarray.array3d(self._surface), axes=(1, 0, 2))\n",
    "    else:\n",
    "        self._draw_surface(show_score=True, show_rays=False)\n",
    "        if self._display is None:\n",
    "            self._make_display()\n",
    "\n",
    "        self._update_display()\n",
    "        self._fps_clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "def new_step(\n",
    "    self,\n",
    "    action: Union[Actions, int],\n",
    ") -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "    \"\"\"Given an action, updates the game state.\n",
    "\n",
    "    Args:\n",
    "        action (Union[FlappyBirdLogic.Actions, int]): The action taken by\n",
    "            the agent. Zero (0) means \"do nothing\" and one (1) means \"flap\".\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing, respectively:\n",
    "\n",
    "            * an observation (horizontal distance to the next pipe\n",
    "                difference between the player's y position and the next hole's\n",
    "                y position)\n",
    "            * a reward (alive = +0.1, pipe = +1.0, dead = -1.0)\n",
    "            * a status report (`True` if the game is over and `False`\n",
    "                otherwise)\n",
    "            * an info dictionary\n",
    "    \"\"\"\n",
    "    \"\"\"Given an action taken by the player, updates the game's state.\n",
    "\n",
    "    Args:\n",
    "        action (Union[FlappyBirdLogic.Actions, int]): The action taken by\n",
    "            the player.\n",
    "\n",
    "    Returns:\n",
    "        `True` if the player is alive and `False` otherwise.\n",
    "    \"\"\"\n",
    "    terminal = False\n",
    "    reward = None\n",
    "\n",
    "    self._sound_cache = None\n",
    "    if action == Actions.FLAP:\n",
    "        if self._player_y > -2 * PLAYER_HEIGHT:\n",
    "            self._player_vel_y = PLAYER_FLAP_ACC\n",
    "            self._player_flapped = True\n",
    "            self._sound_cache = \"wing\"\n",
    "\n",
    "    # check for score\n",
    "    player_mid_pos = self._player_x + PLAYER_WIDTH / 2\n",
    "    for pipe in self._upper_pipes:\n",
    "        pipe_mid_pos = pipe[\"x\"] + PIPE_WIDTH / 2\n",
    "        if pipe_mid_pos <= player_mid_pos < pipe_mid_pos + 4:\n",
    "            self._score += 1\n",
    "            reward = 1.0  # reward for passed pipe\n",
    "            self._sound_cache = \"point\"\n",
    "\n",
    "    # player_index base_x change\n",
    "    if (self._loop_iter + 1) % 3 == 0:\n",
    "        self._player_idx = next(self._player_idx_gen)\n",
    "\n",
    "    self._loop_iter = (self._loop_iter + 1) % 30\n",
    "    self._ground[\"x\"] = -((-self._ground[\"x\"] + 100) % self._base_shift)\n",
    "\n",
    "    # rotate the player\n",
    "    if self._player_rot > -90:\n",
    "        self._player_rot -= PLAYER_VEL_ROT\n",
    "\n",
    "    # player's movement\n",
    "    if self._player_vel_y < PLAYER_MAX_VEL_Y and not self._player_flapped:\n",
    "        self._player_vel_y += PLAYER_ACC_Y\n",
    "\n",
    "    if self._player_flapped:\n",
    "        self._player_flapped = False\n",
    "\n",
    "        # more rotation to cover the threshold\n",
    "        # (calculated in visible rotation)\n",
    "        self._player_rot = 45\n",
    "\n",
    "    self._player_y += min(\n",
    "        self._player_vel_y, self._ground[\"y\"] - self._player_y - PLAYER_HEIGHT\n",
    "    )\n",
    "\n",
    "    # move pipes to left\n",
    "    for up_pipe, low_pipe in zip(self._upper_pipes, self._lower_pipes):\n",
    "        up_pipe[\"x\"] += PIPE_VEL_X\n",
    "        low_pipe[\"x\"] += PIPE_VEL_X\n",
    "\n",
    "        # it is out of the screen\n",
    "        if up_pipe[\"x\"] < -PIPE_WIDTH:\n",
    "            new_up_pipe, new_low_pipe = self._get_random_pipe()\n",
    "            up_pipe[\"x\"] = new_up_pipe[\"x\"]\n",
    "            up_pipe[\"y\"] = new_up_pipe[\"y\"]\n",
    "            low_pipe[\"x\"] = new_low_pipe[\"x\"]\n",
    "            low_pipe[\"y\"] = new_low_pipe[\"y\"]\n",
    "\n",
    "    if self.render_mode == \"human\":\n",
    "        self.render()\n",
    "\n",
    "    obs, reward_private_zone = self._get_observation()\n",
    "    if reward is None:\n",
    "        if reward_private_zone is not None:\n",
    "            reward = float(reward_private_zone)\n",
    "        else:\n",
    "            reward = 0.1  # reward for staying alive\n",
    "\n",
    "    # check\n",
    "    if self._debug and self._use_lidar:\n",
    "        # sort pipes by the distance between pipe and agent\n",
    "        up_pipe = sorted(\n",
    "            self._upper_pipes,\n",
    "            key=lambda x: np.sqrt(\n",
    "                (self._player_x - x[\"x\"]) ** 2\n",
    "                + (self._player_y - (x[\"y\"] + PIPE_HEIGHT)) ** 2\n",
    "            ),\n",
    "        )[0]\n",
    "        # find ray closest to the obstacle\n",
    "        min_index = np.argmin(obs)\n",
    "        min_value = obs[min_index] * LIDAR_MAX_DISTANCE\n",
    "        # mean approach to the obstacle\n",
    "        if \"pipe_mean_value\" in self._statistics:\n",
    "            self._statistics[\"pipe_mean_value\"] = self._statistics[\n",
    "                \"pipe_mean_value\"\n",
    "            ] * 0.99 + min_value * (1 - 0.99)\n",
    "        else:\n",
    "            self._statistics[\"pipe_mean_value\"] = min_value\n",
    "\n",
    "        # Nearest to the pipe\n",
    "        if \"pipe_min_value\" in self._statistics:\n",
    "            if min_value < self._statistics[\"pipe_min_value\"]:\n",
    "                self._statistics[\"pipe_min_value\"] = min_value\n",
    "                self._statistics[\"pipe_min_index\"] = min_index\n",
    "        else:\n",
    "            self._statistics[\"pipe_min_value\"] = min_value\n",
    "            self._statistics[\"pipe_min_index\"] = min_index\n",
    "\n",
    "        # Nearest to the ground\n",
    "        diff = np.abs(self._player_y - self._ground[\"y\"])\n",
    "        if \"ground_min_value\" in self._statistics:\n",
    "            if diff < self._statistics[\"ground_min_value\"]:\n",
    "                self._statistics[\"ground_min_value\"] = diff\n",
    "        else:\n",
    "            self._statistics[\"ground_min_value\"] = diff\n",
    "\n",
    "    # agent touch the top of the screen as punishment\n",
    "    if self._player_y < 0:\n",
    "        reward = -0.5\n",
    "\n",
    "    # check for crash\n",
    "    if self._check_crash():\n",
    "        self._sound_cache = \"hit\"\n",
    "        reward = -1.0  # reward for dying\n",
    "        terminal = True\n",
    "        self._player_vel_y = 0\n",
    "        if self._debug and self._use_lidar:\n",
    "            if ((self._player_x + PLAYER_WIDTH) - up_pipe[\"x\"]) > (0 + 5) and (\n",
    "                self._player_x - up_pipe[\"x\"]\n",
    "            ) < PIPE_WIDTH:\n",
    "                print(\"BETWEEN PIPES\")\n",
    "            elif ((self._player_x + PLAYER_WIDTH) - up_pipe[\"x\"]) < (0 + 5):\n",
    "                print(\"IN FRONT OF\")\n",
    "            print(\n",
    "                f\"obs: [{self._statistics['pipe_min_index']},\"\n",
    "                f\"{self._statistics['pipe_min_value']},\"\n",
    "                f\"{self._statistics['pipe_mean_value']}],\"\n",
    "                f\"Ground: {self._statistics['ground_min_value']}\"\n",
    "            )\n",
    "\n",
    "    info = {\"score\": self._score}\n",
    "\n",
    "    return (\n",
    "        obs,\n",
    "        np.float32(reward),\n",
    "        terminal,\n",
    "        (self._score_limit is not None) and (self._score >= self._score_limit),\n",
    "        info,\n",
    "    )\n",
    "\n",
    "\n",
    "FlappyBirdEnv.render = new_render\n",
    "FlappyBirdEnv.step = new_step\n",
    "\n",
    "\n",
    "\n",
    "def smooth_data(data, window_size=100):\n",
    "    \"\"\"Apply a simple moving average smoothing to the data.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def plot_rewards_with_opacity(rewards_dict, title, xlabel, ylabel, img_name):\n",
    "\n",
    "    avg_rewards = {key: np.mean(rewards) for key, rewards in rewards_dict.items()} ##coefficients of variation\n",
    "    top3_keys = sorted(avg_rewards, key=avg_rewards.get, reverse=True)[:3]\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    num_combinations = len(rewards_dict)\n",
    "    cmap = cm.get_cmap(\"cool\", num_combinations)\n",
    "    \n",
    "    # Loop through each hyperparameter combination and plot with a color from the colormap\n",
    "    for idx, (key, rewards) in enumerate(rewards_dict.items()):\n",
    "\n",
    "        smoothed_rewards = smooth_data(rewards, window_size=100)\n",
    "        color = cmap(idx)  # Get the color for this hyperparameter combination\n",
    "        # Adjust opacity for non-top3 combinations\n",
    "        alpha = 1.0 if key in top3_keys else 0.05\n",
    "        ax.plot(range(1, len(smoothed_rewards) + 1), smoothed_rewards, label=key, color=color, alpha=alpha,linewidth=0.7)\n",
    "    \n",
    "    # Set title with bold font\n",
    "    ax.set_title(title, fontweight='bold',fontsize=18)\n",
    "    ax.set_xlabel(xlabel,fontweight='bold')\n",
    "    ax.set_ylabel(ylabel,fontweight='bold')\n",
    "    \n",
    "    \n",
    "    # Place the legend in the bottom-right corner, remove title, and make it smaller\n",
    "    ax.legend(loc='center left', \n",
    "              bbox_to_anchor=(1.05, 0.5),  # Position the legend box outside to the right\n",
    "              borderaxespad=0., \n",
    "              fancybox=True, \n",
    "              fontsize='small', \n",
    "              markerscale=0.5, \n",
    "            )  # Make the title bold\n",
    "    fig.savefig(f'../img/{img_name}.png',dpi=600) \n",
    "    plt.tight_layout()  # Ensures the legend does not get clipped\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "######DDQN#####\n",
    "###############\n",
    "\n",
    "# Replay Memory to store experiences\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Dueling DQN Network Architecture\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        # self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "\n",
    "        # Value Stream\n",
    "        self.value_stream = nn.Linear(128, 1)\n",
    "\n",
    "        # Advantage Stream\n",
    "        self.advantage_stream = nn.Linear(128, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "\n",
    "        # Calculate value and advantage\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        # Calculate Q-values\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))  # Normalize advantage\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# DQN Agent Class with Dueling Architecture\n",
    "class DuelingDQNAgent:\n",
    "    def __init__(self, env, hyper):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = hyper[\"learning_rate\"]\n",
    "        self.discount_factor = hyper[\"discount_factor\"]\n",
    "        self.epsilon = hyper[\"epsilon\"]\n",
    "        self.epsilon_decay = hyper[\"epsilon_decay\"]\n",
    "        self.epsilon_min = hyper[\"epsilon_min\"]\n",
    "        self.batch_size = hyper[\"batch_size\"]\n",
    "        self.memory_size = hyper[\"memory_size\"]\n",
    "        self.episodes = hyper[\"episodes\"]\n",
    "        self.target_update_freq = hyper[\"target_update_freq\"]\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Initialize policy and target networks with dueling architecture\n",
    "        self.policy_net = DuelingDQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net = DuelingDQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "    def select_action(self, state, testing=0):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_space - 1)  # Random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        res = []\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10, render=\"human\"):\n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render, use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=1)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "        return total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "####DDQN++#####\n",
    "###############\n",
    "\n",
    "# Replay Memory to store experiences\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Dueling DQN Network Architecture\n",
    "class DDQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        # self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "\n",
    "        # Value Stream\n",
    "        self.value_stream = nn.Linear(128, 1)\n",
    "\n",
    "        # Advantage Stream\n",
    "        self.advantage_stream = nn.Linear(128, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "\n",
    "        # Calculate value and advantage\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        # Calculate Q-values\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))  # Normalize advantage\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# DQN Agent Class with Dueling Architecture\n",
    "class DDQNppAgent:\n",
    "    def __init__(self, \n",
    "                 env, \n",
    "                hyper ={\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"discount_factor\" : 0.99,\n",
    "                          \"epsilon\" : 1.0,\n",
    "                          \"epsilon_decay\" :0.999,\n",
    "                          \"epsilon_min\" : 0.01,\n",
    "                          \"batch_size\" : 64,\n",
    "                          \"memory_size\" : 10000,\n",
    "                          \"episodes\" : 100000,\n",
    "                          \"target_update_freq\" : 10,\n",
    "                          \"rho\" :1.0,\n",
    "                          \"kappa\" : 1.0,\n",
    "                          \"eps_update_freq\":100\n",
    "                        }):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = hyper[\"learning_rate\"]\n",
    "        self.discount_factor = hyper[\"discount_factor\"]\n",
    "        self.epsilon = hyper[\"epsilon\"]\n",
    "        self.epsilon_decay = hyper[\"epsilon_decay\"]\n",
    "        self.epsilon_min = hyper[\"epsilon_min\"]\n",
    "        self.batch_size = hyper[\"batch_size\"]\n",
    "        self.memory_size = hyper[\"memory_size\"]\n",
    "        self.episodes = hyper[\"episodes\"]\n",
    "        self.target_update_freq = hyper[\"target_update_freq\"]\n",
    "        self.rho = hyper[\"rho\"]\n",
    "        self.kappa = hyper[\"kappa\"]\n",
    "        self.eps_update_freq=hyper[\"eps_update_freq\"]\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Initialize policy and target networks with dueling architecture\n",
    "        self.policy_net = DDQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net = DDQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, state, testing=0):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return self.oracle(random.randint(0, self.action_space - 1),(testing+1)%2 ) # Random action\n",
    "\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                return self.oracle(self.policy_net(state).argmax(dim=1).item(),(testing+1)%2 )\n",
    "    \n",
    "    def oracle(self, action,option):\n",
    "        next_state, reward, done, _, _ = self.env.step(action)\n",
    "        if done :\n",
    "            action = (action+option)%2\n",
    "\n",
    "        return action\n",
    "        \n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        res = []\n",
    "        res=[]\n",
    "        reward_trace=np.float32(0.0)\n",
    "        old_reward_trace=np.float32(0.0)\n",
    "        counter=0\n",
    "  \n",
    "\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = np.float32(0.0)\n",
    "\n",
    "\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            if abs(self.epsilon-self.epsilon_min) < 1e-4 :\n",
    "\n",
    "                reward_trace+=total_reward\n",
    "                counter+=1\n",
    "                if counter % self.eps_update_freq== 0:\n",
    "                    if (reward_trace-old_reward_trace)/self.eps_update_freq<self.kappa:\n",
    "                        self.epsilon = self.rho\n",
    "                        reward_trace=np.float32(0.0)\n",
    "                        old_reward_trace=np.float32(0.0)\n",
    "                    \n",
    "                    counter=0\n",
    "                    old_reward_trace=reward_trace\n",
    "                    reward_trace=np.float32(0.0)\n",
    "\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10, render=\"human\"):\n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render, use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=1)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "        return total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "######DDQN+####\n",
    "###############\n",
    "\n",
    "# Replay Memory to store experiences\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Dueling DQN Network Architecture\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        # self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "\n",
    "        # Value Stream\n",
    "        self.value_stream = nn.Linear(128, 1)\n",
    "\n",
    "        # Advantage Stream\n",
    "        self.advantage_stream = nn.Linear(128, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "\n",
    "        # Calculate value and advantage\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        # Calculate Q-values\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))  # Normalize advantage\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# DQN Agent Class with Dueling Architecture\n",
    "class DDQNPAgent:\n",
    "    def __init__(self, env, hyper):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = hyper[\"learning_rate\"]\n",
    "        self.discount_factor = hyper[\"discount_factor\"]\n",
    "        self.epsilon = hyper[\"epsilon\"]\n",
    "        self.epsilon_decay = hyper[\"epsilon_decay\"]\n",
    "        self.epsilon_min = hyper[\"epsilon_min\"]\n",
    "        self.batch_size = hyper[\"batch_size\"]\n",
    "        self.memory_size = hyper[\"memory_size\"]\n",
    "        self.episodes = hyper[\"episodes\"]\n",
    "        self.target_update_freq = hyper[\"target_update_freq\"]\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Initialize policy and target networks with dueling architecture\n",
    "        self.policy_net = DuelingDQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net = DuelingDQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "    def select_action(self, state, testing=0):\n",
    "   \n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return self.oracle(random.randint(0, self.action_space - 1),(testing+1)%2  ) # Random action\n",
    "\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                return self.oracle(self.policy_net(state).argmax(dim=1).item(),(testing+1)%2 )\n",
    "    \n",
    "    def oracle(self, action,option):\n",
    "        next_state, reward, done, _, _ = self.env.step(action)\n",
    "        if done :\n",
    "            action = (action+option)%2\n",
    "\n",
    "        return action\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        res = []\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10, render=\"human\"):\n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render, use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=1)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "        return total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "######EAC#######\n",
    "################\n",
    "\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, action_space)\n",
    "        )\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        return F.softmax(self.actor(shared), dim=-1), self.critic(shared)\n",
    "\n",
    "class EAC_agent:\n",
    "    def __init__(self, env, config):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        # Parameters\n",
    "        self.actor_lr = config.get('actor_lr', 0.001)\n",
    "        self.critic_lr = config.get('critic_lr', 0.005)\n",
    "        self.gamma = config.get('gamma', 0.99)\n",
    "        self.gae_lambda = config.get('gae_lambda', 0.95)\n",
    "        self.entropy_coef = config.get('entropy_coef', 0.01)\n",
    "        self.value_loss_coef = config.get('value_loss_coef', 0.5)\n",
    "        self.max_grad_norm = config.get('max_grad_norm', 0.5)\n",
    "        self.episodes = config.get('episodes', 5000)\n",
    "        \n",
    "        self.network = ActorCriticNet(self.state_dim, self.action_space).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.network.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.network.parameters(), lr=self.critic_lr)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.masks = []\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs, value = self.network(state)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "    def compute_gae(self):\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            if step == len(self.rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = self.values[step + 1]\n",
    "                \n",
    "            delta = self.rewards[step] + self.gamma * next_value * self.masks[step] - self.values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * self.masks[step] * gae\n",
    "            returns.insert(0, gae + self.values[step])\n",
    "            \n",
    "        return torch.FloatTensor(returns).to(self.device)\n",
    "\n",
    "    def train_step(self):\n",
    "        returns = self.compute_gae()\n",
    "        \n",
    "        values = torch.FloatTensor(self.values).to(self.device)\n",
    "        log_probs = torch.FloatTensor(self.log_probs).to(self.device)\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        states = torch.FloatTensor(self.states).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        new_probs, new_values = self.network(states)\n",
    "        dist = Categorical(new_probs)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Actor loss\n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        actor_loss = -(surr1.mean() + self.entropy_coef * entropy)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = self.value_loss_coef * F.mse_loss(new_values.squeeze(-1), returns)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Clear memory\n",
    "        self.log_probs.clear()\n",
    "        self.values.clear()\n",
    "        self.rewards.clear()\n",
    "        self.masks.clear()\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "\n",
    "    def train(self):\n",
    "        rewards_history = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            self.states = []\n",
    "            self.actions = []\n",
    "            \n",
    "            while not done:\n",
    "                action, log_prob, value = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                \n",
    "                self.states.append(state)\n",
    "                self.actions.append(action)\n",
    "                self.rewards.append(reward)\n",
    "                self.log_probs.append(log_prob)\n",
    "                self.values.append(value)\n",
    "                self.masks.append(1 - done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            self.train_step()\n",
    "            rewards_history.append(total_reward)\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(rewards_history[-100:])\n",
    "                print(f\"Episode {episode}, Reward: {total_reward:.2f}, Avg: {avg_reward:.2f}\")\n",
    "        \n",
    "        return rewards_history\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render,use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    probs, _ = self.network(state)\n",
    "                action = probs.argmax().item()\n",
    "                state, reward, done, truncated, _ = test_env.step(action)\n",
    "                total_reward += reward\n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode {episode}, Total Reward: {total_reward}\")\n",
    "        \n",
    "        print(f\"Average Test Reward: {np.mean(total_rewards):.2f}\")\n",
    "        return total_rewards\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "######EAC+######\n",
    "################\n",
    "\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, action_space)\n",
    "        )\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        return F.softmax(self.actor(shared), dim=-1), self.critic(shared)\n",
    "\n",
    "class EAC_plus_agent:\n",
    "    def __init__(self, env, config):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        # Parameters\n",
    "        self.actor_lr = config.get('actor_lr', 0.001)\n",
    "        self.critic_lr = config.get('critic_lr', 0.005)\n",
    "        self.gamma = config.get('gamma', 0.99)\n",
    "        self.gae_lambda = config.get('gae_lambda', 0.95)\n",
    "        self.entropy_coef = config.get('entropy_coef', 0.01)\n",
    "        self.value_loss_coef = config.get('value_loss_coef', 0.5)\n",
    "        self.max_grad_norm = config.get('max_grad_norm', 0.5)\n",
    "        self.episodes = config.get('episodes', 5000)\n",
    "        \n",
    "        self.network = ActorCriticNet(self.state_dim, self.action_space).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.network.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.network.parameters(), lr=self.critic_lr)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.masks = []\n",
    "\n",
    "    def oracle(self, action,option):\n",
    "        next_state, reward, done, _, _ = self.env.step(action)\n",
    "        if done :\n",
    "            action = (action+option)%2\n",
    "\n",
    "        return action\n",
    "\n",
    "    def select_action(self, state,testing=0):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs, value = self.network(state)\n",
    "            dist = Categorical(probs)\n",
    "            action =self.oracle(dist.sample(),(testing+1)%2 )\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "    def compute_gae(self):\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            if step == len(self.rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = self.values[step + 1]\n",
    "                \n",
    "            delta = self.rewards[step] + self.gamma * next_value * self.masks[step] - self.values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * self.masks[step] * gae\n",
    "            returns.insert(0, gae + self.values[step])\n",
    "            \n",
    "        return torch.FloatTensor(returns).to(self.device)\n",
    "\n",
    "    def train_step(self):\n",
    "        returns = self.compute_gae()\n",
    "        \n",
    "        values = torch.FloatTensor(self.values).to(self.device)\n",
    "        log_probs = torch.FloatTensor(self.log_probs).to(self.device)\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        states = torch.FloatTensor(self.states).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        new_probs, new_values = self.network(states)\n",
    "        dist = Categorical(new_probs)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Actor loss\n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        actor_loss = -(surr1.mean() + self.entropy_coef * entropy)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = self.value_loss_coef * F.mse_loss(new_values.squeeze(-1), returns)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Clear memory\n",
    "        self.log_probs.clear()\n",
    "        self.values.clear()\n",
    "        self.rewards.clear()\n",
    "        self.masks.clear()\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "\n",
    "    def train(self):\n",
    "        rewards_history = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            self.states = []\n",
    "            self.actions = []\n",
    "            \n",
    "            while not done:\n",
    "                action, log_prob, value = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                \n",
    "                self.states.append(state)\n",
    "                self.actions.append(action)\n",
    "                self.rewards.append(reward)\n",
    "                self.log_probs.append(log_prob)\n",
    "                self.values.append(value)\n",
    "                self.masks.append(1 - done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            self.train_step()\n",
    "            rewards_history.append(total_reward)\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(rewards_history[-100:])\n",
    "                print(f\"Episode {episode}, Reward: {total_reward:.2f}, Avg: {avg_reward:.2f}\")\n",
    "        \n",
    "        return rewards_history\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render,use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    probs, _ = self.network(state)\n",
    "                action = probs.argmax().item()\n",
    "                state, reward, done, truncated, _ = test_env.step(action)\n",
    "                total_reward += reward\n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode {episode}, Total Reward: {total_reward}\")\n",
    "        \n",
    "        print(f\"Average Test Reward: {np.mean(total_rewards):.2f}\")\n",
    "        return total_rewards\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "\n",
    "\n",
    "hyper_eac = {\n",
    "   \"actor_lr\": 0.0005,\n",
    "   \"critic_lr\": 0.001, \n",
    "   \"gamma\": 0.99,\n",
    "   \"gae_lambda\": 0.9,\n",
    "   \"entropy_coef\": 0.05,\n",
    "   \"value_loss_coef\": 1.0,\n",
    "   \"max_grad_norm\": 0.5,\n",
    "   \"episodes\": 1\n",
    "}\n",
    "\n",
    "hyper_eacp = {\n",
    "   \"actor_lr\": 0.0005,\n",
    "   \"critic_lr\": 0.001, \n",
    "   \"gamma\": 0.99,\n",
    "   \"gae_lambda\": 0.9,\n",
    "   \"entropy_coef\": 0.05,\n",
    "   \"value_loss_coef\": 1.0,\n",
    "   \"max_grad_norm\": 0.5,\n",
    "   \"episodes\": 1\n",
    "}\n",
    "\n",
    "hyper_ddqn = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"discount_factor\": 0.999,\n",
    "    \"epsilon\": 1.0,\n",
    "    \"epsilon_decay\": 0.999,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"memory_size\": 10000,\n",
    "    \"episodes\": 1,\n",
    "    \"target_update_freq\": 10\n",
    "}\n",
    "\n",
    "hyper_ddqnp = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"discount_factor\": 0.999,\n",
    "    \"epsilon\": 1.0,\n",
    "    \"epsilon_decay\": 0.999,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"memory_size\": 10000,\n",
    "    \"episodes\": 1,\n",
    "    \"target_update_freq\": 10\n",
    "}\n",
    "\n",
    "hyper_ddqnpp = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"epsilon\": 1.0,\n",
    "    \"epsilon_decay\": 0.999,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"memory_size\": 10000,\n",
    "    \"episodes\": 1,\n",
    "    \"target_update_freq\": 10,\n",
    "    \"rho\":0.2,\n",
    "    \"kappa\":0.0,\n",
    "    \"eps_update_freq\":100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvdg/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/cvdg/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -7.50, Avg: -7.50\n",
      "Test Episode 0, Total Reward: 0.799999475479126\n",
      "Test Episode 1, Total Reward: 0.799999475479126\n",
      "Test Episode 2, Total Reward: 0.799999475479126\n",
      "Test Episode 3, Total Reward: 0.799999475479126\n",
      "Test Episode 4, Total Reward: 0.799999475479126\n",
      "Average Test Reward: 0.80\n",
      "Episode 0, Reward: -4.00, Avg: -4.00\n",
      "Test Episode 0, Total Reward: -9.300000190734863\n",
      "Test Episode 1, Total Reward: -9.300000190734863\n",
      "Test Episode 2, Total Reward: -9.300000190734863\n",
      "Test Episode 3, Total Reward: -9.300000190734863\n",
      "Test Episode 4, Total Reward: -9.300000190734863\n",
      "Average Test Reward: -9.30\n",
      "Episode: 1, Total Reward: -8.100000381469727, Epsilon: 0.9990\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: 0.799999475479126\n",
      "Test Episode: 2, Total Reward: 0.799999475479126\n",
      "Test Episode: 3, Total Reward: 0.799999475479126\n",
      "Test Episode: 4, Total Reward: 0.799999475479126\n",
      "Test Episode: 5, Total Reward: 0.799999475479126\n",
      "\n",
      "Average Reward over 5 Test Episodes: 0.799999475479126\n",
      "Episode: 1, Total Reward: -3.3999996185302734, Epsilon: 0.9990\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -9.300000190734863\n",
      "Test Episode: 3, Total Reward: -9.300000190734863\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -9.300000190734863\n",
      "Episode: 1, Total Reward: -3.999999761581421, Epsilon: 0.9990\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -9.300000190734863\n",
      "Test Episode: 3, Total Reward: -9.300000190734863\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -9.300000190734863\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "exp_res = {}\n",
    "test_res = {}\n",
    "\n",
    "eac_agent = EAC_agent(env, hyper_eac)\n",
    "ddqn_agent = DuelingDQNAgent(env, hyper_ddqn)\n",
    "ddqnpp_agent = DDQNppAgent(env,hyper_ddqnpp)\n",
    "eacp_agent=EAC_plus_agent(env,hyper_eacp)\n",
    "ddqnp_agent = DDQNPAgent(env,hyper_ddqnp)\n",
    "\n",
    "agents={\"EAC\": eac_agent,\"EAC+\":eacp_agent,\"DDQN\" : ddqn_agent,\"DDQN+\":ddqnp_agent ,\"DDQN++\":ddqnpp_agent}\n",
    "\n",
    "for i in [\"EAC\",\"EAC+\",\"DDQN\",\"DDQN+\",\"DDQN++\"]:\n",
    "    exp_key=i\n",
    "\n",
    "    agent = agents[i]\n",
    "    exp_res[exp_key] = agent.train()\n",
    "    test_res[exp_key] = agent.test(num_episodes=5, render=None)\n",
    "    agents[i] =agent\n",
    "    with open(\"../exp_data/comparison2_tr.pkl\", \"wb\") as f:\n",
    "        pickle.dump(exp_res, f)\n",
    "    with open(\"../exp_data/comparison2_test.pkl\", \"wb\") as f:\n",
    "        pickle.dump(test_res, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_408189/3418667986.py:253: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = cm.get_cmap(\"cool\", num_combinations)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"../exp_data/comparison2_tr.pkl\", \"rb\") as f:\n",
    "   exp_res = pickle.load(f)\n",
    "with open(\"../exp_data/comparison2_test.pkl\", \"rb\") as f:\n",
    "   test_res = pickle.load(f)\n",
    "plot_rewards_with_opacity(exp_res, \"Training Rewards (Top3, Avg.)\", \"Episode\", \"Reward\",\"eac_p_tr.png\")\n",
    "\n",
    "# Plot for test_res (test rewards)\n",
    "plot_rewards_with_opacity(test_res, \"Test Rewards (Top3, Avg.)\", \"Episode\", \"Reward\",\"eac_p_test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
