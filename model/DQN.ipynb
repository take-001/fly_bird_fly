{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import pygame\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Grayscale\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs import utils\n",
    "from flappy_bird_gymnasium.envs.lidar import LIDAR\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    BACKGROUND_WIDTH,\n",
    "    BASE_WIDTH,\n",
    "    FILL_BACKGROUND_COLOR,\n",
    "    LIDAR_MAX_DISTANCE,\n",
    "    PIPE_HEIGHT,\n",
    "    PIPE_VEL_X,\n",
    "    PIPE_WIDTH,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_PRIVATE_ZONE,\n",
    "    PLAYER_ROT_THR,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    ")\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "\n",
    "def new_render(self):\n",
    "    \"\"\"Renders the next frame.\"\"\"\n",
    "    if self.render_mode == \"rgb_array\":\n",
    "        self._draw_surface(show_score=False, show_rays=False)\n",
    "        # Flip the image to retrieve a correct aspect\n",
    "        return np.transpose(pygame.surfarray.array3d(self._surface), axes=(1, 0, 2))\n",
    "    else:\n",
    "        self._draw_surface(show_score=True, show_rays=False)\n",
    "        if self._display is None:\n",
    "            self._make_display()\n",
    "\n",
    "        self._update_display()\n",
    "        self._fps_clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "def new_step(\n",
    "    self,\n",
    "    action: Union[Actions, int],\n",
    ") -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "    \"\"\"Given an action, updates the game state.\n",
    "\n",
    "    Args:\n",
    "        action (Union[FlappyBirdLogic.Actions, int]): The action taken by\n",
    "            the agent. Zero (0) means \"do nothing\" and one (1) means \"flap\".\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing, respectively:\n",
    "\n",
    "            * an observation (horizontal distance to the next pipe\n",
    "                difference between the player's y position and the next hole's\n",
    "                y position)\n",
    "            * a reward (alive = +0.1, pipe = +1.0, dead = -1.0)\n",
    "            * a status report (`True` if the game is over and `False`\n",
    "                otherwise)\n",
    "            * an info dictionary\n",
    "    \"\"\"\n",
    "    \"\"\"Given an action taken by the player, updates the game's state.\n",
    "\n",
    "    Args:\n",
    "        action (Union[FlappyBirdLogic.Actions, int]): The action taken by\n",
    "            the player.\n",
    "\n",
    "    Returns:\n",
    "        `True` if the player is alive and `False` otherwise.\n",
    "    \"\"\"\n",
    "    terminal = False\n",
    "    reward = None\n",
    "\n",
    "    self._sound_cache = None\n",
    "    if action == Actions.FLAP:\n",
    "        if self._player_y > -2 * PLAYER_HEIGHT:\n",
    "            self._player_vel_y = PLAYER_FLAP_ACC\n",
    "            self._player_flapped = True\n",
    "            self._sound_cache = \"wing\"\n",
    "\n",
    "    # check for score\n",
    "    player_mid_pos = self._player_x + PLAYER_WIDTH / 2\n",
    "    for pipe in self._upper_pipes:\n",
    "        pipe_mid_pos = pipe[\"x\"] + PIPE_WIDTH / 2\n",
    "        if pipe_mid_pos <= player_mid_pos < pipe_mid_pos + 4:\n",
    "            self._score += 1\n",
    "            reward = 1.0  # reward for passed pipe\n",
    "            self._sound_cache = \"point\"\n",
    "\n",
    "    # player_index base_x change\n",
    "    if (self._loop_iter + 1) % 3 == 0:\n",
    "        self._player_idx = next(self._player_idx_gen)\n",
    "\n",
    "    self._loop_iter = (self._loop_iter + 1) % 30\n",
    "    self._ground[\"x\"] = -((-self._ground[\"x\"] + 100) % self._base_shift)\n",
    "\n",
    "    # rotate the player\n",
    "    if self._player_rot > -90:\n",
    "        self._player_rot -= PLAYER_VEL_ROT\n",
    "\n",
    "    # player's movement\n",
    "    if self._player_vel_y < PLAYER_MAX_VEL_Y and not self._player_flapped:\n",
    "        self._player_vel_y += PLAYER_ACC_Y\n",
    "\n",
    "    if self._player_flapped:\n",
    "        self._player_flapped = False\n",
    "\n",
    "        # more rotation to cover the threshold\n",
    "        # (calculated in visible rotation)\n",
    "        self._player_rot = 45\n",
    "\n",
    "    self._player_y += min(\n",
    "        self._player_vel_y, self._ground[\"y\"] - self._player_y - PLAYER_HEIGHT\n",
    "    )\n",
    "\n",
    "    # move pipes to left\n",
    "    for up_pipe, low_pipe in zip(self._upper_pipes, self._lower_pipes):\n",
    "        up_pipe[\"x\"] += PIPE_VEL_X\n",
    "        low_pipe[\"x\"] += PIPE_VEL_X\n",
    "\n",
    "        # it is out of the screen\n",
    "        if up_pipe[\"x\"] < -PIPE_WIDTH:\n",
    "            new_up_pipe, new_low_pipe = self._get_random_pipe()\n",
    "            up_pipe[\"x\"] = new_up_pipe[\"x\"]\n",
    "            up_pipe[\"y\"] = new_up_pipe[\"y\"]\n",
    "            low_pipe[\"x\"] = new_low_pipe[\"x\"]\n",
    "            low_pipe[\"y\"] = new_low_pipe[\"y\"]\n",
    "\n",
    "    if self.render_mode == \"human\":\n",
    "        self.render()\n",
    "\n",
    "    obs, reward_private_zone = self._get_observation()\n",
    "    if reward is None:\n",
    "        if reward_private_zone is not None:\n",
    "            reward = float(reward_private_zone)\n",
    "        else:\n",
    "            reward = 0.1  # reward for staying alive\n",
    "\n",
    "    # check\n",
    "    if self._debug and self._use_lidar:\n",
    "        # sort pipes by the distance between pipe and agent\n",
    "        up_pipe = sorted(\n",
    "            self._upper_pipes,\n",
    "            key=lambda x: np.sqrt(\n",
    "                (self._player_x - x[\"x\"]) ** 2\n",
    "                + (self._player_y - (x[\"y\"] + PIPE_HEIGHT)) ** 2\n",
    "            ),\n",
    "        )[0]\n",
    "        # find ray closest to the obstacle\n",
    "        min_index = np.argmin(obs)\n",
    "        min_value = obs[min_index] * LIDAR_MAX_DISTANCE\n",
    "        # mean approach to the obstacle\n",
    "        if \"pipe_mean_value\" in self._statistics:\n",
    "            self._statistics[\"pipe_mean_value\"] = self._statistics[\n",
    "                \"pipe_mean_value\"\n",
    "            ] * 0.99 + min_value * (1 - 0.99)\n",
    "        else:\n",
    "            self._statistics[\"pipe_mean_value\"] = min_value\n",
    "\n",
    "        # Nearest to the pipe\n",
    "        if \"pipe_min_value\" in self._statistics:\n",
    "            if min_value < self._statistics[\"pipe_min_value\"]:\n",
    "                self._statistics[\"pipe_min_value\"] = min_value\n",
    "                self._statistics[\"pipe_min_index\"] = min_index\n",
    "        else:\n",
    "            self._statistics[\"pipe_min_value\"] = min_value\n",
    "            self._statistics[\"pipe_min_index\"] = min_index\n",
    "\n",
    "        # Nearest to the ground\n",
    "        diff = np.abs(self._player_y - self._ground[\"y\"])\n",
    "        if \"ground_min_value\" in self._statistics:\n",
    "            if diff < self._statistics[\"ground_min_value\"]:\n",
    "                self._statistics[\"ground_min_value\"] = diff\n",
    "        else:\n",
    "            self._statistics[\"ground_min_value\"] = diff\n",
    "\n",
    "    # agent touch the top of the screen as punishment\n",
    "    if self._player_y < 0:\n",
    "        reward = -0.5\n",
    "\n",
    "    # check for crash\n",
    "    if self._check_crash():\n",
    "        self._sound_cache = \"hit\"\n",
    "        reward = -1.0  # reward for dying\n",
    "        terminal = True\n",
    "        self._player_vel_y = 0\n",
    "        if self._debug and self._use_lidar:\n",
    "            if ((self._player_x + PLAYER_WIDTH) - up_pipe[\"x\"]) > (0 + 5) and (\n",
    "                self._player_x - up_pipe[\"x\"]\n",
    "            ) < PIPE_WIDTH:\n",
    "                print(\"BETWEEN PIPES\")\n",
    "            elif ((self._player_x + PLAYER_WIDTH) - up_pipe[\"x\"]) < (0 + 5):\n",
    "                print(\"IN FRONT OF\")\n",
    "            print(\n",
    "                f\"obs: [{self._statistics['pipe_min_index']},\"\n",
    "                f\"{self._statistics['pipe_min_value']},\"\n",
    "                f\"{self._statistics['pipe_mean_value']}],\"\n",
    "                f\"Ground: {self._statistics['ground_min_value']}\"\n",
    "            )\n",
    "\n",
    "    info = {\"score\": self._score}\n",
    "\n",
    "    return (\n",
    "        obs,\n",
    "        np.float32(reward),\n",
    "        terminal,\n",
    "        (self._score_limit is not None) and (self._score >= self._score_limit),\n",
    "        info,\n",
    "    )\n",
    "\n",
    "\n",
    "FlappyBirdEnv.render = new_render\n",
    "FlappyBirdEnv.step = new_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "######DQN######\n",
    "###############\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)   \n",
    "        self.fc3 = nn.Linear(128, 128)      # Second hidden layer\n",
    "        self.fc4 = nn.Linear(128, action_space)  # Output layer for Q-values\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "\n",
    "# Replay Memory to store experiences\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# DQN Agent Class\n",
    "class DQNAgent:\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 hyper ={\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"discount_factor\" : 0.99,\n",
    "                          \"epsilon\" : 1.0,\n",
    "                          \"epsilon_decay\" :0.999,\n",
    "                          \"epsilon_min\" : 0.01,\n",
    "                          \"batch_size\" : 64,\n",
    "                          \"memory_size\" : 10000,\n",
    "                          \"episodes\" : 100000,\n",
    "                          \"target_update_freq\" : 10\n",
    "                        }\n",
    "                 ):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]  # First dimension of observation\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = hyper[\"learning_rate\"]\n",
    "        self.discount_factor = hyper[\"discount_factor\"]\n",
    "        self.epsilon = hyper[\"epsilon\"]\n",
    "        self.epsilon_decay = hyper[\"epsilon_decay\"]\n",
    "        self.epsilon_min = hyper[\"epsilon_min\"]\n",
    "        self.batch_size = hyper[\"batch_size\"]\n",
    "        self.memory_size = hyper[\"memory_size\"]\n",
    "        self.episodes = hyper[\"episodes\"]\n",
    "        self.target_update_freq = hyper[\"target_update_freq\"]\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        print(self.device)\n",
    "\n",
    "\n",
    "        # Initialize policy and target networks\n",
    "        self.policy_net = DQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net = DQN(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "    def select_action(self, state, testing=False):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_space - 1)  # Random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        res=[]\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render,use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=True)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Episode: 1, Total Reward: -8.100000381469727, Epsilon: 0.9950\n",
      "Episode: 2, Total Reward: -8.100000381469727, Epsilon: 0.9900\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -9.300000190734863\n",
      "Test Episode: 3, Total Reward: -9.300000190734863\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -9.300000190734863\n",
      "Finished training and testing for: lr=0.001_decay=0.995_freq=10\n",
      "cuda\n",
      "Episode: 1, Total Reward: -8.100000381469727, Epsilon: 0.9950\n",
      "Episode: 2, Total Reward: -8.100000381469727, Epsilon: 0.9900\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -0.9000015258789062\n",
      "Test Episode: 2, Total Reward: -0.9000015258789062\n",
      "Test Episode: 3, Total Reward: -0.9000015258789062\n",
      "Test Episode: 4, Total Reward: -0.9000015258789062\n",
      "Test Episode: 5, Total Reward: -1.5000016689300537\n",
      "\n",
      "Average Reward over 5 Test Episodes: -1.0200016498565674\n",
      "Finished training and testing for: lr=0.001_decay=0.995_freq=20\n",
      "cuda\n",
      "Episode: 1, Total Reward: -6.30000114440918, Epsilon: 0.9990\n",
      "Episode: 2, Total Reward: -7.500000953674316, Epsilon: 0.9980\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -9.300000190734863\n",
      "Test Episode: 3, Total Reward: -9.300000190734863\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -9.300000190734863\n",
      "Finished training and testing for: lr=0.001_decay=0.999_freq=10\n",
      "cuda\n",
      "Episode: 1, Total Reward: -6.90000057220459, Epsilon: 0.9990\n",
      "Episode: 2, Total Reward: -8.100000381469727, Epsilon: 0.9980\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -6.30000114440918\n",
      "Test Episode: 3, Total Reward: -6.30000114440918\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -8.100000381469727\n",
      "Finished training and testing for: lr=0.001_decay=0.999_freq=20\n",
      "cuda\n",
      "Episode: 1, Total Reward: -2.100001573562622, Epsilon: 0.9950\n",
      "Episode: 2, Total Reward: -7.500000953674316, Epsilon: 0.9900\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -9.300000190734863\n",
      "Test Episode: 3, Total Reward: -9.300000190734863\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -9.300000190734863\n",
      "Finished training and testing for: lr=0.005_decay=0.995_freq=10\n",
      "cuda\n",
      "Episode: 1, Total Reward: -6.30000114440918, Epsilon: 0.9950\n",
      "Episode: 2, Total Reward: -7.500000953674316, Epsilon: 0.9900\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -9.300000190734863\n",
      "Test Episode: 3, Total Reward: -9.300000190734863\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -9.300000190734863\n",
      "Finished training and testing for: lr=0.005_decay=0.995_freq=20\n",
      "cuda\n",
      "Episode: 1, Total Reward: -6.90000057220459, Epsilon: 0.9990\n",
      "Episode: 2, Total Reward: -4.500000953674316, Epsilon: 0.9980\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: -9.300000190734863\n",
      "Test Episode: 2, Total Reward: -9.300000190734863\n",
      "Test Episode: 3, Total Reward: -9.300000190734863\n",
      "Test Episode: 4, Total Reward: -9.300000190734863\n",
      "Test Episode: 5, Total Reward: -9.300000190734863\n",
      "\n",
      "Average Reward over 5 Test Episodes: -9.300000190734863\n",
      "Finished training and testing for: lr=0.005_decay=0.999_freq=10\n",
      "cuda\n",
      "Episode: 1, Total Reward: -8.100000381469727, Epsilon: 0.9990\n",
      "Episode: 2, Total Reward: -6.30000114440918, Epsilon: 0.9980\n",
      "Training complete!\n",
      "\n",
      "Testing the trained policy...\n",
      "\n",
      "Test Episode: 1, Total Reward: 0.799999475479126\n",
      "Test Episode: 2, Total Reward: 0.799999475479126\n",
      "Test Episode: 3, Total Reward: 0.799999475479126\n",
      "Test Episode: 4, Total Reward: 0.799999475479126\n",
      "Test Episode: 5, Total Reward: 0.799999475479126\n",
      "\n",
      "Average Reward over 5 Test Episodes: 0.799999475479126\n",
      "Finished training and testing for: lr=0.005_decay=0.999_freq=20\n",
      "All parameter combinations processed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "\n",
    "# Hyperparameters\n",
    "hyper = {\n",
    "    \"learning_rate\": [0.001, 0.005],\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"epsilon\": 1.0,\n",
    "    \"epsilon_decay\": [0.995, 0.999],\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"memory_size\": 10000,\n",
    "    \"episodes\": 20000,\n",
    "    \"target_update_freq\": [10, 20]\n",
    "}\n",
    "\n",
    "param_combinations = itertools.product(\n",
    "    hyper[\"learning_rate\"], \n",
    "    hyper[\"epsilon_decay\"], \n",
    "    hyper[\"target_update_freq\"]\n",
    ")\n",
    "\n",
    "# Initialize result dictionaries\n",
    "exp_res = {}\n",
    "test_res = {}\n",
    "\n",
    "# Iterate through parameter combinations\n",
    "for lr, epsilon_decay, target_update_freq in param_combinations:\n",
    "    current_hyperparams = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"discount_factor\": hyper[\"discount_factor\"],\n",
    "        \"epsilon\": hyper[\"epsilon\"],\n",
    "        \"epsilon_decay\": epsilon_decay,\n",
    "        \"epsilon_min\": hyper[\"epsilon_min\"],\n",
    "        \"batch_size\": hyper[\"batch_size\"],\n",
    "        \"memory_size\": hyper[\"memory_size\"],\n",
    "        \"episodes\": hyper[\"episodes\"],\n",
    "        \"target_update_freq\": target_update_freq\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Train the agent\n",
    "        agent = DQNAgent(env, current_hyperparams)\n",
    "        exp_key = f\"lr={lr}_decay={epsilon_decay}_freq={target_update_freq}\"\n",
    "        exp_res[exp_key] = agent.train()\n",
    "        \n",
    "        # Test the agent\n",
    "        test_res[exp_key] = agent.test(num_episodes=5000, render=None)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        with open(\"../exp_data/DQN_tr.pkl\", \"wb\") as f:\n",
    "            pickle.dump(exp_res, f)\n",
    "        with open(\"../exp_data/DQN_test.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_res, f)\n",
    "        \n",
    "        print(f\"Finished training and testing for: {exp_key}\")\n",
    "    \n",
    "    finally:\n",
    "        # Free resources\n",
    "        del agent\n",
    "        gc.collect()\n",
    "        env.close()\n",
    "\n",
    "\n",
    "with open(\"../exp_data/DQN_tr.pkl\", \"rb\") as f:\n",
    "    exp_res = pickle.load(f)\n",
    "with open(\"../exp_data/DQN_test.pkl\", \"rb\") as f:\n",
    "    test_res = pickle.load(f)\n",
    "\n",
    "print(\"All parameter combinations processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
