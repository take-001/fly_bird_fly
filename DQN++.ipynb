{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import pygame\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Grayscale\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs.lidar import LIDAR\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    "    PIPE_WIDTH,\n",
    "    PIPE_VEL_X,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "\n",
    "def new_render(self):\n",
    "    \"\"\"Renders the next frame.\"\"\"\n",
    "    if self.render_mode == \"rgb_array\":\n",
    "        self._draw_surface(show_score=False, show_rays=False)\n",
    "        # Flip the image to retrieve a correct aspect\n",
    "        return np.transpose(pygame.surfarray.array3d(self._surface), axes=(1, 0, 2))\n",
    "    else:\n",
    "        self._draw_surface(show_score=True, show_rays=False)\n",
    "        if self._display is None:\n",
    "            self._make_display()\n",
    "\n",
    "        self._update_display()\n",
    "        self._fps_clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "\n",
    "FlappyBirdEnv.render = new_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "######DQN++######\n",
    "################\n",
    "\n",
    "class DQN_pp(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DQN_pp, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)   \n",
    "        self.fc3 = nn.Linear(128, 128)      # Second hidden layer\n",
    "        self.fc4 = nn.Linear(128, action_space)  # Output layer for Q-values\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN_pp_Agent:\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 hyper ={\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"discount_factor\" : 0.99,\n",
    "                          \"epsilon\" : 1.0,\n",
    "                          \"epsilon_decay\" :0.999,\n",
    "                          \"epsilon_min\" : 0.01,\n",
    "                          \"batch_size\" : 64,\n",
    "                          \"memory_size\" : 10000,\n",
    "                          \"episodes\" : 100000,\n",
    "                          \"target_update_freq\" : 10,\n",
    "                          \"rho\" :1.0,\n",
    "                          \"kappa\" : 1.0,\n",
    "                          \"eps_update_freq\":100\n",
    "                        }\n",
    "                 ):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Device setup for GPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = hyper[\"learning_rate\"]\n",
    "        self.discount_factor = hyper[\"discount_factor\"]\n",
    "        self.epsilon = hyper[\"epsilon\"]\n",
    "        self.epsilon_decay = hyper[\"epsilon_decay\"]\n",
    "        self.epsilon_min = hyper[\"epsilon_min\"]\n",
    "        self.batch_size = hyper[\"batch_size\"]\n",
    "        self.memory_size = hyper[\"memory_size\"]\n",
    "        self.episodes = hyper[\"episodes\"]\n",
    "        self.target_update_freq = hyper[\"target_update_freq\"]\n",
    "        self.rho = hyper[\"rho\"]\n",
    "        self.kappa = hyper[\"kappa\"]\n",
    "        self.eps_update_freq=hyper[\"eps_update_freq\"]\n",
    "\n",
    "        # Initialize policy and target networks\n",
    "        self.policy_net = DQN_plus(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net = DQN_plus(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "    def select_action(self, state, testing=False):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return self.oracle(random.randint(0, self.action_space - 1) ) # Random action\n",
    "\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                return self.oracle(self.policy_net(state).argmax(dim=1).item())\n",
    "    \n",
    "    def oracle(self, action):\n",
    "        next_state, reward, done, _, _ = self.env.step(action)\n",
    "        if done :\n",
    "            action = (action+1)%2\n",
    "\n",
    "        return action\n",
    "        \n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        res=[]\n",
    "        reward_trace=0\n",
    "        old_reward_trace=0\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            \n",
    "            # Update target network periodically\n",
    "            \n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            if abs(self.epsilon-self.epsilon_min) < 1e-4 :\n",
    "                reward_trace+=total_reward\n",
    "                if (reward_trace-old_reward_trace)/self.eps_update_freq <self.kappa and  episode % self.eps_update_freq== 0:\n",
    "                    self.epsilon = self.rho\n",
    "                    \n",
    "                    old_reward_trace=reward_trace\n",
    "                    reward_trace=0\n",
    "\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        \n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render,use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=True)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "        \n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        \n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Episode: 1, Total Reward: -1.5999999999999994, Epsilon: 0.7000\n",
      "Episode: 2, Total Reward: -4.6, Epsilon: 0.4900\n",
      "Episode: 3, Total Reward: -5.2, Epsilon: 0.3430\n",
      "Episode: 4, Total Reward: -3.3999999999999995, Epsilon: 0.2401\n",
      "Episode: 5, Total Reward: -3.3999999999999995, Epsilon: 0.1681\n",
      "Episode: 6, Total Reward: -2.1999999999999993, Epsilon: 0.1176\n",
      "Episode: 7, Total Reward: -2.1999999999999993, Epsilon: 0.0824\n",
      "Episode: 8, Total Reward: -0.6999999999999993, Epsilon: 0.0576\n",
      "Episode: 9, Total Reward: -0.9999999999999996, Epsilon: 0.0404\n",
      "Episode: 10, Total Reward: -2.2999999999999994, Epsilon: 0.0282\n",
      "Episode: 11, Total Reward: -0.9999999999999996, Epsilon: 0.0198\n",
      "Episode: 12, Total Reward: -0.9999999999999996, Epsilon: 0.0138\n",
      "Episode: 13, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 14, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 15, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 16, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 17, Total Reward: -0.39999999999999947, Epsilon: 0.0100\n",
      "Episode: 18, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 19, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 20, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 21, Total Reward: -0.9999999999999996, Epsilon: 0.1000\n",
      "Episode: 22, Total Reward: -1.0999999999999996, Epsilon: 0.0700\n",
      "Episode: 23, Total Reward: -1.4999999999999996, Epsilon: 0.0490\n",
      "Episode: 24, Total Reward: -1.1999999999999995, Epsilon: 0.0343\n",
      "Episode: 25, Total Reward: -0.9999999999999996, Epsilon: 0.0240\n",
      "Episode: 26, Total Reward: -0.9999999999999996, Epsilon: 0.0168\n",
      "Episode: 27, Total Reward: -0.9999999999999996, Epsilon: 0.0118\n",
      "Episode: 28, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 29, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 30, Total Reward: 0.9000000000000008, Epsilon: 0.0100\n",
      "Episode: 31, Total Reward: 0.8000000000000007, Epsilon: 0.0100\n",
      "Episode: 32, Total Reward: -1.1999999999999995, Epsilon: 0.0100\n",
      "Episode: 33, Total Reward: -1.3999999999999995, Epsilon: 0.0100\n",
      "Episode: 34, Total Reward: -0.8999999999999996, Epsilon: 0.0100\n",
      "Episode: 35, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 36, Total Reward: -0.2999999999999994, Epsilon: 0.0100\n",
      "Episode: 37, Total Reward: -2.3999999999999995, Epsilon: 0.0100\n",
      "Episode: 38, Total Reward: -1.5999999999999996, Epsilon: 0.0100\n",
      "Episode: 39, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 40, Total Reward: -1.5999999999999994, Epsilon: 0.0100\n",
      "Episode: 41, Total Reward: 0.8000000000000007, Epsilon: 0.1000\n",
      "Episode: 42, Total Reward: -2.2999999999999994, Epsilon: 0.0700\n",
      "Episode: 43, Total Reward: 0.8000000000000007, Epsilon: 0.0490\n",
      "Episode: 44, Total Reward: -2.1999999999999993, Epsilon: 0.0343\n",
      "Episode: 45, Total Reward: -0.9999999999999996, Epsilon: 0.0240\n",
      "Episode: 46, Total Reward: -0.9999999999999996, Epsilon: 0.0168\n",
      "Episode: 47, Total Reward: -0.9999999999999953, Epsilon: 0.0118\n",
      "Episode: 48, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 49, Total Reward: 0.40000000000000124, Epsilon: 0.0100\n",
      "Episode: 50, Total Reward: -0.7999999999999996, Epsilon: 0.0100\n",
      "Episode: 51, Total Reward: 0.5000000000000013, Epsilon: 0.0100\n",
      "Episode: 52, Total Reward: 0.20000000000000062, Epsilon: 0.0100\n",
      "Episode: 53, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 54, Total Reward: -5.799999999999999, Epsilon: 0.0100\n",
      "Episode: 55, Total Reward: -0.7999999999999994, Epsilon: 0.0100\n",
      "Episode: 56, Total Reward: -1.0999999999999996, Epsilon: 0.0100\n",
      "Episode: 57, Total Reward: 0.4000000000000008, Epsilon: 0.0100\n",
      "Episode: 58, Total Reward: -3.1999999999999997, Epsilon: 0.0100\n",
      "Episode: 59, Total Reward: 0.4000000000000006, Epsilon: 0.0100\n",
      "Episode: 60, Total Reward: 1.6000000000000023, Epsilon: 0.0100\n",
      "Episode: 61, Total Reward: -0.6999999999999996, Epsilon: 0.1000\n",
      "Episode: 62, Total Reward: -0.9999999999999996, Epsilon: 0.0700\n",
      "Episode: 63, Total Reward: -2.7999999999999994, Epsilon: 0.0490\n",
      "Episode: 64, Total Reward: 0.4000000000000008, Epsilon: 0.0343\n",
      "Episode: 65, Total Reward: -0.9999999999999996, Epsilon: 0.0240\n",
      "Episode: 66, Total Reward: 0.20000000000000062, Epsilon: 0.0168\n",
      "Episode: 67, Total Reward: 0.9000000000000008, Epsilon: 0.0118\n",
      "Episode: 68, Total Reward: 0.9000000000000008, Epsilon: 0.0100\n",
      "Episode: 69, Total Reward: 1.8000000000000016, Epsilon: 0.0100\n",
      "Episode: 70, Total Reward: -1.5999999999999996, Epsilon: 0.0100\n",
      "Episode: 71, Total Reward: -0.9999999999999987, Epsilon: 0.0100\n",
      "Episode: 72, Total Reward: 0.600000000000001, Epsilon: 0.0100\n",
      "Episode: 73, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 74, Total Reward: 0.3000000000000007, Epsilon: 0.0100\n",
      "Episode: 75, Total Reward: -0.19999999999999796, Epsilon: 0.0100\n",
      "Episode: 76, Total Reward: 1.0000000000000009, Epsilon: 0.0100\n",
      "Episode: 77, Total Reward: 0.20000000000000062, Epsilon: 0.0100\n",
      "Episode: 78, Total Reward: -2.5999999999999996, Epsilon: 0.0100\n",
      "Episode: 79, Total Reward: -0.39999999999999947, Epsilon: 0.0100\n",
      "Episode: 80, Total Reward: -3.4, Epsilon: 0.0100\n",
      "Episode: 81, Total Reward: 0.9000000000000008, Epsilon: 0.0100\n",
      "Episode: 82, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 83, Total Reward: -0.6999999999999996, Epsilon: 0.0100\n",
      "Episode: 84, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 85, Total Reward: 0.9000000000000008, Epsilon: 0.0100\n",
      "Episode: 86, Total Reward: 1.100000000000001, Epsilon: 0.0100\n",
      "Episode: 87, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 88, Total Reward: 0.9000000000000008, Epsilon: 0.0100\n",
      "Episode: 89, Total Reward: 0.8000000000000007, Epsilon: 0.0100\n",
      "Episode: 90, Total Reward: 0.20000000000000062, Epsilon: 0.0100\n",
      "Episode: 91, Total Reward: 0.600000000000001, Epsilon: 0.0100\n",
      "Episode: 92, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n",
      "Episode: 93, Total Reward: 0.7000000000000011, Epsilon: 0.0100\n",
      "Episode: 94, Total Reward: -0.39999999999999947, Epsilon: 0.0100\n",
      "Episode: 95, Total Reward: 0.20000000000000062, Epsilon: 0.0100\n",
      "Episode: 96, Total Reward: -0.9999999999999996, Epsilon: 0.0100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQN_pp_Agent(env, current_hyperparams)\n\u001b[1;32m     57\u001b[0m exp_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrho\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_kappa=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkappa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 58\u001b[0m exp_res_DQN_pp[exp_key] \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Test the agent\u001b[39;00m\n\u001b[1;32m     61\u001b[0m test_res_DQN_pp[exp_key] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtest(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[17], line 143\u001b[0m, in \u001b[0;36mDQN_pp_Agent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# Select and execute action\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state, done)\n",
      "Cell \u001b[0;32mIn[17], line 93\u001b[0m, in \u001b[0;36mDQN_pp_Agent.select_action\u001b[0;34m(self, state, testing)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 93\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net(state)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "import itertools\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "\n",
    "# Hyperparameters\n",
    "hyper = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"epsilon\": 1.0,\n",
    "    \"epsilon_decay\": 0.7,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"memory_size\": 10000,\n",
    "    \"episodes\": 40000,\n",
    "    \"target_update_freq\": 10,\n",
    "    \"rho\":[0.1,0.2],\n",
    "    \"kappa\":[0.25,0.5,0.75],\n",
    "    \"eps_update_freq\":[10,100,1000]\n",
    "}\n",
    "\n",
    "\n",
    "param_combinations = itertools.product(\n",
    "    hyper[\"rho\"], \n",
    "    hyper[\"kappa\"],\n",
    "    hyper[\"eps_update_freq\"]\n",
    ")\n",
    "\n",
    "# Initialize result dictionaries\n",
    "exp_res_DQN_pp = {}\n",
    "test_res_DQN_pp  = {}\n",
    "\n",
    "# Iterate through parameter combinations\n",
    "for rho, kappa,eps_update_freq in param_combinations:\n",
    "    current_hyperparams = {\n",
    "        \"learning_rate\": hyper[\"learning_rate\"],\n",
    "        \"discount_factor\": hyper[\"discount_factor\"],\n",
    "        \"epsilon\": hyper[\"epsilon\"],\n",
    "        \"epsilon_decay\": hyper[\"epsilon_decay\"],\n",
    "        \"epsilon_min\": hyper[\"epsilon_min\"],\n",
    "        \"batch_size\": hyper[\"batch_size\"],\n",
    "        \"memory_size\": hyper[\"memory_size\"],\n",
    "        \"episodes\": hyper[\"episodes\"],\n",
    "        \"target_update_freq\": hyper[\"target_update_freq\"],\n",
    "        \"rho\" :rho,\n",
    "        \"kappa\" :kappa,\n",
    "        \"eps_update_freq\" : eps_update_freq\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Train the agent\n",
    "        agent = DQN_pp_Agent(env, current_hyperparams)\n",
    "        exp_key = f\"rho={rho}_kappa={kappa}\"\n",
    "        exp_res_DQN_pp[exp_key] = agent.train()\n",
    "        \n",
    "        # Test the agent\n",
    "        test_res_DQN_pp[exp_key] = agent.test(num_episodes=5000, render=None)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        with open(\"exp_res_DQN_pp.pkl\", \"wb\") as f:\n",
    "            pickle.dump(exp_res_DQN_pp, f)\n",
    "        with open(\"test_res_DQN_pp.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_res_DQN_pp, f)\n",
    "        \n",
    "        print(f\"Finished training and testing for: {exp_key}\")\n",
    "    \n",
    "    finally:\n",
    "        # Free resources\n",
    "        #del agent\n",
    "        gc.collect()\n",
    "        env.close()\n",
    "\n",
    "\n",
    "with open(\"exp_res_DQN_pp.pkl\", \"rb\") as f:\n",
    "    exp_res_DQN_pp = pickle.load(f)\n",
    "with open(\"test_res_DQN_pp.pkl\", \"rb\") as f:\n",
    "    test_res_DQN_pp = pickle.load(f)\n",
    "\n",
    "print(\"All parameter combinations processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
