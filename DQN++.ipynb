{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import pygame\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Grayscale\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs.lidar import LIDAR\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    "    PIPE_WIDTH,\n",
    "    PIPE_VEL_X,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "\n",
    "def new_render(self):\n",
    "    \"\"\"Renders the next frame.\"\"\"\n",
    "    if self.render_mode == \"rgb_array\":\n",
    "        self._draw_surface(show_score=False, show_rays=False)\n",
    "        # Flip the image to retrieve a correct aspect\n",
    "        return np.transpose(pygame.surfarray.array3d(self._surface), axes=(1, 0, 2))\n",
    "    else:\n",
    "        self._draw_surface(show_score=True, show_rays=False)\n",
    "        if self._display is None:\n",
    "            self._make_display()\n",
    "\n",
    "        self._update_display()\n",
    "        self._fps_clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "\n",
    "FlappyBirdEnv.render = new_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "######DQN++######\n",
    "################\n",
    "\n",
    "class DQN_pp(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(DQN_pp, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)   \n",
    "        self.fc3 = nn.Linear(128, 128)      # Second hidden layer\n",
    "        self.fc4 = nn.Linear(128, action_space)  # Output layer for Q-values\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN_pp_Agent:\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 hyper ={\n",
    "                          \"learning_rate\": 0.001,\n",
    "                          \"discount_factor\" : 0.99,\n",
    "                          \"epsilon\" : 1.0,\n",
    "                          \"epsilon_decay\" :0.999,\n",
    "                          \"epsilon_min\" : 0.01,\n",
    "                          \"batch_size\" : 64,\n",
    "                          \"memory_size\" : 10000,\n",
    "                          \"episodes\" : 100000,\n",
    "                          \"target_update_freq\" : 10,\n",
    "                          \"rho\" :1.0,\n",
    "                          \"kappa\" : 1.0,\n",
    "                          \"eps_update_freq\":100\n",
    "                        }\n",
    "                 ):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        # Device setup for GPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = hyper[\"learning_rate\"]\n",
    "        self.discount_factor = hyper[\"discount_factor\"]\n",
    "        self.epsilon = hyper[\"epsilon\"]\n",
    "        self.epsilon_decay = hyper[\"epsilon_decay\"]\n",
    "        self.epsilon_min = hyper[\"epsilon_min\"]\n",
    "        self.batch_size = hyper[\"batch_size\"]\n",
    "        self.memory_size = hyper[\"memory_size\"]\n",
    "        self.episodes = hyper[\"episodes\"]\n",
    "        self.target_update_freq = hyper[\"target_update_freq\"]\n",
    "        self.rho = hyper[\"rho\"]\n",
    "        self.kappa = hyper[\"kappa\"]\n",
    "        self.eps_update_freq=hyper[\"eps_update_freq\"]\n",
    "\n",
    "        # Initialize policy and target networks\n",
    "        self.policy_net = DQN_pp(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net = DQN_pp(self.state_dim, self.action_space).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer and Replay Memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "\n",
    "    def select_action(self, state, testing=False):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not testing and random.random() < self.epsilon:\n",
    "            return self.oracle(random.randint(0, self.action_space - 1) ) # Random action\n",
    "\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                return self.oracle(self.policy_net(state).argmax(dim=1).item())\n",
    "    \n",
    "    def oracle(self, action):\n",
    "        next_state, reward, done, _, _ = self.env.step(action)\n",
    "        if done :\n",
    "            action = (action+1)%2\n",
    "\n",
    "        return action\n",
    "        \n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Sample a batch from memory and optimize the policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-values and targets\n",
    "        q_values = self.policy_net(states).gather(1, actions)\n",
    "        next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # Loss and backpropagation\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        res=[]\n",
    "        reward_trace=0.0\n",
    "        old_reward_trace=0.0\n",
    "        counter=0\n",
    "  \n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                # Select and execute action\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.memory.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward = round(total_reward+reward,2)\n",
    "\n",
    "\n",
    "\n",
    "                # Optimize model\n",
    "                self.optimize_model()\n",
    "\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            \n",
    "            # Update target network periodically\n",
    "            \n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            # Update epsilon to rho\n",
    "\n",
    "            if abs(self.epsilon-self.epsilon_min) < 1e-4 :\n",
    "\n",
    "                reward_trace=round(reward_trace+total_reward,2)\n",
    "                counter+=1\n",
    "                if counter % self.eps_update_freq== 0:\n",
    "                    if (reward_trace-old_reward_trace)/self.eps_update_freq<self.kappa:\n",
    "                        self.epsilon = self.rho\n",
    "                        reward_trace=0.0\n",
    "                        old_reward_trace=0.0\n",
    "                    \n",
    "                    counter=0\n",
    "                    old_reward_trace=reward_trace\n",
    "                    reward_trace=0.0\n",
    "\n",
    "\n",
    "            res.append(total_reward)\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "        self.env.close()\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def test(self, num_episodes=10,render=\"human\"):\n",
    "        \n",
    "        \"\"\"Test the trained policy with real-time rendering.\"\"\"\n",
    "        print(\"\\nTesting the trained policy...\\n\")\n",
    "        self.epsilon = 0.0  # Disable exploration\n",
    "        test_env = gymnasium.make(\"FlappyBird-v0\", render_mode=render,use_lidar=True)  # Render in \"human\" mode \n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state, testing=True)\n",
    "                next_state, reward, done, _, _ = test_env.step(action)\n",
    "                state = next_state\n",
    "                total_reward = round(total_reward +reward,2)\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "        \n",
    "            print(f\"Test Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "        \n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f\"\\nAverage Reward over {num_episodes} Test Episodes: {avg_reward}\")\n",
    "        test_env.close()\n",
    "\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Episode: 1, Total Reward: -4.0, Epsilon: 0.9000, Delta: 0.0\n",
      "Episode: 2, Total Reward: -1.6, Epsilon: 0.8100, Delta: 0.0\n",
      "Episode: 3, Total Reward: -4.0, Epsilon: 0.7290, Delta: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvdg/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/cvdg/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4, Total Reward: -3.4, Epsilon: 0.6561, Delta: 0.0\n",
      "Episode: 5, Total Reward: -4.6, Epsilon: 0.5905, Delta: 0.0\n",
      "Episode: 6, Total Reward: -4.0, Epsilon: 0.5314, Delta: 0.0\n",
      "Episode: 7, Total Reward: -4.0, Epsilon: 0.4783, Delta: 0.0\n",
      "Episode: 8, Total Reward: -1.0, Epsilon: 0.4305, Delta: 0.0\n",
      "Episode: 9, Total Reward: -4.6, Epsilon: 0.3874, Delta: 0.0\n",
      "Episode: 10, Total Reward: -1.0, Epsilon: 0.3487, Delta: 0.0\n",
      "Episode: 11, Total Reward: -2.2, Epsilon: 0.3138, Delta: 0.0\n",
      "Episode: 12, Total Reward: -1.0, Epsilon: 0.2824, Delta: 0.0\n",
      "Episode: 13, Total Reward: -1.0, Epsilon: 0.2542, Delta: 0.0\n",
      "Episode: 14, Total Reward: -1.0, Epsilon: 0.2288, Delta: 0.0\n",
      "Episode: 15, Total Reward: -1.0, Epsilon: 0.2059, Delta: 0.0\n",
      "Episode: 16, Total Reward: -2.8, Epsilon: 0.1853, Delta: 0.0\n",
      "Episode: 17, Total Reward: -1.0, Epsilon: 0.1668, Delta: 0.0\n",
      "Episode: 18, Total Reward: -1.0, Epsilon: 0.1501, Delta: 0.0\n",
      "Episode: 19, Total Reward: -3.4, Epsilon: 0.1351, Delta: 0.0\n",
      "Episode: 20, Total Reward: 0.8, Epsilon: 0.1216, Delta: 0.0\n",
      "Episode: 21, Total Reward: -4.0, Epsilon: 0.1094, Delta: 0.0\n",
      "Episode: 22, Total Reward: -1.0, Epsilon: 0.0985, Delta: 0.0\n",
      "Episode: 23, Total Reward: -2.6, Epsilon: 0.0886, Delta: 0.0\n",
      "Episode: 24, Total Reward: -0.4, Epsilon: 0.0798, Delta: 0.0\n",
      "Episode: 25, Total Reward: -2.8, Epsilon: 0.0718, Delta: 0.0\n",
      "Episode: 26, Total Reward: -1.6, Epsilon: 0.0646, Delta: 0.0\n",
      "Episode: 27, Total Reward: -1.0, Epsilon: 0.0581, Delta: 0.0\n",
      "Episode: 28, Total Reward: -0.4, Epsilon: 0.0523, Delta: 0.0\n",
      "Episode: 29, Total Reward: -2.2, Epsilon: 0.0471, Delta: 0.0\n",
      "Episode: 30, Total Reward: 0.2, Epsilon: 0.0424, Delta: 0.0\n",
      "Episode: 31, Total Reward: -1.9, Epsilon: 0.0382, Delta: 0.0\n",
      "Episode: 32, Total Reward: -0.4, Epsilon: 0.0343, Delta: 0.0\n",
      "Episode: 33, Total Reward: -0.9, Epsilon: 0.0309, Delta: 0.0\n",
      "Episode: 34, Total Reward: 0.8, Epsilon: 0.0278, Delta: 0.0\n",
      "Episode: 35, Total Reward: -1.0, Epsilon: 0.0250, Delta: 0.0\n",
      "Episode: 36, Total Reward: 0.8, Epsilon: 0.0225, Delta: 0.0\n",
      "Episode: 37, Total Reward: -1.0, Epsilon: 0.0203, Delta: 0.0\n",
      "Episode: 38, Total Reward: 0.3, Epsilon: 0.0182, Delta: 0.0\n",
      "Episode: 39, Total Reward: -1.6, Epsilon: 0.0164, Delta: 0.0\n",
      "Episode: 40, Total Reward: -1.2, Epsilon: 0.0148, Delta: 0.0\n",
      "Episode: 41, Total Reward: -1.4, Epsilon: 0.0133, Delta: 0.0\n",
      "Episode: 42, Total Reward: -2.1, Epsilon: 0.0120, Delta: 0.0\n",
      "Episode: 43, Total Reward: 0.0, Epsilon: 0.0108, Delta: 0.0\n",
      "Episode: 44, Total Reward: -0.4, Epsilon: 0.0100, Delta: -0.04\n",
      "Episode: 45, Total Reward: -0.7, Epsilon: 0.0100, Delta: -0.11000000000000001\n",
      "Episode: 46, Total Reward: -0.1, Epsilon: 0.0100, Delta: -0.12\n",
      "Episode: 47, Total Reward: -1.0, Epsilon: 0.0100, Delta: -0.22000000000000003\n",
      "Episode: 48, Total Reward: -0.7, Epsilon: 0.0100, Delta: -0.29\n",
      "Episode: 49, Total Reward: 0.8, Epsilon: 0.0100, Delta: -0.21000000000000002\n",
      "Episode: 50, Total Reward: -4.5, Epsilon: 0.0100, Delta: -0.6599999999999999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQN_pp_Agent(env, current_hyperparams)\n\u001b[1;32m     57\u001b[0m exp_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrho\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_kappa=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkappa\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 58\u001b[0m exp_res_DQN_pp[exp_key] \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Test the agent\u001b[39;00m\n\u001b[1;32m     61\u001b[0m test_res_DQN_pp[exp_key] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtest(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[64], line 146\u001b[0m, in \u001b[0;36mDQN_pp_Agent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# Select and execute action\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m--> 146\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state, done)\n\u001b[1;32m    148\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/flappy_bird_gymnasium/envs/flappy_bird_env.py:262\u001b[0m, in \u001b[0;36mFlappyBirdEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m--> 262\u001b[0m obs, reward_private_zone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward_private_zone \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/flappy_bird_gymnasium/envs/flappy_bird_env.py:529\u001b[0m, in \u001b[0;36mFlappyBirdEnv._get_observation_lidar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_observation_lidar\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;66;03m# obstacles\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lidar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_player_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_player_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_player_rot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upper_pipes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lower_pipes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(distances \u001b[38;5;241m<\u001b[39m PLAYER_PRIVATE_ZONE):\n\u001b[1;32m    539\u001b[0m         reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/flappy/lib/python3.10/site-packages/flappy_bird_gymnasium/envs/lidar.py:100\u001b[0m, in \u001b[0;36mLIDAR.scan\u001b[0;34m(self, player_x, player_y, player_rot, upper_pipes, lower_pipes, ground)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollisions[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ground[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# calculate distance\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     result[i] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset_x\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollisions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollisions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "import itertools\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "\n",
    "# Hyperparameters\n",
    "hyper = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"discount_factor\": 0.99,\n",
    "    \"epsilon\": 1.0,\n",
    "    \"epsilon_decay\": 0.9,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"memory_size\": 10000,\n",
    "    \"episodes\": 40000,\n",
    "    \"target_update_freq\": 10,\n",
    "    \"rho\":[0.1,0.2],\n",
    "    \"kappa\":[0.25,0.5,0.75],\n",
    "    \"eps_update_freq\":[10,100,1000]\n",
    "}\n",
    "\n",
    "\n",
    "param_combinations = itertools.product(\n",
    "    hyper[\"rho\"], \n",
    "    hyper[\"kappa\"],\n",
    "    hyper[\"eps_update_freq\"]\n",
    ")\n",
    "\n",
    "# Initialize result dictionaries\n",
    "exp_res_DQN_pp = {}\n",
    "test_res_DQN_pp  = {}\n",
    "\n",
    "# Iterate through parameter combinations\n",
    "for rho, kappa,eps_update_freq in param_combinations:\n",
    "    current_hyperparams = {\n",
    "        \"learning_rate\": hyper[\"learning_rate\"],\n",
    "        \"discount_factor\": hyper[\"discount_factor\"],\n",
    "        \"epsilon\": hyper[\"epsilon\"],\n",
    "        \"epsilon_decay\": hyper[\"epsilon_decay\"],\n",
    "        \"epsilon_min\": hyper[\"epsilon_min\"],\n",
    "        \"batch_size\": hyper[\"batch_size\"],\n",
    "        \"memory_size\": hyper[\"memory_size\"],\n",
    "        \"episodes\": hyper[\"episodes\"],\n",
    "        \"target_update_freq\": hyper[\"target_update_freq\"],\n",
    "        \"rho\" :rho,\n",
    "        \"kappa\" :kappa,\n",
    "        \"eps_update_freq\" : eps_update_freq\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Train the agent\n",
    "        agent = DQN_pp_Agent(env, current_hyperparams)\n",
    "        exp_key = f\"rho={rho}_kappa={kappa}\"\n",
    "        exp_res_DQN_pp[exp_key] = agent.train()\n",
    "        \n",
    "        # Test the agent\n",
    "        test_res_DQN_pp[exp_key] = agent.test(num_episodes=5000, render=None)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        with open(\"exp_res_DQN_pp.pkl\", \"wb\") as f:\n",
    "            pickle.dump(exp_res_DQN_pp, f)\n",
    "        with open(\"test_res_DQN_pp.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_res_DQN_pp, f)\n",
    "        \n",
    "        print(f\"Finished training and testing for: {exp_key}\")\n",
    "    \n",
    "    finally:\n",
    "        # Free resources\n",
    "        #del agent\n",
    "        gc.collect()\n",
    "        env.close()\n",
    "\n",
    "\n",
    "with open(\"exp_res_DQN_pp.pkl\", \"rb\") as f:\n",
    "    exp_res_DQN_pp = pickle.load(f)\n",
    "with open(\"test_res_DQN_pp.pkl\", \"rb\") as f:\n",
    "    test_res_DQN_pp = pickle.load(f)\n",
    "\n",
    "print(\"All parameter combinations processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
