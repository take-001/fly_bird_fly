{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "import numpy as np\n",
    "import pygame\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Grayscale\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import FlappyBirdEnv\n",
    "from flappy_bird_gymnasium.envs.flappy_bird_env import Actions\n",
    "from flappy_bird_gymnasium.envs.lidar import LIDAR\n",
    "from flappy_bird_gymnasium.envs.constants import (\n",
    "    PLAYER_FLAP_ACC,\n",
    "    PLAYER_ACC_Y,\n",
    "    PLAYER_MAX_VEL_Y,\n",
    "    PLAYER_HEIGHT,\n",
    "    PLAYER_VEL_ROT,\n",
    "    PLAYER_WIDTH,\n",
    "    PIPE_WIDTH,\n",
    "    PIPE_VEL_X,\n",
    ")\n",
    "\n",
    "def new_render(self):\n",
    "    \"\"\"Renders the next frame.\"\"\"\n",
    "    if self.render_mode == \"rgb_array\":\n",
    "        self._draw_surface(show_score=False, show_rays=False)\n",
    "        # Flip the image to retrieve a correct aspect\n",
    "        return np.transpose(pygame.surfarray.array3d(self._surface), axes=(1, 0, 2))\n",
    "    else:\n",
    "        self._draw_surface(show_score=True, show_rays=False)\n",
    "        if self._display is None:\n",
    "            self._make_display()\n",
    "\n",
    "        self._update_display()\n",
    "        self._fps_clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "\n",
    "FlappyBirdEnv.render = new_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -8.10, Avg: -8.10\n",
      "Episode 100, Reward: 1.40, Avg: -2.01\n",
      "Episode 200, Reward: 0.80, Avg: -0.10\n",
      "Episode 300, Reward: 2.10, Avg: 0.05\n",
      "Episode 400, Reward: -0.90, Avg: 0.16\n",
      "Episode 500, Reward: 0.80, Avg: 0.35\n",
      "Episode 600, Reward: 1.50, Avg: 0.35\n",
      "Episode 700, Reward: 1.80, Avg: 0.59\n",
      "Episode 800, Reward: -0.90, Avg: 0.19\n",
      "Episode 900, Reward: 1.50, Avg: 0.65\n",
      "Episode 1000, Reward: 0.80, Avg: 0.13\n",
      "Episode 1100, Reward: 1.60, Avg: 0.56\n",
      "Episode 1200, Reward: 0.80, Avg: 0.28\n",
      "Episode 1300, Reward: 0.80, Avg: 0.67\n",
      "Episode 1400, Reward: -0.90, Avg: 0.31\n",
      "Episode 1500, Reward: 0.80, Avg: 0.07\n",
      "Episode 1600, Reward: 2.10, Avg: 0.39\n",
      "Episode 1700, Reward: 0.10, Avg: 0.35\n",
      "Episode 1800, Reward: 0.80, Avg: 0.06\n",
      "Episode 1900, Reward: -0.30, Avg: -0.19\n",
      "Episode 2000, Reward: -0.40, Avg: -0.09\n",
      "Episode 2100, Reward: -0.30, Avg: 0.80\n",
      "Episode 2200, Reward: -1.50, Avg: 1.25\n",
      "Episode 2300, Reward: 12.70, Avg: 3.44\n",
      "Episode 2400, Reward: 10.50, Avg: 3.51\n",
      "Episode 2500, Reward: 24.00, Avg: 4.52\n",
      "Episode 2600, Reward: -0.90, Avg: 6.52\n",
      "Episode 2700, Reward: 42.40, Avg: 7.53\n",
      "Episode 2800, Reward: 5.10, Avg: 9.41\n",
      "Episode 2900, Reward: 33.80, Avg: 13.14\n",
      "Episode 3000, Reward: 128.50, Avg: 14.75\n",
      "Episode 3100, Reward: 15.20, Avg: 22.20\n",
      "Episode 3200, Reward: 5.70, Avg: 15.77\n",
      "Episode 3300, Reward: 14.60, Avg: 8.77\n",
      "Episode 3400, Reward: 5.00, Avg: 16.31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 293\u001b[0m\n\u001b[0;32m    291\u001b[0m    agent \u001b[38;5;241m=\u001b[39m OptimizedActorCritic(env, current_hyperparams)\n\u001b[0;32m    292\u001b[0m    exp_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_c_lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcritic_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gae=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgae_lambda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ent=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentropy_coef\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_val=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_loss_coef\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 293\u001b[0m    exp_res[exp_key] \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m    test_res[exp_key] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtest(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m#    with open(\"ac_exp_res.pkl\", \"wb\") as f:\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m#        pickle.dump(exp_res, f)\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m#    with open(\"ac_test_res.pkl\", \"wb\") as f:\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m#        pickle.dump(test_res, f)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 136\u001b[0m, in \u001b[0;36mOptimizedActorCritic.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 136\u001b[0m     action, log_prob, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "Cell \u001b[1;32mIn[7], line 61\u001b[0m, in \u001b[0;36mOptimizedActorCritic.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     59\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m     60\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 61\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), log_prob\u001b[38;5;241m.\u001b[39mitem(), value\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\mdprl\\lib\\site-packages\\torch\\distributions\\categorical.py:139\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    141\u001b[0m     value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\mdprl\\lib\\site-packages\\torch\\distributions\\distribution.py:314\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######## Optimized AC ########\n",
    "######## For Video Record ########\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, action_space)\n",
    "        )\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        return F.softmax(self.actor(shared), dim=-1), self.critic(shared)\n",
    "\n",
    "class OptimizedActorCritic:\n",
    "    def __init__(self, env, config):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        # Parameters\n",
    "        self.actor_lr = config.get('actor_lr', 0.001)\n",
    "        self.critic_lr = config.get('critic_lr', 0.005)\n",
    "        self.gamma = config.get('gamma', 0.99)\n",
    "        self.gae_lambda = config.get('gae_lambda', 0.95)\n",
    "        self.entropy_coef = config.get('entropy_coef', 0.01)\n",
    "        self.value_loss_coef = config.get('value_loss_coef', 0.5)\n",
    "        self.max_grad_norm = config.get('max_grad_norm', 0.5)\n",
    "        self.episodes = config.get('episodes', 5000)\n",
    "        \n",
    "        self.network = ActorCriticNet(self.state_dim, self.action_space).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.network.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.network.parameters(), lr=self.critic_lr)\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.masks = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs, value = self.network(state)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "    def compute_gae(self):\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            if step == len(self.rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = self.values[step + 1]\n",
    "                \n",
    "            delta = self.rewards[step] + self.gamma * next_value * self.masks[step] - self.values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * self.masks[step] * gae\n",
    "            returns.insert(0, gae + self.values[step])\n",
    "            \n",
    "        return torch.FloatTensor(returns).to(self.device)\n",
    "\n",
    "    def train_step(self):\n",
    "        returns = self.compute_gae()\n",
    "        \n",
    "        values = torch.FloatTensor(self.values).to(self.device)\n",
    "        log_probs = torch.FloatTensor(self.log_probs).to(self.device)\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        states = torch.FloatTensor(self.states).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        new_probs, new_values = self.network(states)\n",
    "        dist = Categorical(new_probs)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Actor loss\n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        actor_loss = -(surr1.mean() + self.entropy_coef * entropy)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = self.value_loss_coef * F.mse_loss(new_values.squeeze(-1), returns)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Optimize\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Clear memory\n",
    "        self.log_probs.clear()\n",
    "        self.values.clear()\n",
    "        self.rewards.clear()\n",
    "        self.masks.clear()\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "\n",
    "    def train(self):\n",
    "        rewards_history = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            self.states = []\n",
    "            self.actions = []\n",
    "            \n",
    "            while not done:\n",
    "                action, log_prob, value = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                \n",
    "                self.states.append(state)\n",
    "                self.actions.append(action)\n",
    "                self.rewards.append(reward)\n",
    "                self.log_probs.append(log_prob)\n",
    "                self.values.append(value)\n",
    "                self.masks.append(1 - done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            self.train_step()\n",
    "            rewards_history.append(total_reward)\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(rewards_history[-100:])\n",
    "                print(f\"Episode {episode}, Reward: {total_reward:.2f}, Avg: {avg_reward:.2f}\")\n",
    "        \n",
    "        return rewards_history\n",
    "\n",
    "    def test(self, num_episodes=10):\n",
    "        total_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    probs, _ = self.network(state)\n",
    "                action = probs.argmax().item()\n",
    "                state, reward, done, truncated, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if truncated:\n",
    "                    break\n",
    "            \n",
    "            total_rewards.append(total_reward)\n",
    "            # print(f\"Test Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(total_rewards[-100:])\n",
    "                print(f\"Test Episode {episode}, Reward: {total_reward:.2f}, Avg: {avg_reward:.2f}\")\n",
    "                \n",
    "        print(f\"Average Test Reward: {np.mean(total_rewards):.2f}\")\n",
    "        return total_rewards\n",
    "    \n",
    "    def play(self, num_episodes=1, render=True):\n",
    "        \"\"\"\n",
    "        Play the game with the trained agent and visualize the gameplay using a separate environment.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes (int): Number of episodes to play\n",
    "            render (bool): Whether to render the environment\n",
    "        \"\"\"\n",
    "        # Create a separate environment for visualization\n",
    "        if render:\n",
    "            viz_env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=True)\n",
    "        else:\n",
    "            viz_env = self.env\n",
    "            \n",
    "        try:\n",
    "            for episode in range(num_episodes):\n",
    "                state, _ = viz_env.reset()\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                episode_steps = 0\n",
    "                \n",
    "                while not done:\n",
    "                    if render:\n",
    "                        viz_env.render()\n",
    "                    \n",
    "                    # Convert state to tensor and get action\n",
    "                    state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        probs, _ = self.network(state)\n",
    "                    action = probs.argmax().item()\n",
    "                    \n",
    "                    # Take action in environment\n",
    "                    state, reward, done, truncated, _ = viz_env.step(action)\n",
    "                    total_reward += reward\n",
    "                    episode_steps += 1\n",
    "                    \n",
    "                    if truncated:\n",
    "                        break\n",
    "                \n",
    "                print(f\"Episode {episode + 1} finished with reward: {total_reward:.2f} in {episode_steps} steps\")\n",
    "        \n",
    "        finally:\n",
    "            if render:\n",
    "                viz_env.close()\n",
    "                \n",
    "        return total_reward\n",
    "            \n",
    "            \n",
    "import gymnasium\n",
    "import itertools\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "\n",
    "# Real\n",
    "hyper = {\n",
    "   \"actor_lr\": [0.0005],\n",
    "   \"critic_lr\": [0.001], \n",
    "   \"gamma\": 0.99,\n",
    "   \"gae_lambda\": [0.9],\n",
    "   \"entropy_coef\": [0.05],\n",
    "   \"value_loss_coef\": [1.0],\n",
    "   \"max_grad_norm\": 0.5,\n",
    "   \"episodes\": 30000\n",
    "}\n",
    "\n",
    "# For test\n",
    "# hyper = {\n",
    "#    \"actor_lr\": [0.0005],\n",
    "#    \"critic_lr\": [0.005], \n",
    "#    \"gamma\": 0.99,\n",
    "#    \"gae_lambda\": [0.9, 0.95],\n",
    "#    \"entropy_coef\": [0.01, 0.05],\n",
    "#    \"value_loss_coef\": [0.5, 1.0],\n",
    "#    \"max_grad_norm\": 0.5,\n",
    "#    \"episodes\": 200\n",
    "# }\n",
    "\n",
    "param_combinations = itertools.product(\n",
    "   hyper[\"actor_lr\"],\n",
    "   hyper[\"critic_lr\"], \n",
    "   hyper[\"gae_lambda\"],\n",
    "   hyper[\"entropy_coef\"],\n",
    "   hyper[\"value_loss_coef\"]\n",
    ")\n",
    "\n",
    "exp_res = {}\n",
    "test_res = {}\n",
    "\n",
    "for actor_lr, critic_lr, gae_lambda, entropy_coef, value_loss_coef in param_combinations:\n",
    "   current_hyperparams = {\n",
    "       \"actor_lr\": actor_lr,\n",
    "       \"critic_lr\": critic_lr,\n",
    "       \"gamma\": hyper[\"gamma\"],\n",
    "       \"gae_lambda\": gae_lambda,\n",
    "       \"entropy_coef\": entropy_coef,\n",
    "       \"value_loss_coef\": value_loss_coef,\n",
    "       \"max_grad_norm\": hyper[\"max_grad_norm\"],\n",
    "       \"episodes\": hyper[\"episodes\"]\n",
    "   }\n",
    "   \n",
    "   try:\n",
    "       agent = OptimizedActorCritic(env, current_hyperparams)\n",
    "       exp_key = f\"a_lr={actor_lr}_c_lr={critic_lr}_gae={gae_lambda}_ent={entropy_coef}_val={value_loss_coef}\"\n",
    "       exp_res[exp_key] = agent.train()\n",
    "       test_res[exp_key] = agent.test(num_episodes=3000)\n",
    "       \n",
    "    #    with open(\"ac_exp_res.pkl\", \"wb\") as f:\n",
    "    #        pickle.dump(exp_res, f)\n",
    "    #    with open(\"ac_test_res.pkl\", \"wb\") as f:\n",
    "    #        pickle.dump(test_res, f)\n",
    "           \n",
    "       print(f\"Completed: {exp_key}\")\n",
    "       \n",
    "   finally:\n",
    "       gc.collect()\n",
    "       env.close()\n",
    "\n",
    "# with open(\"ac_exp_res.pkl\", \"rb\") as f:\n",
    "#    exp_res = pickle.load(f)\n",
    "# with open(\"ac_test_res.pkl\", \"rb\") as f:\n",
    "#    test_res = pickle.load(f)\n",
    "\n",
    "print(\"Experiment complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward: 17.40 in 296 steps\n",
      "Episode 2 finished with reward: 25.00 in 633 steps\n",
      "Episode 3 finished with reward: 29.90 in 445 steps\n",
      "Episode 4 finished with reward: 13.80 in 296 steps\n",
      "Episode 5 finished with reward: 147.80 in 2965 steps\n",
      "Episode 6 finished with reward: 128.00 in 2176 steps\n",
      "Episode 7 finished with reward: 82.90 in 1347 steps\n",
      "Episode 8 finished with reward: 15.80 in 331 steps\n",
      "Episode 9 finished with reward: 9.10 in 219 steps\n",
      "Episode 10 finished with reward: 28.80 in 818 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.8000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.play(num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
